{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGP AS-Level Topology: Graph Feature Extraction Pipeline",
    "",
    "**Purpose:** Download RIPE RIS MRT data (RIB dumps), parse with bgpkit-parser,",
    "construct AS-level topology graphs, and extract comprehensive graph-theoretic features for BGP anomaly detection.",
    "",
    "**Features extracted:**",
    "- **16 Graph-level metrics:** assortativity, diameter, algebraic connectivity, spectral radius, symmetry ratio, natural connectivity, effective graph resistance, spanning tree count, weighted spectrum stats, percolation limit, node/edge connectivity, clustering coefficient, density, rich-club coefficient, betweenness distribution stats, k-core decomposition metrics",
    "- **10 Node-level metrics:** degree centrality, betweenness centrality, closeness centrality, eigenvector centrality, PageRank, local clustering coefficient, average neighbor degree, node clique number, eccentricity, k-shell/core number",
    "",
    "**Enhancements over original:**",
    "- Private/reserved ASN filtering (RFC 6996, RFC 7300)",
    "- Edge weight tracking (number of AS paths traversing each link)",
    "- Consolidated matrix computations (adjacency/Laplacian computed once, reused)",
    "- Fixed symmetry ratio and natural connectivity for partial spectrum",
    "- Eliminated duplicate betweenness and k-core computations",
    "- Improved eccentricity handling for large graphs via NetworKit BFS",
    "- Proper clique number fallback with greedy approximation",
    "",
    "**Data source:** Configurable RIPE RIS collector and time period. RIB-only mode.",
    "",
    "**References:**",
    "- Willinger, W. & Roughan, M. \"Internet Topology Research Redux,\" *Recent Advances in Networking*, ACM SIGCOMM (2013)",
    "- Newman, M.E.J. *Networks: An Introduction*, Oxford University Press (2010)",
    "- Li, L. et al. \"A First-Principles Approach to Understanding the Internet's Router-Level Topology,\" *ACM SIGCOMM* (2004)",
    "- Sanchez, O.R. et al. \"Comparing ML Algorithms for BGP Anomaly Detection using Graph Features,\" *Big-DAMA '19* (2019)",
    "",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install pybgpkit networkx scipy numpy pandas matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Optional, Dict, List, Tuple, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy import stats as sp_stats\n",
    "import bgpkit\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Optional: NetworKit for high-performance computation on large graphs\n",
    "try:\n",
    "    import networkit as nk\n",
    "    HAS_NETWORKIT = True\n",
    "    logger.info(\"NetworKit available - will use for performance-critical computations\")\n",
    "except ImportError:\n",
    "    HAS_NETWORKIT = False\n",
    "    logger.info(\"NetworKit not available - using NetworkX only (slower for large graphs)\")\n",
    "\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"NetworKit available: {HAS_NETWORKIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "All parameters are configurable. Adjust the collector, date range, and processing mode below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================",
    "# CONFIGURATION - Modify these parameters as needed",
    "# ============================================================================",
    "",
    "# --- RIPE RIS Collector ---",
    "COLLECTOR = \"rrc04\"",
    "",
    "# --- Date Range ---",
    "START_DATE = \"2025-11-17\"  # YYYY-MM-DD",
    "END_DATE = \"2025-11-18\"    # YYYY-MM-DD (inclusive)",
    "",
    "# --- Output Directories ---",
    "BASE_DIR = Path(\"./bgp_graph_features\")",
    "DATA_DIR = BASE_DIR / \"data\"",
    "OUTPUT_DIR = BASE_DIR / \"output\"",
    "FIGURES_DIR = BASE_DIR / \"figures\"",
    "",
    "for d in [DATA_DIR, OUTPUT_DIR, FIGURES_DIR]:",
    "    d.mkdir(parents=True, exist_ok=True)",
    "",
    "# --- Private / Reserved ASN Ranges (to be filtered out) ---",
    "# RFC 6996: 64512-65534 (16-bit private), 4200000000-4294967294 (32-bit private)",
    "# RFC 7300: 65535 (last 16-bit), 4294967295 (last 32-bit)",
    "# IANA reserved: 0, 23456 (AS_TRANS, RFC 6793)",
    "PRIVATE_ASNS = (",
    "    set(range(64512, 65535))              # 16-bit private",
    "    | set(range(4200000000, 4294967295))  # 32-bit private",
    "    | {0, 23456, 65535, 4294967295}       # reserved / special",
    ")",
    "",
    "def is_valid_public_asn(asn: int) -> bool:",
    "    \"\"\"Return True if the ASN is a valid public (non-private, non-reserved) ASN.\"\"\"",
    "    return asn not in PRIVATE_ASNS",
    "",
    "# --- Performance Settings ---",
    "BETWEENNESS_SAMPLE_K = 500",
    "COMPUTE_SPECTRAL = True",
    "MAX_NODES_FOR_CLIQUE = 5000",
    "",
    "# --- RIPE RIS URL Pattern ---",
    "RIPE_BASE_URL = \"https://data.ris.ripe.net\"",
    "",
    "print(f\"Configuration:\")",
    "print(f\"  Collector: {COLLECTOR}\")",
    "print(f\"  Date range: {START_DATE} to {END_DATE}\")",
    "print(f\"  Output: {OUTPUT_DIR}\")",
    "print(f\"  Private ASN filter: {len(PRIVATE_ASNS):,} ASNs will be excluded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Discovery & Download",
    "",
    "Uses BGPKIT Broker to discover available MRT files for the configured collector and time range,",
    "then constructs direct URLs for RIB dumps (`bview.*`).",
    "",
    "**RIPE RIS URL pattern:**",
    "```",
    "https://data.ris.ripe.net/{collector}/{YYYY.MM}/bview.{YYYYMMDD}.{HHMM}.gz",
    "```",
    "",
    "RIB dumps are generated every **8 hours** at 00:00, 08:00, 16:00 UTC."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_rib_urls(collector: str, start_date: str, end_date: str) -> List[str]:",
    "    \"\"\"",
    "    Generate URLs for all RIB dump files in the given date range.",
    "    RIB dumps are available at 00:00, 08:00, 16:00 UTC daily.",
    "    \"\"\"",
    "    urls = []",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")",
    "    rib_hours = [0, 8, 16]",
    "",
    "    current = start",
    "    while current <= end:",
    "        year_month = current.strftime(\"%Y.%m\")",
    "        for hour in rib_hours:",
    "            ts = current.replace(hour=hour, minute=0)",
    "            if ts < start or ts > end + timedelta(days=1) - timedelta(seconds=1):",
    "                continue",
    "            filename = f\"bview.{ts.strftime('%Y%m%d.%H%M')}.gz\"",
    "            url = f\"{RIPE_BASE_URL}/{collector}/{year_month}/{filename}\"",
    "            urls.append(url)",
    "        current += timedelta(days=1)",
    "",
    "    return urls",
    "",
    "",
    "def discover_files_via_broker(collector: str, start_date: str, end_date: str) -> List[dict]:",
    "    \"\"\"",
    "    Use BGPKIT Broker to discover available RIB MRT files.",
    "    Falls back to URL generation if Broker is unavailable.",
    "    \"\"\"",
    "    try:",
    "        broker = bgpkit.Broker()",
    "        items = broker.query(",
    "            ts_start=f\"{start_date}T00:00:00\",",
    "            ts_end=f\"{end_date}T23:59:59\",",
    "            collector_id=collector,",
    "            data_type=\"rib\"",
    "        )",
    "        if items:",
    "            logger.info(f\"Broker found {len(items)} RIB files\")",
    "            return items",
    "    except Exception as e:",
    "        logger.warning(f\"Broker query failed: {e}. Falling back to URL generation.\")",
    "",
    "    urls = generate_rib_urls(collector, start_date, end_date)",
    "    logger.info(f\"Generated {len(urls)} RIB URLs\")",
    "    return [{\"url\": url} for url in urls]",
    "",
    "",
    "# Discover RIB files",
    "rib_files = discover_files_via_broker(COLLECTOR, START_DATE, END_DATE)",
    "print(f\"\\nRIB files discovered: {len(rib_files)}\")",
    "for f in rib_files[:5]:",
    "    url = f['url'] if isinstance(f, dict) else f.url",
    "    print(f\"  {url}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. MRT Parsing with bgpkit-parser\n\n**bgpkit-parser** is a Rust-based MRT parser with Python bindings (`pybgpkit`).  \nIt handles gzip decompression and MRT parsing transparently.\n\n### Workflow (CSV-first approach)\n1. **Download** MRT `.gz` files locally (saved to `data/mrt_files/`)\n2. **Parse** each file into structured rows matching the standard TABLE_DUMP2 format\n3. **Save** all rows as a single CSV (saved to `output/`)\n4. **Build** AS edges from the CSV for graph construction\n\n### CSV Schema (TABLE_DUMP2 format for RIB entries)\n\n| Column | Description | Example |\n|--------|-------------|---------|\n| `MRT_Type` | Always `TABLE_DUMP2` for RIB dumps | `TABLE_DUMP2` |\n| `Timestamp` | Dump timestamp (UTC) | `2025-11-17 00:00:00` |\n| `Entry_Type` | Always `B` (table entry) for RIB | `B` |\n| `Peer_IP` | IP of the BGP peer | `198.32.132.97` |\n| `Peer_AS` | ASN of the peer | `13335` |\n| `Prefix` | Announced prefix | `1.0.0.0/24` |\n| `AS_Path` | Full AS path (space-separated) | `3356 1299 13335` |\n| `Origin` | ORIGIN attribute (IGP/EGP/INCOMPLETE) | `IGP` |\n| `Next_Hop` | Next-hop IP | `198.32.132.97` |\n| `Local_Pref` | LOCAL_PREF value | `100` |\n| `MED` | Multi-Exit Discriminator | `0` |\n| `Community` | BGP communities (space-separated) | `3356:100 3356:123` |\n| `Atomic_Aggregate` | Atomic aggregate flag | `AG` or empty |\n| `Aggregator` | Aggregator AS and IP | `13335 198.32.132.97` |\n\n**Note:** RIB dumps use `TABLE_DUMP2` with entry type `B`, unlike UPDATE files which use `BGP4MP` with `A` (announcement) and `W` (withdrawal).\n\n### Graph construction from AS_PATH\n1. Parse AS_PATH string (space-separated ASNs)\n2. Remove AS prepending (consecutive duplicates)\n3. Skip AS_SET entries (e.g., `{1234,5678}`)\n4. Filter private/reserved ASNs (RFC 6996, RFC 7300)\n5. Extract pairwise adjacent links -> undirected edges"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import urllib.request\n\ndef parse_as_path(as_path_str: str) -> List[int]:\n    \"\"\"\n    Parse an AS_PATH string into a deduplicated list of valid public ASNs.\n    Handles:\n    - Standard AS paths: \"3356 1299 13335\"\n    - AS prepending: \"3356 3356 3356 1299\" -> [3356, 1299]\n    - AS_SETs: \"{1234,5678}\" -> skipped entirely\n    - Private/reserved ASNs: filtered out (RFC 6996, RFC 7300)\n    \"\"\"\n    if not as_path_str:\n        return []\n\n    tokens = as_path_str.split()\n    deduped = []\n    for token in tokens:\n        if '{' in token or '}' in token:\n            continue\n        try:\n            asn = int(token)\n            if not is_valid_public_asn(asn):\n                continue\n            if not deduped or asn != deduped[-1]:\n                deduped.append(asn)\n        except ValueError:\n            continue\n    return deduped\n\n\ndef extract_edges_from_as_path(as_path: List[int]) -> List[Tuple[int, int]]:\n    \"\"\"\n    Extract pairwise AS adjacency edges from a parsed AS path.\n    Returns a list (not set) so callers can count occurrences for weighting.\n    \"\"\"\n    edges = []\n    for i in range(len(as_path) - 1):\n        edge = tuple(sorted([as_path[i], as_path[i + 1]]))\n        if edge[0] != edge[1]:\n            edges.append(edge)\n    return edges\n\n\ndef download_mrt_file(url: str, dest_dir: Path) -> Path:\n    \"\"\"Download an MRT file and save it locally. Returns the local file path.\"\"\"\n    filename = url.split('/')[-1]\n    local_path = dest_dir / filename\n\n    if local_path.exists():\n        size_mb = local_path.stat().st_size / (1024 * 1024)\n        logger.info(f\"  Already downloaded: {filename} ({size_mb:.1f} MB)\")\n        return local_path\n\n    logger.info(f\"  Downloading: {filename}\")\n    t0 = time.time()\n    urllib.request.urlretrieve(url, str(local_path))\n    elapsed = time.time() - t0\n    size_mb = local_path.stat().st_size / (1024 * 1024)\n    logger.info(f\"  Saved: {filename} ({size_mb:.1f} MB, {elapsed:.1f}s)\")\n    return local_path\n\n\ndef parse_mrt_to_rows(file_path: str) -> Tuple[List[dict], dict]:\n    \"\"\"\n    Parse a single MRT RIB dump file into structured TABLE_DUMP2 rows.\n\n    Each RIB entry is mapped to:\n        TABLE_DUMP2|timestamp|B|peer_ip|peer_as|prefix|as_path|origin|\n        next_hop|local_pref|med|community|atomic_agg|aggregator\n\n    Returns:\n        rows: list of dicts (one per RIB entry)\n        stats: parsing statistics\n    \"\"\"\n    rows = []\n    stats = {\n        'total_elements': 0,\n        'announcements': 0,\n        'withdrawals': 0,\n        'unique_prefixes': set(),\n        'unique_peers': set(),\n        'parse_errors': 0,\n    }\n\n    logger.info(f\"  Parsing: {Path(file_path).name}\")\n    t0 = time.time()\n\n    try:\n        parser = bgpkit.Parser(url=str(file_path))\n        for elem in parser:\n            stats['total_elements'] += 1\n\n            if elem.elem_type == \"W\":\n                stats['withdrawals'] += 1\n                continue\n\n            stats['announcements'] += 1\n\n            if elem.prefix:\n                stats['unique_prefixes'].add(elem.prefix)\n            if elem.peer_asn:\n                stats['unique_peers'].add(elem.peer_asn)\n\n            # Convert timestamp to readable UTC string\n            ts = datetime.fromtimestamp(\n                elem.timestamp, tz=timezone.utc\n            ).strftime('%Y-%m-%d %H:%M:%S')\n\n            # Build community string (space-separated)\n            communities = ''\n            if elem.communities:\n                communities = ' '.join(str(c) for c in elem.communities)\n\n            # Build aggregator string\n            aggregator = ''\n            aggr_asn = getattr(elem, 'aggr_asn', None)\n            aggr_ip = getattr(elem, 'aggr_ip', None)\n            if aggr_asn:\n                aggregator = f\"{aggr_asn} {aggr_ip}\".strip() if aggr_ip else str(aggr_asn)\n\n            row = {\n                'MRT_Type': 'TABLE_DUMP2',\n                'Timestamp': ts,\n                'Entry_Type': 'B',\n                'Peer_IP': elem.peer_ip or '',\n                'Peer_AS': elem.peer_asn if elem.peer_asn else '',\n                'Prefix': elem.prefix or '',\n                'AS_Path': elem.as_path or '',\n                'Origin': elem.origin or '',\n                'Next_Hop': elem.next_hop or '',\n                'Local_Pref': elem.local_pref if elem.local_pref is not None else '',\n                'MED': elem.med if elem.med is not None else '',\n                'Community': communities,\n                'Atomic_Aggregate': 'AG' if elem.atomic else '',\n                'Aggregator': aggregator,\n            }\n            rows.append(row)\n\n    except Exception as e:\n        logger.error(f\"Error parsing {file_path}: {e}\")\n        stats['parse_errors'] += 1\n\n    elapsed = time.time() - t0\n    stats['unique_prefixes'] = len(stats['unique_prefixes'])\n    stats['unique_peers'] = len(stats['unique_peers'])\n    stats['rows_parsed'] = len(rows)\n    stats['parse_time_sec'] = round(elapsed, 2)\n\n    logger.info(\n        f\"  -> {stats['total_elements']:,} elements, \"\n        f\"{len(rows):,} rows, {stats['unique_prefixes']:,} prefixes, \"\n        f\"{stats['unique_peers']:,} peers in {elapsed:.1f}s\"\n    )\n    return rows, stats\n\n\nprint(\"Parse functions defined:\")\nprint(\"  - parse_as_path(as_path_str) -> List[int]\")\nprint(\"  - extract_edges_from_as_path(as_path) -> List[Tuple[int,int]]\")\nprint(\"  - download_mrt_file(url, dest_dir) -> Path\")\nprint(\"  - parse_mrt_to_rows(file_path) -> (rows, stats)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================================\n# STEP 1: Download MRT files locally & parse to structured CSV\n# ============================================================================\n# This cell downloads the raw MRT .gz files and parses every RIB entry\n# into a CSV with TABLE_DUMP2 format. Both artifacts are saved to disk.\n# Re-running this cell will skip already-downloaded files.\n# ============================================================================\n\nMRT_DIR = DATA_DIR / \"mrt_files\"\nMRT_DIR.mkdir(parents=True, exist_ok=True)\n\nall_rows = []\nall_stats = []\n\nprint(\"=\" * 70)\nprint(\"STEP 1: DOWNLOAD MRT FILES & PARSE TO CSV\")\nprint(\"=\" * 70)\n\nfor i, f in enumerate(rib_files):\n    url = f['url'] if isinstance(f, dict) else f.url\n    print(f\"\\n[{i+1}/{len(rib_files)}] {url}\")\n\n    # --- Download MRT file locally ---\n    local_path = download_mrt_file(url, MRT_DIR)\n\n    # --- Parse to structured rows ---\n    rows, stats = parse_mrt_to_rows(str(local_path))\n    stats['file_type'] = 'rib'\n    stats['url'] = url\n    stats['local_path'] = str(local_path)\n    all_rows.extend(rows)\n    all_stats.append(stats)\n    print(f\"  Running total: {len(all_rows):,} rows\")\n\n# --- Save combined CSV ---\ncsv_filename = f\"rib_parsed_{COLLECTOR}_{START_DATE}_{END_DATE}.csv\"\ncsv_path = OUTPUT_DIR / csv_filename\nprint(f\"\\nSaving parsed CSV to {csv_path}...\")\nrib_df = pd.DataFrame(all_rows)\nrib_df.to_csv(csv_path, index=False)\ncsv_size_mb = csv_path.stat().st_size / (1024 * 1024)\n\n# --- Save parsing stats ---\nstats_df = pd.DataFrame(all_stats)\nstats_df.to_csv(OUTPUT_DIR / \"parsing_stats.csv\", index=False)\n\nprint(f\"\\n{'=' * 70}\")\nprint(f\"DOWNLOAD & PARSE COMPLETE\")\nprint(f\"{'=' * 70}\")\nprint(f\"  MRT files saved to: {MRT_DIR}/\")\nfor mrt_file in sorted(MRT_DIR.iterdir()):\n    sz = mrt_file.stat().st_size / (1024 * 1024)\n    print(f\"    {mrt_file.name:<40} {sz:.1f} MB\")\nprint(f\"  CSV saved to: {csv_path} ({csv_size_mb:.1f} MB)\")\nprint(f\"  Total rows: {len(rib_df):,}\")\nprint(f\"  Columns: {list(rib_df.columns)}\")\nprint(f\"  Files processed: {len(all_stats)}\")\nprint(f\"{'=' * 70}\")\n\n# Preview first rows\nrib_df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# STEP 2: Load CSV & build AS topology edges\n# ============================================================================\n# This cell can run independently — if rib_df is not in memory, it loads\n# the CSV from disk. This means you only parse MRT files once.\n# ============================================================================\nfrom collections import Counter\n\ncsv_path = OUTPUT_DIR / f\"rib_parsed_{COLLECTOR}_{START_DATE}_{END_DATE}.csv\"\n\nif 'rib_df' not in dir() or rib_df is None:\n    print(f\"Loading CSV from {csv_path}...\")\n    rib_df = pd.read_csv(csv_path)\n\nprint(f\"Building AS topology edges from {len(rib_df):,} RIB entries...\")\nprint(\"=\" * 70)\n\n# --- Check for private/reserved ASNs in raw data ---\nprivate_found = set()\nall_edges_list = []\n\nfor as_path_raw in rib_df['AS_Path'].dropna().astype(str):\n    if as_path_raw == '' or as_path_raw == 'nan':\n        continue\n\n    # Check for private ASNs (before filtering)\n    for token in as_path_raw.split():\n        if '{' in token or '}' in token:\n            continue\n        try:\n            asn = int(token)\n            if asn in PRIVATE_ASNS:\n                private_found.add(asn)\n        except ValueError:\n            pass\n\n    # Parse AS path (filters private ASNs + removes prepending)\n    as_path = parse_as_path(as_path_raw)\n    edges = extract_edges_from_as_path(as_path)\n    all_edges_list.extend(edges)\n\n# --- Report private ASN findings ---\nprint(f\"\\nPrivate/reserved ASN check:\")\nif private_found:\n    print(f\"  Found {len(private_found)} private/reserved ASNs in raw data:\")\n    for asn in sorted(private_found):\n        label = \"AS_TRANS\" if asn == 23456 else \"private\" if asn >= 64512 else \"reserved\"\n        print(f\"    AS{asn} ({label})\")\n    print(f\"  These have been filtered out of the edge set.\")\nelse:\n    print(f\"  None found — all ASNs in the data are public. Filter had no effect.\")\n\n# --- Build edge weights from observation counts ---\nedge_counts = Counter(all_edges_list)\nall_edges = set(edge_counts.keys())\n\n# --- Summary ---\nprint(f\"\\n{'=' * 70}\")\nprint(f\"EDGE EXTRACTION COMPLETE\")\nprint(f\"  Total unique edges: {len(all_edges):,}\")\nprint(f\"  Total edge observations: {len(all_edges_list):,}\")\nif edge_counts:\n    weights = sorted(edge_counts.values())\n    print(f\"  Weight range: min={weights[0]:,}, \"\n          f\"max={weights[-1]:,}, \"\n          f\"median={weights[len(weights)//2]:,}\")\nprint(f\"{'=' * 70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AS-Level Graph Construction\n",
    "\n",
    "Build an undirected NetworkX graph from the extracted AS adjacency pairs.\n",
    "Each node represents an Autonomous System (ASN) and each edge represents\n",
    "an observed routing adjacency in BGP AS_PATH attributes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_as_graph(edges: Set[Tuple[int, int]],",
    "                   edge_weights: dict = None) -> nx.Graph:",
    "    \"\"\"",
    "    Construct an undirected AS-level topology graph from adjacency pairs.",
    "    Optionally attach edge weights (number of AS-path observations).",
    "    \"\"\"",
    "    G = nx.Graph()",
    "    if edge_weights:",
    "        for (u, v), w in edge_weights.items():",
    "            G.add_edge(u, v, weight=w)",
    "    else:",
    "        G.add_edges_from(edges)",
    "",
    "    G.graph['name'] = f\"AS-level topology ({COLLECTOR})\"",
    "    G.graph['collector'] = COLLECTOR",
    "    G.graph['start_date'] = START_DATE",
    "    G.graph['end_date'] = END_DATE",
    "    G.graph['created'] = datetime.now(timezone.utc).isoformat()",
    "",
    "    return G",
    "",
    "",
    "# Build the graph with edge weights",
    "G = build_as_graph(all_edges, edge_weights=edge_counts)",
    "",
    "print(f\"AS-Level Topology Graph:\")",
    "print(f\"  Nodes (ASes): {G.number_of_nodes():,}\")",
    "print(f\"  Edges (links): {G.number_of_edges():,}\")",
    "print(f\"  Connected: {nx.is_connected(G)}\")",
    "",
    "if not nx.is_connected(G):",
    "    components = list(nx.connected_components(G))",
    "    sizes = sorted([len(c) for c in components], reverse=True)",
    "    print(f\"  Connected components: {len(components)}\")",
    "    print(f\"  Largest component: {sizes[0]:,} nodes ({100*sizes[0]/G.number_of_nodes():.1f}%)\")",
    "    if len(sizes) > 1:",
    "        print(f\"  2nd largest: {sizes[1]:,} nodes\")",
    "    print(f\"  Components with 1 node: {sizes.count(1)}\")",
    "",
    "    largest_cc = max(components, key=len)",
    "    G_lcc = G.subgraph(largest_cc).copy()",
    "    print(f\"\\n  -> Using largest connected component for analysis\")",
    "else:",
    "    G_lcc = G",
    "",
    "print(f\"\\nAnalysis graph: {G_lcc.number_of_nodes():,} nodes, {G_lcc.number_of_edges():,} edges\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic degree distribution overview\n",
    "degrees = [d for _, d in G_lcc.degree()]\n",
    "degree_series = pd.Series(degrees)\n",
    "\n",
    "print(\"Degree distribution statistics:\")\n",
    "print(f\"  Min: {degree_series.min()}\")\n",
    "print(f\"  Max: {degree_series.max():,}\")\n",
    "print(f\"  Mean: {degree_series.mean():.2f}\")\n",
    "print(f\"  Median: {degree_series.median():.1f}\")\n",
    "print(f\"  Std: {degree_series.std():.2f}\")\n",
    "print(f\"  Skewness: {degree_series.skew():.2f}\")\n",
    "\n",
    "# Quick degree distribution plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear histogram\n",
    "axes[0].hist(degrees, bins=100, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Degree')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Degree Distribution (Linear Scale)')\n",
    "axes[0].axvline(x=degree_series.mean(), color='red', linestyle='--', label=f'Mean={degree_series.mean():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Log-log CCDF\n",
    "sorted_deg = np.sort(degrees)[::-1]\n",
    "ccdf = np.arange(1, len(sorted_deg) + 1) / len(sorted_deg)\n",
    "axes[1].loglog(sorted_deg, ccdf, '.', markersize=3, color='steelblue')\n",
    "axes[1].set_xlabel('Degree k')\n",
    "axes[1].set_ylabel('P(X ≥ k)')\n",
    "axes[1].set_title('Degree CCDF (Log-Log Scale)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'degree_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'degree_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Graph-Level Feature Extraction\n",
    "\n",
    "Extract 16 graph-level metrics with proper mathematical definitions and academic citations.\n",
    "\n",
    "Each metric includes:\n",
    "- **Definition**: Mathematical formula\n",
    "- **Interpretation**: What it captures in AS topology\n",
    "- **Citation**: Authoritative academic reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Helper: convert NetworkX → NetworKit (if available)\n",
    "# ============================================================================\n",
    "\n",
    "def nx_to_nk(G_nx: nx.Graph):\n",
    "    \"\"\"\n",
    "    Convert a NetworkX graph to NetworKit format.\n",
    "    Returns (nk_graph, node_mapping) where node_mapping maps NX→NK node IDs.\n",
    "    \"\"\"\n",
    "    if not HAS_NETWORKIT:\n",
    "        return None, None\n",
    "    \n",
    "    # NetworKit requires contiguous integer node IDs starting from 0\n",
    "    node_list = sorted(G_nx.nodes())\n",
    "    nx_to_nk_map = {n: i for i, n in enumerate(node_list)}\n",
    "    nk_to_nx_map = {i: n for n, i in nx_to_nk_map.items()}\n",
    "    \n",
    "    G_nk = nk.Graph(len(node_list), weighted=False, directed=False)\n",
    "    for u, v in G_nx.edges():\n",
    "        G_nk.addEdge(nx_to_nk_map[u], nx_to_nk_map[v])\n",
    "    \n",
    "    return G_nk, nx_to_nk_map, nk_to_nx_map\n",
    "\n",
    "\n",
    "# Pre-convert if NetworKit is available\n",
    "if HAS_NETWORKIT:\n",
    "    G_nk, nx2nk_map, nk2nx_map = nx_to_nk(G_lcc)\n",
    "    print(f\"NetworKit graph: {G_nk.numberOfNodes()} nodes, {G_nk.numberOfEdges()} edges\")\n",
    "else:\n",
    "    G_nk, nx2nk_map, nk2nx_map = None, None, None\n",
    "    print(\"Using NetworkX only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Graph-Level Feature Extraction\n",
    "# ============================================================================\n",
    "\n",
    "graph_features = {}\n",
    "n_nodes = G_lcc.number_of_nodes()\n",
    "n_edges = G_lcc.number_of_edges()\n",
    "\n",
    "graph_features['n_nodes'] = n_nodes\n",
    "graph_features['n_edges'] = n_edges\n",
    "\n",
    "print(f\"Extracting graph-level features for {n_nodes:,} nodes, {n_edges:,} edges...\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. ASSORTATIVITY (Degree Assortativity Coefficient)\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: Pearson correlation of degrees at either end of each edge.\n",
    "#   r = [M^-1 Σ_i j_i k_i - (M^-1 Σ_i ½(j_i + k_i))^2] /\n",
    "#       [M^-1 Σ_i ½(j_i² + k_i²) - (M^-1 Σ_i ½(j_i + k_i))^2]\n",
    "# Range: [-1, 1]. Internet AS graphs are typically disassortative (r < 0).\n",
    "# Citation: Newman, M.E.J. \"Assortative Mixing in Networks,\"\n",
    "#           Physical Review Letters 89, 208701 (2002).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "graph_features['assortativity'] = nx.degree_assortativity_coefficient(G_lcc)\n",
    "print(f\"[1/16] Assortativity: {graph_features['assortativity']:.6f}  ({time.time()-t0:.1f}s)\")\n",
    "print(f\"        → {'Disassortative' if graph_features['assortativity'] < 0 else 'Assortative'} \"\n",
    "      f\"(high-degree nodes preferentially connect to {'low' if graph_features['assortativity'] < 0 else 'high'}-degree nodes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 2. DENSITY\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: ρ(G) = 2|E| / [|V|(|V|-1)]\n",
    "# Internet AS graphs are extremely sparse (~10^-5).\n",
    "# Citation: Standard graph theory definition.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "graph_features['density'] = nx.density(G_lcc)\n",
    "print(f\"[2/16] Density: {graph_features['density']:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 3. CLUSTERING COEFFICIENT (Global & Average Local)\n",
    "# --------------------------------------------------------------------------\n",
    "# Global (transitivity): C_global = 3 × triangles / connected_triples\n",
    "# Average local: C̄ = (1/n) Σ_v C(v), where\n",
    "#   C(v) = 2|{edges among N(v)}| / [d_v(d_v - 1)]\n",
    "# Higher than random graphs → regional peering communities, IXP cliques.\n",
    "# Citation: Watts, D.J. & Strogatz, S.H. \"Collective dynamics of 'small-world'\n",
    "#           networks,\" Nature 393, 440-442 (1998).\n",
    "#           Newman, M.E.J. \"The structure and function of complex networks,\"\n",
    "#           SIAM Review 45(2), 167-256 (2003).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "if HAS_NETWORKIT:\n",
    "    graph_features['clustering_global'] = nk.globals.ClusteringCoefficient.exactGlobal(G_nk)\n",
    "    graph_features['clustering_avg_local'] = nk.globals.ClusteringCoefficient.sequentialAvgLocal(G_nk)\n",
    "else:\n",
    "    graph_features['clustering_global'] = nx.transitivity(G_lcc)\n",
    "    graph_features['clustering_avg_local'] = nx.average_clustering(G_lcc)\n",
    "\n",
    "print(f\"[3/16] Clustering coefficient  ({time.time()-t0:.1f}s)\")\n",
    "print(f\"        Global (transitivity): {graph_features['clustering_global']:.6f}\")\n",
    "print(f\"        Average local: {graph_features['clustering_avg_local']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 4. DIAMETER & AVERAGE PATH LENGTH\n",
    "# --------------------------------------------------------------------------\n",
    "# Diameter: D = max_{u,v} d(u,v) (longest shortest path)\n",
    "# Average path length: ℓ = 2/[n(n-1)] · Σ_{u<v} d(u,v)\n",
    "# Internet AS graph: D ≈ 8-11, ℓ ≈ 3-4 hops (small-world).\n",
    "# Citation: Watts, D.J. & Strogatz, S.H. Nature 393 (1998).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    # NetworKit's iFUB algorithm is near-linear for real-world networks\n",
    "    diam_algo = nk.distance.Diameter(G_nk, algo=nk.distance.DiameterAlgo.AUTOMATIC)\n",
    "    diam_algo.run()\n",
    "    graph_features['diameter'] = diam_algo.getDiameter()[0]\n",
    "    print(f\"[4/16] Diameter: {graph_features['diameter']}  ({time.time()-t0:.1f}s, NetworKit iFUB)\")\n",
    "else:\n",
    "    # NetworkX: only feasible for smaller graphs\n",
    "    if n_nodes < 50000:\n",
    "        graph_features['diameter'] = nx.diameter(G_lcc)\n",
    "        print(f\"[4/16] Diameter: {graph_features['diameter']}  ({time.time()-t0:.1f}s)\")\n",
    "    else:\n",
    "        # Approximate via BFS from random nodes\n",
    "        sample_nodes = np.random.choice(list(G_lcc.nodes()), size=min(100, n_nodes), replace=False)\n",
    "        max_ecc = 0\n",
    "        for node in sample_nodes:\n",
    "            ecc = nx.eccentricity(G_lcc, v=node)\n",
    "            max_ecc = max(max_ecc, ecc)\n",
    "        graph_features['diameter'] = max_ecc\n",
    "        print(f\"[4/16] Diameter (approx, 100 samples): {graph_features['diameter']}  ({time.time()-t0:.1f}s)\")\n",
    "\n",
    "# Average path length: expensive, sample-based for large graphs\n",
    "t1 = time.time()\n",
    "if n_nodes < 20000:\n",
    "    graph_features['avg_path_length'] = nx.average_shortest_path_length(G_lcc)\n",
    "    print(f\"        Avg path length: {graph_features['avg_path_length']:.4f}  ({time.time()-t1:.1f}s)\")\n",
    "else:\n",
    "    # Sample-based estimation\n",
    "    sample_size = min(500, n_nodes)\n",
    "    sample_nodes = np.random.choice(list(G_lcc.nodes()), size=sample_size, replace=False)\n",
    "    total_dist = 0\n",
    "    count = 0\n",
    "    for node in sample_nodes:\n",
    "        lengths = nx.single_source_shortest_path_length(G_lcc, node)\n",
    "        total_dist += sum(lengths.values())\n",
    "        count += len(lengths) - 1  # exclude self\n",
    "    graph_features['avg_path_length'] = total_dist / count if count > 0 else float('inf')\n",
    "    print(f\"        Avg path length (sampled, {sample_size} sources): \"\n",
    "          f\"{graph_features['avg_path_length']:.4f}  ({time.time()-t1:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 5. ALGEBRAIC CONNECTIVITY (Fiedler Value)\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: Second smallest eigenvalue of the Laplacian L = D - A:\n",
    "#   a(G) = μ₂(L),  0 = μ₁ ≤ μ₂ ≤ ... ≤ μ_n\n",
    "# μ₂ > 0 iff G is connected. Bounds: μ₂ ≤ κ_v(G) ≤ κ_e(G).\n",
    "# Higher values → harder to partition.\n",
    "# For large graphs: use scipy.sparse.linalg.eigsh with k=2, which='SM'.\n",
    "# Citation: Fiedler, M. \"Algebraic connectivity of graphs,\"\n",
    "#           Czechoslovak Mathematical Journal 23, 298-305 (1973).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if n_nodes < 30000:\n",
    "    try:\n",
    "        graph_features['algebraic_connectivity'] = nx.algebraic_connectivity(\n",
    "            G_lcc, method='tracemin_pcg'\n",
    "        )\n",
    "        print(f\"[5/16] Algebraic connectivity: {graph_features['algebraic_connectivity']:.6f}  ({time.time()-t0:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[5/16] Algebraic connectivity: FAILED ({e})\")\n",
    "        graph_features['algebraic_connectivity'] = None\n",
    "else:\n",
    "    # Use sparse eigenvalue solver for large graphs\n",
    "    try:\n",
    "        L_sparse = nx.laplacian_matrix(G_lcc).astype(float)\n",
    "        # Compute 2 smallest eigenvalues\n",
    "        eigenvalues = eigsh(L_sparse, k=2, which='SM', return_eigenvectors=False)\n",
    "        graph_features['algebraic_connectivity'] = float(np.sort(eigenvalues)[1])\n",
    "        print(f\"[5/16] Algebraic connectivity: {graph_features['algebraic_connectivity']:.6f}  \"\n",
    "              f\"({time.time()-t0:.1f}s, sparse eigsh)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[5/16] Algebraic connectivity: FAILED ({e})\")\n",
    "        graph_features['algebraic_connectivity'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 6. SPECTRAL RADIUS (Largest Eigenvalue of Adjacency Matrix)\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: λ₁ = max_i |λ_i(A)|\n",
    "# By Perron-Frobenius: λ₁ is positive and real for connected graphs.\n",
    "# Bounds: d̄ ≤ λ₁ ≤ d_max. Governs epidemic spreading dynamics.\n",
    "# Citation: Cvetković, D. et al. \"An Introduction to the Theory of Graph\n",
    "#           Spectra,\" Cambridge University Press (2010).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "try:\n",
    "    A_sparse = nx.adjacency_matrix(G_lcc).astype(float)\n",
    "    spectral_radius_vals = eigsh(A_sparse, k=1, which='LM', return_eigenvectors=False)\n",
    "    graph_features['spectral_radius'] = float(spectral_radius_vals[0])\n",
    "    print(f\"[6/16] Spectral radius: {graph_features['spectral_radius']:.4f}  ({time.time()-t0:.1f}s)\")\n",
    "except Exception as e:\n",
    "    print(f\"[6/16] Spectral radius: FAILED ({e})\")\n",
    "    graph_features['spectral_radius'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 7. PERCOLATION LIMIT / EPIDEMIC THRESHOLD\n",
    "# --------------------------------------------------------------------------\n",
    "# For the SIS epidemic model: τ_c ≥ 1/λ₁(A)\n",
    "# Epidemic persists if β/δ > 1/λ₁, dies out below.\n",
    "# Scale-free AS topologies have vanishing threshold as n → ∞.\n",
    "# Citation: Pastor-Satorras, R. & Vespignani, A. \"Epidemic Spreading in\n",
    "#           Scale-Free Networks,\" Phys. Rev. Lett. 86, 3200-3203 (2001).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "if graph_features.get('spectral_radius'):\n",
    "    graph_features['percolation_limit'] = 1.0 / graph_features['spectral_radius']\n",
    "    print(f\"[7/16] Percolation limit: {graph_features['percolation_limit']:.6f}\")\n",
    "    print(f\"        → Worms/epidemics can spread at infection rates above {graph_features['percolation_limit']:.4f}\")\n",
    "else:\n",
    "    graph_features['percolation_limit'] = None\n",
    "    print(f\"[7/16] Percolation limit: SKIPPED (requires spectral radius)\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------",
    "# 8-11. SPECTRAL METRICS (Conditional on COMPUTE_SPECTRAL flag)",
    "# --------------------------------------------------------------------------",
    "",
    "if COMPUTE_SPECTRAL:",
    "    t0 = time.time()",
    "",
    "    # Compute adjacency & Laplacian matrices ONCE and reuse",
    "    L_sparse = nx.laplacian_matrix(G_lcc).astype(float)",
    "    A_sparse = nx.adjacency_matrix(G_lcc).astype(float)",
    "",
    "    n_eigs = min(n_nodes - 2, 300)",
    "    use_full_spectrum = n_nodes < 5000",
    "",
    "    if use_full_spectrum:",
    "        print(f\"Computing full spectrum ({n_nodes} eigenvalues)...\")",
    "        L_dense = L_sparse.toarray()",
    "        A_dense = A_sparse.toarray()",
    "        laplacian_eigs = np.sort(np.real(np.linalg.eigvalsh(L_dense)))",
    "        adjacency_eigs = np.sort(np.real(np.linalg.eigvalsh(A_dense)))[::-1]",
    "    else:",
    "        print(f\"Computing partial spectrum ({n_eigs} eigenvalues)...\")",
    "        laplacian_eigs_small = eigsh(L_sparse, k=min(n_eigs, n_nodes-2),",
    "                                     which='SM', return_eigenvectors=False)",
    "        laplacian_eigs = np.sort(laplacian_eigs_small)",
    "",
    "        adjacency_eigs_large = eigsh(A_sparse, k=min(n_eigs, n_nodes-2),",
    "                                     which='LM', return_eigenvectors=False)",
    "        adjacency_eigs = np.sort(adjacency_eigs_large)[::-1]",
    "",
    "    print(f\"  Spectrum computation: {time.time()-t0:.1f}s\")",
    "",
    "    # ------------------------------------------------------------------",
    "    # 8. SYMMETRY RATIO",
    "    # SR(G) = |distinct eigenvalues of A| / (D + 1)",
    "    # FIX: For partial spectrum, flag as lower bound.",
    "    # Citation: Dekker, CATS (2005).",
    "    # ------------------------------------------------------------------",
    "    distinct_eigs = len(np.unique(np.round(adjacency_eigs, 8)))",
    "    D = graph_features.get('diameter', 10)",
    "",
    "    if use_full_spectrum:",
    "        graph_features['symmetry_ratio'] = distinct_eigs / (D + 1)",
    "        print(f\"[8/16] Symmetry ratio: {graph_features['symmetry_ratio']:.4f} \"",
    "              f\"({distinct_eigs} distinct eigs / {D+1})\")",
    "    else:",
    "        graph_features['symmetry_ratio'] = distinct_eigs / (D + 1)",
    "        graph_features['symmetry_ratio_partial'] = True",
    "        print(f\"[8/16] Symmetry ratio (lower bound): {graph_features['symmetry_ratio']:.4f} \"",
    "              f\"({distinct_eigs} distinct of {len(adjacency_eigs)} computed eigs / {D+1})\")",
    "        print(f\"        NOTE: Partial spectrum - true ratio >= this value\")",
    "",
    "    # ------------------------------------------------------------------",
    "    # 9. NATURAL CONNECTIVITY",
    "    # lambda_bar = ln[(1/n) sum exp(lambda_i)]",
    "    # FIX: For partial spectrum, denominator must be n (total nodes),",
    "    # not len(computed eigenvalues), to avoid upward bias.",
    "    # Citation: Wu et al., IEEE Trans. SMC-A 41 (2011).",
    "    # ------------------------------------------------------------------",
    "    if use_full_spectrum:",
    "        max_eig = np.max(adjacency_eigs)",
    "        graph_features['natural_connectivity'] = float(",
    "            max_eig + np.log(np.mean(np.exp(adjacency_eigs - max_eig)))",
    "        )",
    "        print(f\"[9/16] Natural connectivity: {graph_features['natural_connectivity']:.4f}\")",
    "    else:",
    "        max_eig = np.max(adjacency_eigs)",
    "        shifted = np.exp(adjacency_eigs - max_eig)",
    "        # Use n_nodes as denominator, not len(adjacency_eigs)",
    "        graph_features['natural_connectivity'] = float(",
    "            max_eig + np.log(np.sum(shifted) / n_nodes)",
    "        )",
    "        print(f\"[9/16] Natural connectivity (approx, {len(adjacency_eigs)} of {n_nodes} eigs, \"",
    "              f\"denominator=n): {graph_features['natural_connectivity']:.4f}\")",
    "",
    "    # ------------------------------------------------------------------",
    "    # 10. EFFECTIVE GRAPH RESISTANCE (Kirchhoff Index)",
    "    # K_f(G) = n * sum_{k>=2} 1/mu_k",
    "    # Citation: Klein & Randic, J. Math. Chem. 12 (1993).",
    "    # ------------------------------------------------------------------",
    "    nonzero_lap_eigs = laplacian_eigs[laplacian_eigs > 1e-10]",
    "    if len(nonzero_lap_eigs) > 0:",
    "        graph_features['kirchhoff_index'] = float(n_nodes * np.sum(1.0 / nonzero_lap_eigs))",
    "        label = \"\" if use_full_spectrum else f\" (partial, {len(nonzero_lap_eigs)} eigs)\"",
    "        print(f\"[10/16] Kirchhoff index{label}: {graph_features['kirchhoff_index']:.2f}\")",
    "    else:",
    "        graph_features['kirchhoff_index'] = None",
    "        print(f\"[10/16] Kirchhoff index: SKIPPED (no nonzero Laplacian eigenvalues)\")",
    "",
    "    # ------------------------------------------------------------------",
    "    # 11. NUMBER OF SPANNING TREES (requires full spectrum)",
    "    # Kirchhoff's Matrix Tree Theorem: tau(G) = (1/n) prod_{k>=2} mu_k",
    "    # Citation: Kirchhoff (1847); Godsil & Royle, Springer (2001).",
    "    # ------------------------------------------------------------------",
    "    if use_full_spectrum and len(nonzero_lap_eigs) > 0:",
    "        log_spanning_trees = np.sum(np.log(nonzero_lap_eigs)) - np.log(n_nodes)",
    "        graph_features['log_spanning_trees'] = float(log_spanning_trees)",
    "        print(f\"[11/16] log(spanning trees): {graph_features['log_spanning_trees']:.2f}\")",
    "    else:",
    "        graph_features['log_spanning_trees'] = None",
    "        print(f\"[11/16] Spanning trees: SKIPPED (requires full spectrum)\")",
    "",
    "else:",
    "    print(\"[8-11] Spectral metrics SKIPPED (COMPUTE_SPECTRAL=False)\")",
    "    graph_features['symmetry_ratio'] = None",
    "    graph_features['natural_connectivity'] = None",
    "    graph_features['kirchhoff_index'] = None",
    "    graph_features['log_spanning_trees'] = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 12. NODE CONNECTIVITY & EDGE CONNECTIVITY\n",
    "# --------------------------------------------------------------------------\n",
    "# Node connectivity κ_v(G): min vertices to remove to disconnect G.\n",
    "# Edge connectivity κ_e(G): min edges to remove to disconnect G.\n",
    "# Whitney's inequality: κ_v(G) ≤ κ_e(G) ≤ δ(G).\n",
    "# By Menger's theorem: equals max disjoint paths between some pair.\n",
    "# Citation: Whitney, H. \"Congruent graphs and the connectivity of graphs,\"\n",
    "#           American J. Math. 54(1), 150-168 (1932).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Edge connectivity (faster than node connectivity)\n",
    "try:\n",
    "    graph_features['edge_connectivity'] = nx.edge_connectivity(G_lcc)\n",
    "    print(f\"[12/16] Edge connectivity: {graph_features['edge_connectivity']}  ({time.time()-t0:.1f}s)\")\n",
    "except Exception as e:\n",
    "    graph_features['edge_connectivity'] = None\n",
    "    print(f\"[12/16] Edge connectivity: FAILED ({e})\")\n",
    "\n",
    "# Node connectivity\n",
    "t1 = time.time()\n",
    "try:\n",
    "    graph_features['node_connectivity'] = nx.node_connectivity(G_lcc)\n",
    "    print(f\"        Node connectivity: {graph_features['node_connectivity']}  ({time.time()-t1:.1f}s)\")\n",
    "except Exception as e:\n",
    "    graph_features['node_connectivity'] = None\n",
    "    print(f\"        Node connectivity: FAILED ({e})\")\n",
    "\n",
    "min_degree = min(d for _, d in G_lcc.degree())\n",
    "print(f\"        Min degree δ(G): {min_degree}\")\n",
    "print(f\"        Whitney check: κ_v ≤ κ_e ≤ δ → \"\n",
    "      f\"{graph_features.get('node_connectivity','?')} ≤ \"\n",
    "      f\"{graph_features.get('edge_connectivity','?')} ≤ {min_degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 13. RICH-CLUB COEFFICIENT\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: φ(k) = 2E_{>k} / [N_{>k}(N_{>k} - 1)]\n",
    "#   N_{>k} = nodes with degree > k, E_{>k} = edges among them.\n",
    "# Normalized: ρ(k) = φ(k) / φ_rand(k). ρ(k) > 1 → rich-club ordering.\n",
    "# Internet: strong rich-club (Tier-1 providers form dense core).\n",
    "# Citation: Zhou, S. & Mondragón, R.J. \"The Rich-Club Phenomenon in the\n",
    "#           Internet Topology,\" IEEE Comm. Lett. 8(3), 180-182 (2004).\n",
    "#           Colizza, V. et al. \"Detecting rich-club ordering in complex\n",
    "#           networks,\" Nature Physics 2, 110-115 (2006).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "try:\n",
    "    rc = nx.rich_club_coefficient(G_lcc, normalized=False)\n",
    "    # Store at selected degree thresholds\n",
    "    rc_keys = sorted(rc.keys())\n",
    "    # Sample at percentiles of degree distribution\n",
    "    p25_k = int(np.percentile(degrees, 25))\n",
    "    p50_k = int(np.percentile(degrees, 50))\n",
    "    p75_k = int(np.percentile(degrees, 75))\n",
    "    p90_k = int(np.percentile(degrees, 90))\n",
    "    p95_k = int(np.percentile(degrees, 95))\n",
    "    \n",
    "    graph_features['rich_club_p25'] = rc.get(p25_k, None)\n",
    "    graph_features['rich_club_p50'] = rc.get(p50_k, None)\n",
    "    graph_features['rich_club_p75'] = rc.get(p75_k, None)\n",
    "    graph_features['rich_club_p90'] = rc.get(p90_k, None)\n",
    "    graph_features['rich_club_p95'] = rc.get(p95_k, None)\n",
    "    \n",
    "    print(f\"[13/16] Rich-club coefficient (unnormalized)  ({time.time()-t0:.1f}s)\")\n",
    "    print(f\"        φ(k={p25_k}): {graph_features['rich_club_p25']:.6f}\" if graph_features['rich_club_p25'] else \"\")\n",
    "    print(f\"        φ(k={p50_k}): {graph_features['rich_club_p50']:.6f}\" if graph_features['rich_club_p50'] else \"\")\n",
    "    print(f\"        φ(k={p75_k}): {graph_features['rich_club_p75']:.6f}\" if graph_features['rich_club_p75'] else \"\")\n",
    "    print(f\"        φ(k={p90_k}): {graph_features['rich_club_p90']:.6f}\" if graph_features['rich_club_p90'] else \"\")\n",
    "    print(f\"        φ(k={p95_k}): {graph_features['rich_club_p95']:.6f}\" if graph_features['rich_club_p95'] else \"\")\n",
    "    \n",
    "    # Store full RC curve for plotting\n",
    "    rc_df = pd.DataFrame({'k': list(rc.keys()), 'phi': list(rc.values())})\n",
    "    rc_df.to_csv(OUTPUT_DIR / 'rich_club_curve.csv', index=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[13/16] Rich-club coefficient: FAILED ({e})\")\n",
    "    graph_features['rich_club_p50'] = None"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------",
    "# 14. BETWEENNESS CENTRALITY DISTRIBUTION STATISTICS",
    "# --------------------------------------------------------------------------",
    "# Compute C_B(v) ONCE here, store _bc_map for node-level reuse.",
    "# Citation: Brandes, U. J. Math. Sociology 25(2), 163-177 (2001).",
    "# --------------------------------------------------------------------------",
    "",
    "t0 = time.time()",
    "",
    "if HAS_NETWORKIT:",
    "    if BETWEENNESS_SAMPLE_K:",
    "        bc_algo = nk.centrality.ApproxBetweenness(G_nk, epsilon=0.01, delta=0.1)",
    "        bc_algo.run()",
    "        bc_scores_nk = bc_algo.scores()",
    "        print(f\"[14/16] Betweenness distribution (NetworKit approx)  ({time.time()-t0:.1f}s)\")",
    "    else:",
    "        bc_algo = nk.centrality.Betweenness(G_nk, normalized=True)",
    "        bc_algo.run()",
    "        bc_scores_nk = bc_algo.scores()",
    "        print(f\"[14/16] Betweenness distribution (NetworKit exact)  ({time.time()-t0:.1f}s)\")",
    "    _bc_map = {nk2nx_map[i]: bc_scores_nk[i] for i in range(len(bc_scores_nk))}",
    "    bc_scores = np.array(list(_bc_map.values()))",
    "else:",
    "    _bc_map = nx.betweenness_centrality(G_lcc, k=BETWEENNESS_SAMPLE_K, normalized=True)",
    "    bc_scores = np.array(list(_bc_map.values()))",
    "    label = f\"sampled k={BETWEENNESS_SAMPLE_K}\" if BETWEENNESS_SAMPLE_K else \"exact\"",
    "    print(f\"[14/16] Betweenness distribution (NetworkX {label})  ({time.time()-t0:.1f}s)\")",
    "",
    "graph_features['betweenness_mean'] = float(np.mean(bc_scores))",
    "graph_features['betweenness_max'] = float(np.max(bc_scores))",
    "graph_features['betweenness_std'] = float(np.std(bc_scores))",
    "graph_features['betweenness_skewness'] = float(sp_stats.skew(bc_scores))",
    "",
    "print(f\"        Mean: {graph_features['betweenness_mean']:.8f}\")",
    "print(f\"        Max:  {graph_features['betweenness_max']:.8f}\")",
    "print(f\"        Std:  {graph_features['betweenness_std']:.8f}\")",
    "print(f\"        Skew: {graph_features['betweenness_skewness']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------",
    "# 15. K-CORE DECOMPOSITION METRICS",
    "# --------------------------------------------------------------------------",
    "# Compute ONCE here, store _core_map for node-level reuse.",
    "# Citation: Seidman, Social Networks 5(3), 269-287 (1983).",
    "# --------------------------------------------------------------------------",
    "",
    "t0 = time.time()",
    "",
    "if HAS_NETWORKIT:",
    "    cd = nk.centrality.CoreDecomposition(G_nk)",
    "    cd.run()",
    "    _core_scores_nk = cd.scores()",
    "    _core_map = {nk2nx_map[i]: int(_core_scores_nk[i]) for i in range(len(_core_scores_nk))}",
    "    core_numbers = np.array(list(_core_map.values()))",
    "    graph_features['degeneracy'] = int(cd.maxCoreNumber())",
    "else:",
    "    _core_map = nx.core_number(G_lcc)",
    "    core_numbers = np.array(list(_core_map.values()))",
    "    graph_features['degeneracy'] = int(np.max(core_numbers))",
    "",
    "graph_features['core_mean'] = float(np.mean(core_numbers))",
    "graph_features['core_std'] = float(np.std(core_numbers))",
    "graph_features['core_median'] = float(np.median(core_numbers))",
    "",
    "innermost_count = int(np.sum(core_numbers == graph_features['degeneracy']))",
    "graph_features['innermost_core_size'] = innermost_count",
    "",
    "print(f\"[15/16] k-Core decomposition  ({time.time()-t0:.1f}s)\")",
    "print(f\"        Degeneracy (k_max): {graph_features['degeneracy']}\")",
    "print(f\"        Mean core number: {graph_features['core_mean']:.2f}\")",
    "print(f\"        Median core number: {graph_features['core_median']:.1f}\")",
    "print(f\"        Innermost core size: {innermost_count} nodes\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 16. WEIGHTED SPECTRUM STATISTICS\n",
    "# --------------------------------------------------------------------------\n",
    "# The eigenvalue spectrum of the adjacency (or Laplacian) matrix,\n",
    "# summarized via: spectral gap (λ₁ - λ₂), spectral norm,\n",
    "# and normalized Laplacian spectral gap.\n",
    "# In AS topology, the spectral gap relates to expansion properties\n",
    "# and mixing time of random walks.\n",
    "# Citation: Chung, F. \"Spectral Graph Theory,\" AMS (1997).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "if COMPUTE_SPECTRAL:\n",
    "    # Spectral gap = λ₁ - λ₂ of adjacency matrix\n",
    "    if len(adjacency_eigs) >= 2:\n",
    "        sorted_adj_eigs = np.sort(adjacency_eigs)[::-1]  # descending\n",
    "        graph_features['spectral_gap'] = float(sorted_adj_eigs[0] - sorted_adj_eigs[1])\n",
    "        graph_features['adj_eig_ratio_1_2'] = float(sorted_adj_eigs[0] / sorted_adj_eigs[1]) if sorted_adj_eigs[1] != 0 else None\n",
    "        print(f\"[16/16] Spectral gap (λ₁-λ₂): {graph_features['spectral_gap']:.4f}\")\n",
    "        print(f\"        λ₁/λ₂ ratio: {graph_features['adj_eig_ratio_1_2']:.4f}\" if graph_features['adj_eig_ratio_1_2'] else \"\")\n",
    "    else:\n",
    "        graph_features['spectral_gap'] = None\n",
    "        print(f\"[16/16] Spectral gap: SKIPPED (insufficient eigenvalues)\")\n",
    "else:\n",
    "    graph_features['spectral_gap'] = None\n",
    "    print(f\"[16/16] Spectral gap: SKIPPED (COMPUTE_SPECTRAL=False)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GRAPH-LEVEL FEATURE EXTRACTION COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Node-Level Feature Extraction\n",
    "\n",
    "Extract 10 node-level metrics for every AS in the topology graph.\n",
    "\n",
    "Each metric includes its mathematical definition, interpretation in AS topology context,\n",
    "and authoritative citation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Node-Level Feature Extraction\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Extracting node-level features for {n_nodes:,} nodes...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize DataFrame with ASN as index\n",
    "node_features = pd.DataFrame(index=sorted(G_lcc.nodes()))\n",
    "node_features.index.name = 'asn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. DEGREE CENTRALITY\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: C_D(v) = deg(v) / (n - 1)\n",
    "# Identifies major transit providers in AS topology.\n",
    "# Citation: Freeman, L.C. \"Centrality in social networks: Conceptual\n",
    "#           clarification,\" Social Networks 1(3), 215-239 (1979).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "dc = nx.degree_centrality(G_lcc)\n",
    "node_features['degree_centrality'] = node_features.index.map(dc)\n",
    "node_features['degree'] = node_features.index.map(dict(G_lcc.degree()))\n",
    "print(f\"[1/10] Degree centrality  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------",
    "# 2. BETWEENNESS CENTRALITY (reused from graph-level computation)",
    "# --------------------------------------------------------------------------",
    "# Already computed above. Reuse _bc_map to avoid duplicate O(nm) work.",
    "# Citation: Brandes, U. J. Math. Sociology 25(2), 163-177 (2001).",
    "# --------------------------------------------------------------------------",
    "",
    "t0 = time.time()",
    "node_features['betweenness_centrality'] = node_features.index.map(_bc_map)",
    "print(f\"[2/10] Betweenness centrality (reused from graph-level)  ({time.time()-t0:.1f}s)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 3. CLOSENESS CENTRALITY\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: C_C(v) = (n - 1) / Σ_{u≠v} d(v, u)\n",
    "# Measures how quickly an AS reaches all others.\n",
    "# Wasserman-Faust variant handles disconnected graphs.\n",
    "# Citation: Sabidussi, G. \"The centrality index of a graph,\"\n",
    "#           Psychometrika 31(4), 581-603 (1966).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    cc_algo = nk.centrality.Closeness(G_nk, True, nk.centrality.ClosenessVariant.GENERALIZED)\n",
    "    cc_algo.run()\n",
    "    cc_scores = cc_algo.scores()\n",
    "    cc_map = {nk2nx_map[i]: cc_scores[i] for i in range(len(cc_scores))}\n",
    "    node_features['closeness_centrality'] = node_features.index.map(cc_map)\n",
    "    print(f\"[3/10] Closeness centrality (NetworKit)  ({time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    cc = nx.closeness_centrality(G_lcc, wf_improved=True)\n",
    "    node_features['closeness_centrality'] = node_features.index.map(cc)\n",
    "    print(f\"[3/10] Closeness centrality (NetworkX)  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 4. EIGENVECTOR CENTRALITY\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: Principal eigenvector of A: Ax = λ₁x.\n",
    "# A node's centrality ∝ sum of neighbors' centralities.\n",
    "# Citation: Bonacich, P. \"Factoring and weighting approaches to status\n",
    "#           scores and clique identification,\" J. Math. Sociology 2(1),\n",
    "#           113-120 (1972).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    ev_algo = nk.centrality.EigenvectorCentrality(G_nk, tol=1e-8)\n",
    "    ev_algo.run()\n",
    "    ev_scores = ev_algo.scores()\n",
    "    ev_map = {nk2nx_map[i]: ev_scores[i] for i in range(len(ev_scores))}\n",
    "    node_features['eigenvector_centrality'] = node_features.index.map(ev_map)\n",
    "    print(f\"[4/10] Eigenvector centrality (NetworKit)  ({time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    try:\n",
    "        ev = nx.eigenvector_centrality(G_lcc, max_iter=200, tol=1e-6)\n",
    "        node_features['eigenvector_centrality'] = node_features.index.map(ev)\n",
    "        print(f\"[4/10] Eigenvector centrality (NetworkX)  ({time.time()-t0:.1f}s)\")\n",
    "    except nx.PowerIterationFailedConvergence:\n",
    "        ev = nx.eigenvector_centrality_numpy(G_lcc)\n",
    "        node_features['eigenvector_centrality'] = node_features.index.map(ev)\n",
    "        print(f\"[4/10] Eigenvector centrality (NetworkX numpy fallback)  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 5. PAGERANK\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: Stationary distribution of random walk with damping d=0.85:\n",
    "#   PR(v) = (1-d)/n + d Σ_{u∈N(v)} PR(u)/deg(u)\n",
    "# Citation: Brin, S. & Page, L. \"The anatomy of a large-scale hypertextual\n",
    "#           web search engine,\" Computer Networks 30(1-7), 107-117 (1998).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    pr_algo = nk.centrality.PageRank(G_nk, damp=0.85, tol=1e-8)\n",
    "    pr_algo.run()\n",
    "    pr_scores = pr_algo.scores()\n",
    "    pr_map = {nk2nx_map[i]: pr_scores[i] for i in range(len(pr_scores))}\n",
    "    node_features['pagerank'] = node_features.index.map(pr_map)\n",
    "    print(f\"[5/10] PageRank (NetworKit, d=0.85)  ({time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    pr = nx.pagerank(G_lcc, alpha=0.85)\n",
    "    node_features['pagerank'] = node_features.index.map(pr)\n",
    "    print(f\"[5/10] PageRank (NetworkX, d=0.85)  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 6. LOCAL CLUSTERING COEFFICIENT\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: C(v) = 2·triangles(v) / [d_v(d_v - 1)]\n",
    "# High clustering at stub ASes → regional peering clusters.\n",
    "# Low clustering at transit ASes.\n",
    "# Citation: Watts & Strogatz, Nature 393 (1998).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    lcc_algo = nk.centrality.LocalClusteringCoefficient(G_nk, turbo=True)\n",
    "    lcc_algo.run()\n",
    "    lcc_scores = lcc_algo.scores()\n",
    "    lcc_map = {nk2nx_map[i]: lcc_scores[i] for i in range(len(lcc_scores))}\n",
    "    node_features['local_clustering'] = node_features.index.map(lcc_map)\n",
    "    print(f\"[6/10] Local clustering coefficient (NetworKit turbo)  ({time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    clust = nx.clustering(G_lcc)\n",
    "    node_features['local_clustering'] = node_features.index.map(clust)\n",
    "    print(f\"[6/10] Local clustering coefficient (NetworkX)  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 7. AVERAGE NEIGHBOR DEGREE\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: k_nn(v) = (1/d_v) Σ_{u∈N(v)} d_u\n",
    "# Decreasing k_nn(v) vs d_v confirms disassortative mixing in AS topology.\n",
    "# Citation: Pastor-Satorras, R. et al. \"Dynamical and correlation properties\n",
    "#           of the Internet,\" Phys. Rev. Lett. 87(25), 258701 (2001).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "and_dict = nx.average_neighbor_degree(G_lcc)\n",
    "node_features['avg_neighbor_degree'] = node_features.index.map(and_dict)\n",
    "print(f\"[7/10] Average neighbor degree  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------",
    "# 8. NODE CLIQUE NUMBER",
    "# --------------------------------------------------------------------------",
    "# NP-hard. Exact for small graphs, greedy approximation for large ones.",
    "# Citation: Standard NP-completeness result (Karp, 1972).",
    "# --------------------------------------------------------------------------",
    "",
    "t0 = time.time()",
    "if n_nodes <= MAX_NODES_FOR_CLIQUE:",
    "    ncn = nx.node_clique_number(G_lcc)",
    "    node_features['node_clique_number'] = node_features.index.map(ncn)",
    "    print(f\"[8/10] Node clique number (exact)  ({time.time()-t0:.1f}s)\")",
    "else:",
    "    # Greedy approximation: expand from each node by adding highest-degree",
    "    # neighbors that form a clique",
    "    def greedy_clique_size(G, node):",
    "        clique = {node}",
    "        candidates = set(G.neighbors(node))",
    "        for cand in sorted(candidates, key=lambda x: G.degree(x), reverse=True):",
    "            if all(G.has_edge(cand, c) for c in clique):",
    "                clique.add(cand)",
    "        return len(clique)",
    "",
    "    k_max = graph_features['degeneracy']",
    "    core_subgraph = nx.k_core(G_lcc, k=k_max)",
    "    ncn_approx = {}",
    "",
    "    # Exact for innermost core if small enough",
    "    if core_subgraph.number_of_nodes() <= MAX_NODES_FOR_CLIQUE:",
    "        ncn_core = nx.node_clique_number(core_subgraph)",
    "        ncn_approx.update(ncn_core)",
    "        print(f\"  Exact clique number for {core_subgraph.number_of_nodes()} innermost core nodes\")",
    "",
    "    # Greedy for remaining",
    "    for node in G_lcc.nodes():",
    "        if node not in ncn_approx:",
    "            ncn_approx[node] = greedy_clique_size(G_lcc, node)",
    "",
    "    node_features['node_clique_number'] = node_features.index.map(ncn_approx)",
    "    print(f\"[8/10] Node clique number (core exact + greedy approx)  ({time.time()-t0:.1f}s)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------",
    "# 9. ECCENTRICITY",
    "# --------------------------------------------------------------------------",
    "# FIX: For large graphs use NetworKit BFS (much faster than NetworkX),",
    "# fall back to sampling only when NetworKit is unavailable.",
    "# Citation: Standard graph theory definition.",
    "# --------------------------------------------------------------------------",
    "",
    "t0 = time.time()",
    "",
    "if n_nodes < 30000:",
    "    ecc = nx.eccentricity(G_lcc)",
    "    node_features['eccentricity'] = node_features.index.map(ecc)",
    "    graph_features['radius'] = min(ecc.values())",
    "    print(f\"[9/10] Eccentricity  ({time.time()-t0:.1f}s)\")",
    "    print(f\"        Radius: {graph_features['radius']}\")",
    "elif HAS_NETWORKIT:",
    "    # NetworKit BFS for all nodes — much faster than NetworkX",
    "    ecc_map = {}",
    "    node_list = sorted(G_lcc.nodes())",
    "    nx_to_nk_local = {n: i for i, n in enumerate(node_list)}",
    "",
    "    for node in node_list:",
    "        bfs = nk.distance.BFS(G_nk, nx_to_nk_local[node])",
    "        bfs.run()",
    "        distances = bfs.getDistances()",
    "        reachable = [d for d in distances if d < float('inf')]",
    "        ecc_map[node] = int(max(reachable)) if reachable else 0",
    "",
    "    node_features['eccentricity'] = node_features.index.map(ecc_map)",
    "    graph_features['radius'] = min(ecc_map.values())",
    "    print(f\"[9/10] Eccentricity (NetworKit BFS, all nodes)  ({time.time()-t0:.1f}s)\")",
    "    print(f\"        Radius: {graph_features['radius']}\")",
    "else:",
    "    # Sample-based for large graphs without NetworKit",
    "    sample_size = min(500, n_nodes)",
    "    sample_nodes = np.random.choice(list(G_lcc.nodes()), size=sample_size, replace=False)",
    "    ecc_sample = {}",
    "    for node in sample_nodes:",
    "        lengths = nx.single_source_shortest_path_length(G_lcc, node)",
    "        ecc_sample[node] = max(lengths.values())",
    "    node_features['eccentricity'] = node_features.index.map(",
    "        lambda x: ecc_sample.get(x, np.nan)",
    "    )",
    "    graph_features['radius'] = min(ecc_sample.values()) if ecc_sample else None",
    "    print(f\"[9/10] Eccentricity (sampled, {sample_size} nodes)  ({time.time()-t0:.1f}s)\")",
    "    print(f\"        Approx radius: {graph_features.get('radius')}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------",
    "# 10. K-SHELL / CORE NUMBER (reused from graph-level computation)",
    "# --------------------------------------------------------------------------",
    "# Reuse _core_map computed above. No duplicate computation.",
    "# Citation: Seidman, Social Networks 5(3), 269-287 (1983).",
    "# --------------------------------------------------------------------------",
    "",
    "t0 = time.time()",
    "node_features['core_number'] = node_features.index.map(_core_map)",
    "print(f\"[10/10] Core number (reused from graph-level)  ({time.time()-t0:.1f}s)\")",
    "",
    "print(f\"\\n{'='*70}\")",
    "print(\"NODE-LEVEL FEATURE EXTRACTION COMPLETE\")",
    "print(f\"{'='*70}\")",
    "print(f\"\\nNode features shape: {node_features.shape}\")",
    "node_features.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Summary & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Summary of Graph-Level Features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GRAPH-LEVEL FEATURES SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Feature':<35} {'Value':>20} {'Citation'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "feature_citations = {\n",
    "    'n_nodes': 'Graph property',\n",
    "    'n_edges': 'Graph property',\n",
    "    'assortativity': 'Newman, Phys.Rev.Lett. 89 (2002)',\n",
    "    'density': 'Standard',\n",
    "    'clustering_global': 'Watts & Strogatz, Nature 393 (1998)',\n",
    "    'clustering_avg_local': 'Watts & Strogatz, Nature 393 (1998)',\n",
    "    'diameter': 'Watts & Strogatz, Nature 393 (1998)',\n",
    "    'avg_path_length': 'Watts & Strogatz, Nature 393 (1998)',\n",
    "    'algebraic_connectivity': 'Fiedler, Czech.Math.J. 23 (1973)',\n",
    "    'spectral_radius': 'Cvetković et al., Cambridge (2010)',\n",
    "    'percolation_limit': 'Pastor-Satorras, Phys.Rev.Lett. 86 (2001)',\n",
    "    'symmetry_ratio': 'Dekker, CATS (2005)',\n",
    "    'natural_connectivity': 'Wu et al., IEEE Trans. SMC-A 41 (2011)',\n",
    "    'kirchhoff_index': 'Klein & Randić, J.Math.Chem. 12 (1993)',\n",
    "    'log_spanning_trees': 'Kirchhoff (1847)',\n",
    "    'edge_connectivity': 'Whitney, Am.J.Math. 54 (1932)',\n",
    "    'node_connectivity': 'Whitney, Am.J.Math. 54 (1932)',\n",
    "    'rich_club_p50': 'Zhou & Mondragón, IEEE Comm.Lett. (2004)',\n",
    "    'rich_club_p90': 'Zhou & Mondragón, IEEE Comm.Lett. (2004)',\n",
    "    'betweenness_mean': 'Brandes, J.Math.Soc. 25 (2001)',\n",
    "    'betweenness_max': 'Brandes, J.Math.Soc. 25 (2001)',\n",
    "    'betweenness_std': 'Brandes, J.Math.Soc. 25 (2001)',\n",
    "    'betweenness_skewness': 'Brandes, J.Math.Soc. 25 (2001)',\n",
    "    'degeneracy': 'Seidman, Social Networks 5 (1983)',\n",
    "    'core_mean': 'Seidman, Social Networks 5 (1983)',\n",
    "    'innermost_core_size': 'Alvarez-Hamelin et al. (2008)',\n",
    "    'spectral_gap': 'Chung, AMS (1997)',\n",
    "    'radius': 'Standard',\n",
    "}\n",
    "\n",
    "for feat, val in graph_features.items():\n",
    "    if val is not None:\n",
    "        if isinstance(val, float):\n",
    "            val_str = f\"{val:.6f}\" if abs(val) < 100 else f\"{val:.2f}\"\n",
    "        else:\n",
    "            val_str = f\"{val:,}\" if isinstance(val, int) else str(val)\n",
    "        citation = feature_citations.get(feat, '')\n",
    "        print(f\"{feat:<35} {val_str:>20}  {citation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Export Results\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Graph-level features as JSON\n",
    "graph_features_serializable = {}\n",
    "for k, v in graph_features.items():\n",
    "    if isinstance(v, (np.integer, np.int64)):\n",
    "        graph_features_serializable[k] = int(v)\n",
    "    elif isinstance(v, (np.floating, np.float64)):\n",
    "        graph_features_serializable[k] = float(v)\n",
    "    else:\n",
    "        graph_features_serializable[k] = v\n",
    "\n",
    "with open(OUTPUT_DIR / 'graph_level_features.json', 'w') as f:\n",
    "    json.dump(graph_features_serializable, f, indent=2, default=str)\n",
    "\n",
    "# 2. Node-level features as CSV\n",
    "node_features.to_csv(OUTPUT_DIR / 'node_level_features.csv')\n",
    "\n",
    "# 3. Edge list\n",
    "edges_df = pd.DataFrame(list(G_lcc.edges()), columns=['source_asn', 'target_asn'])\n",
    "edges_df.to_csv(OUTPUT_DIR / 'as_edges.csv', index=False)\n",
    "\n",
    "# 4. Graph in multiple formats\n",
    "nx.write_graphml(G_lcc, str(OUTPUT_DIR / 'as_topology.graphml'))\n",
    "\n",
    "print(f\"Files saved to {OUTPUT_DIR}:\")\n",
    "for f in sorted(OUTPUT_DIR.iterdir()):\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {f.name:<35} {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization: Centrality Correlations\n",
    "# ============================================================================\n",
    "\n",
    "centrality_cols = ['degree_centrality', 'betweenness_centrality', 'closeness_centrality',\n",
    "                   'eigenvector_centrality', 'pagerank']\n",
    "existing_cols = [c for c in centrality_cols if c in node_features.columns]\n",
    "\n",
    "if len(existing_cols) >= 2:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    corr = node_features[existing_cols].corr(method='spearman')\n",
    "    im = ax.imshow(corr, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    ax.set_xticks(range(len(existing_cols)))\n",
    "    ax.set_yticks(range(len(existing_cols)))\n",
    "    short_names = [c.replace('_centrality', '').replace('_', ' ').title() for c in existing_cols]\n",
    "    ax.set_xticklabels(short_names, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(short_names)\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(existing_cols)):\n",
    "        for j in range(len(existing_cols)):\n",
    "            ax.text(j, i, f\"{corr.iloc[i, j]:.2f}\", ha='center', va='center',\n",
    "                    color='white' if abs(corr.iloc[i, j]) > 0.5 else 'black', fontsize=10)\n",
    "    \n",
    "    plt.colorbar(im, label='Spearman ρ')\n",
    "    ax.set_title('Centrality Measure Correlations (Spearman)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'centrality_correlations.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'centrality_correlations.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization: Core Number Distribution\n",
    "# ============================================================================\n",
    "\n",
    "if 'core_number' in node_features.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Core number histogram\n",
    "    core_vals = node_features['core_number'].dropna()\n",
    "    axes[0].hist(core_vals, bins=range(int(core_vals.min()), int(core_vals.max()) + 2),\n",
    "                 edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[0].set_xlabel('Core Number (k-shell)')\n",
    "    axes[0].set_ylabel('Number of ASes')\n",
    "    axes[0].set_title('k-Core Decomposition Distribution')\n",
    "    axes[0].set_yscale('log')\n",
    "    \n",
    "    # Degree vs Core Number scatter\n",
    "    sample_idx = np.random.choice(len(node_features), size=min(5000, len(node_features)), replace=False)\n",
    "    sample_df = node_features.iloc[sample_idx]\n",
    "    axes[1].scatter(sample_df['degree'], sample_df['core_number'],\n",
    "                    alpha=0.3, s=5, c='steelblue')\n",
    "    axes[1].set_xlabel('Degree')\n",
    "    axes[1].set_ylabel('Core Number')\n",
    "    axes[1].set_title('Degree vs Core Number')\n",
    "    axes[1].set_xscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'kcore_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'kcore_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization: Disassortative Mixing (k_nn vs k)\n",
    "# ============================================================================\n",
    "\n",
    "if 'avg_neighbor_degree' in node_features.columns:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Bin by degree for cleaner visualization\n",
    "    df_plot = node_features[['degree', 'avg_neighbor_degree']].dropna()\n",
    "    degree_bins = pd.cut(df_plot['degree'], bins=50)\n",
    "    grouped = df_plot.groupby(degree_bins, observed=True).agg(\n",
    "        mean_k=('degree', 'mean'),\n",
    "        mean_knn=('avg_neighbor_degree', 'mean'),\n",
    "        count=('degree', 'count')\n",
    "    ).dropna()\n",
    "    \n",
    "    ax.scatter(grouped['mean_k'], grouped['mean_knn'],\n",
    "               s=np.clip(grouped['count'], 1, 500), alpha=0.6, c='steelblue')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Degree k')\n",
    "    ax.set_ylabel('Average Neighbor Degree k_nn(k)')\n",
    "    ax.set_title(f'Disassortative Mixing (r = {graph_features[\"assortativity\"]:.4f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'disassortative_mixing.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'disassortative_mixing.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization: Rich-Club Coefficient\n",
    "# ============================================================================\n",
    "\n",
    "rc_path = OUTPUT_DIR / 'rich_club_curve.csv'\n",
    "if rc_path.exists():\n",
    "    rc_df = pd.read_csv(rc_path)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(rc_df['k'], rc_df['phi'], '-', color='crimson', linewidth=1.5)\n",
    "    ax.set_xlabel('Degree k')\n",
    "    ax.set_ylabel('φ(k)')\n",
    "    ax.set_title('Rich-Club Coefficient (Unnormalized)')\n",
    "    ax.set_xscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'rich_club_coefficient.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'rich_club_coefficient.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Top ASes by Various Centrality Measures\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nTop 15 ASes by different centrality measures:\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for metric in ['degree_centrality', 'betweenness_centrality', 'pagerank', 'eigenvector_centrality']:\n",
    "    if metric in node_features.columns:\n",
    "        top = node_features[metric].nlargest(15)\n",
    "        print(f\"\\n--- {metric.replace('_', ' ').title()} ---\")\n",
    "        for asn, val in top.items():\n",
    "            core_n = node_features.loc[asn, 'core_number'] if 'core_number' in node_features.columns else '?'\n",
    "            deg = node_features.loc[asn, 'degree'] if 'degree' in node_features.columns else '?'\n",
    "            print(f\"  AS{asn:<8}  {metric}: {val:.8f}  degree: {deg}  core: {core_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Definitions Reference\n",
    "\n",
    "### Graph-Level Features (16)\n",
    "\n",
    "| # | Feature | Definition | Citation |\n",
    "|---|---------|------------|----------|\n",
    "| 1 | **Assortativity** | Pearson correlation of degrees at edge endpoints | Newman, *Phys. Rev. Lett.* 89, 208701 (2002) |\n",
    "| 2 | **Density** | ρ = 2\\|E\\| / [\\|V\\|(\\|V\\|-1)] | Standard |\n",
    "| 3 | **Clustering (global)** | 3×triangles / connected triples | Watts & Strogatz, *Nature* 393 (1998) |\n",
    "| 4 | **Diameter** | max\\_{u,v} d(u,v) | Watts & Strogatz (1998) |\n",
    "| 5 | **Algebraic connectivity** | μ₂(L), 2nd smallest Laplacian eigenvalue | Fiedler, *Czech. Math. J.* 23 (1973) |\n",
    "| 6 | **Spectral radius** | λ₁(A), largest adjacency eigenvalue | Cvetković et al., Cambridge (2010) |\n",
    "| 7 | **Percolation limit** | τ\\_c = 1/λ₁(A), epidemic threshold | Pastor-Satorras & Vespignani, *PRL* 86 (2001) |\n",
    "| 8 | **Symmetry ratio** | \\|distinct eigenvalues\\| / (D+1) | Dekker, CATS (2005) |\n",
    "| 9 | **Natural connectivity** | ln[(1/n) Σ exp(λᵢ)] | Wu et al., *IEEE Trans. SMC-A* 41 (2011) |\n",
    "| 10 | **Kirchhoff index** | n Σ\\_{k≥2} 1/μ\\_k | Klein & Randić, *J. Math. Chem.* 12 (1993) |\n",
    "| 11 | **log(Spanning trees)** | (1/n) Π\\_{k≥2} μ\\_k (via matrix tree theorem) | Kirchhoff (1847) |\n",
    "| 12 | **Edge/node connectivity** | Min cut size | Whitney, *Am. J. Math.* 54 (1932) |\n",
    "| 13 | **Rich-club coefficient** | φ(k) = 2E\\_{>k} / [N\\_{>k}(N\\_{>k}-1)] | Zhou & Mondragón, *IEEE Comm. Lett.* (2004) |\n",
    "| 14 | **Betweenness distribution** | Mean, max, std, skewness of C\\_B(v) | Brandes, *J. Math. Soc.* 25 (2001) |\n",
    "| 15 | **k-Core metrics** | Degeneracy k\\_max, core distribution | Seidman, *Social Networks* 5 (1983) |\n",
    "| 16 | **Spectral gap** | λ₁ - λ₂ of adjacency matrix | Chung, *Spectral Graph Theory*, AMS (1997) |\n",
    "\n",
    "### Node-Level Features (10)\n",
    "\n",
    "| # | Feature | Definition | Citation |\n",
    "|---|---------|------------|----------|\n",
    "| 1 | **Degree centrality** | C\\_D(v) = deg(v)/(n-1) | Freeman, *Social Networks* 1 (1979) |\n",
    "| 2 | **Betweenness centrality** | C\\_B(v) = Σ σ\\_st(v)/σ\\_st | Brandes, *J. Math. Soc.* 25 (2001) |\n",
    "| 3 | **Closeness centrality** | C\\_C(v) = (n-1)/Σ d(v,u) | Sabidussi, *Psychometrika* 31 (1966) |\n",
    "| 4 | **Eigenvector centrality** | Principal eigenvector of A | Bonacich, *J. Math. Soc.* 2 (1972) |\n",
    "| 5 | **PageRank** | Stationary random walk dist. (d=0.85) | Brin & Page, *Computer Networks* 30 (1998) |\n",
    "| 6 | **Local clustering** | C(v) = 2·tri(v)/[d(d-1)] | Watts & Strogatz (1998) |\n",
    "| 7 | **Avg neighbor degree** | k\\_nn(v) = (1/d) Σ\\_{u∈N(v)} d\\_u | Pastor-Satorras et al., *PRL* 87 (2001) |\n",
    "| 8 | **Node clique number** | ω(v) = max clique containing v | NP-hard (Karp, 1972) |\n",
    "| 9 | **Eccentricity** | ε(v) = max\\_u d(v,u) | Standard |\n",
    "| 10 | **Core number (k-shell)** | max{k : v ∈ H\\_k} | Seidman, *Social Networks* 5 (1983) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 70)\nprint(\"PIPELINE COMPLETE\")\nprint(\"=\" * 70)\nprint(f\"\\nCollector: {COLLECTOR}\")\nprint(f\"Date range: {START_DATE} to {END_DATE}\")\nprint(f\"Graph: {G_lcc.number_of_nodes():,} nodes, {G_lcc.number_of_edges():,} edges\")\nprint(f\"Graph-level features: {len([v for v in graph_features.values() if v is not None])}\")\nprint(f\"Node-level features: {node_features.shape[1]} columns x {node_features.shape[0]:,} ASes\")\nprint(f\"\\nOutput directory: {OUTPUT_DIR}\")\nprint(f\"Figures directory: {FIGURES_DIR}\")\nprint(f\"MRT files directory: {MRT_DIR}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}