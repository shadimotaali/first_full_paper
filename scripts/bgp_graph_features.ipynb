{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGP AS-Level Topology: Graph Feature Extraction Pipeline\n",
    "\n",
    "**Purpose:** Download RIPE RIS MRT data (RIB dumps and/or UPDATE files), parse with bgpkit-parser,\n",
    "construct AS-level topology graphs, and extract comprehensive graph-theoretic features for BGP anomaly detection.\n",
    "\n",
    "**Features extracted:**\n",
    "- **16 Graph-level metrics:** assortativity, diameter, algebraic connectivity, spectral radius, symmetry ratio, natural connectivity, effective graph resistance, spanning tree count, weighted spectrum stats, percolation limit, node/edge connectivity, clustering coefficient, density, rich-club coefficient, betweenness distribution stats, k-core decomposition metrics\n",
    "- **10 Node-level metrics:** degree centrality, betweenness centrality, closeness centrality, eigenvector centrality, PageRank, local clustering coefficient, average neighbor degree, node clique number, eccentricity, k-shell/core number\n",
    "\n",
    "**Data sources:** Configurable RIPE RIS collector and time period. Supports both RIB-only and RIB+UPDATE modes.\n",
    "\n",
    "**References:**\n",
    "- Willinger, W. & Roughan, M. \"Internet Topology Research Redux,\" *Recent Advances in Networking*, ACM SIGCOMM (2013)\n",
    "- Newman, M.E.J. *Networks: An Introduction*, Oxford University Press (2010)\n",
    "- Li, L. et al. \"A First-Principles Approach to Understanding the Internet's Router-Level Topology,\" *ACM SIGCOMM* (2004)\n",
    "- Sanchez, O.R. et al. \"Comparing ML Algorithms for BGP Anomaly Detection using Graph Features,\" *Big-DAMA '19* (2019)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install pybgpkit networkx scipy numpy pandas matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Optional, Dict, List, Tuple, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy import stats as sp_stats\n",
    "import bgpkit\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Optional: NetworKit for high-performance computation on large graphs\n",
    "try:\n",
    "    import networkit as nk\n",
    "    HAS_NETWORKIT = True\n",
    "    logger.info(\"NetworKit available - will use for performance-critical computations\")\n",
    "except ImportError:\n",
    "    HAS_NETWORKIT = False\n",
    "    logger.info(\"NetworKit not available - using NetworkX only (slower for large graphs)\")\n",
    "\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"NetworKit available: {HAS_NETWORKIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "All parameters are configurable. Adjust the collector, date range, and processing mode below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these parameters as needed\n",
    "# ============================================================================\n",
    "\n",
    "# --- RIPE RIS Collector ---\n",
    "# Available collectors: rrc00 (Amsterdam, multi-hop global), rrc01 (London/LINX),\n",
    "# rrc03 (Amsterdam/AMS-IX), rrc04 (Geneva/CIXP), rrc05 (Vienna/VIX),\n",
    "# rrc06 (Otemachi/DIX-IE), rrc07 (Stockholm/Netnod), rrc10 (Milan/MIX),\n",
    "# rrc11 (New York/NYIIX), rrc12 (Frankfurt/DE-CIX), rrc13 (Moscow/MSK-IX),\n",
    "# rrc14 (Palo Alto), rrc15 (Sao Paulo/PTTMetro-SP), rrc16 (Miami/NOTA),\n",
    "# rrc18 (Barcelona/CATNIX), rrc19 (Johannesburg/NAP Africa),\n",
    "# rrc20 (Zurich/SwissIX), rrc21 (Paris/FranceIX), rrc22 (Bucharest/InterLAN),\n",
    "# rrc23 (Singapore/Equinix), rrc24 (multi-hop, Montevideo/UY),\n",
    "# rrc25 (multi-hop, global), rrc26 (Dubai/UAE-IX)\n",
    "COLLECTOR = \"rrc04\"\n",
    "\n",
    "# --- Date Range ---\n",
    "# RIB dumps are available every 8 hours: 00:00, 08:00, 16:00 UTC\n",
    "# UPDATE files are available every 5 minutes\n",
    "START_DATE = \"2025-11-17\"  # YYYY-MM-DD\n",
    "END_DATE = \"2025-11-18\"    # YYYY-MM-DD (inclusive)\n",
    "\n",
    "# --- Processing Mode ---\n",
    "# 'rib_only': Build graph from RIB snapshots only (static topology)\n",
    "# 'rib_and_updates': Build base graph from RIB, then track changes via UPDATEs\n",
    "MODE = \"rib_only\"  # Options: 'rib_only', 'rib_and_updates'\n",
    "\n",
    "# --- Output Directories ---\n",
    "BASE_DIR = Path(\"./bgp_graph_features\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "FIGURES_DIR = BASE_DIR / \"figures\"\n",
    "\n",
    "for d in [DATA_DIR, OUTPUT_DIR, FIGURES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Performance Settings ---\n",
    "# For large graphs (70k+ nodes), exact betweenness is infeasible.\n",
    "# Set to None for exact computation, or an integer for sampled approximation.\n",
    "BETWEENNESS_SAMPLE_K = 500  # None for exact, 500 recommended for AS-level graphs\n",
    "\n",
    "# Whether to compute expensive spectral metrics (natural connectivity,\n",
    "# effective graph resistance, spanning tree count). These require eigenvalue\n",
    "# decomposition which can be slow on very large graphs.\n",
    "COMPUTE_SPECTRAL = True\n",
    "\n",
    "# Maximum graph size for exact clique computation (NP-hard).\n",
    "# For larger graphs, node clique number is skipped.\n",
    "MAX_NODES_FOR_CLIQUE = 5000\n",
    "\n",
    "# --- RIPE RIS URL Pattern ---\n",
    "RIPE_BASE_URL = \"https://data.ris.ripe.net\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Collector: {COLLECTOR}\")\n",
    "print(f\"  Date range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"  Mode: {MODE}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Discovery & Download\n",
    "\n",
    "Uses BGPKIT Broker to discover available MRT files for the configured collector and time range,\n",
    "then constructs direct URLs for RIB dumps (`bview.*`) and optionally UPDATE files (`updates.*`).\n",
    "\n",
    "**RIPE RIS URL pattern:**\n",
    "```\n",
    "https://data.ris.ripe.net/{collector}/{YYYY.MM}/bview.{YYYYMMDD}.{HHMM}.gz\n",
    "https://data.ris.ripe.net/{collector}/{YYYY.MM}/updates.{YYYYMMDD}.{HHMM}.gz\n",
    "```\n",
    "\n",
    "RIB dumps are generated every **8 hours** at 00:00, 08:00, 16:00 UTC.  \n",
    "UPDATE files are generated every **5 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rib_urls(collector: str, start_date: str, end_date: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate URLs for all RIB dump files in the given date range.\n",
    "    RIB dumps are available at 00:00, 08:00, 16:00 UTC daily.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    collector : str\n",
    "        RIPE RIS collector ID (e.g., 'rrc04')\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format (inclusive)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of URLs for RIB dump files\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    rib_hours = [0, 8, 16]  # RIB dumps at these hours\n",
    "    \n",
    "    current = start\n",
    "    while current <= end:\n",
    "        year_month = current.strftime(\"%Y.%m\")\n",
    "        for hour in rib_hours:\n",
    "            ts = current.replace(hour=hour, minute=0)\n",
    "            if ts < start or ts > end + timedelta(days=1):\n",
    "                continue\n",
    "            filename = f\"bview.{ts.strftime('%Y%m%d.%H%M')}.gz\"\n",
    "            url = f\"{RIPE_BASE_URL}/{collector}/{year_month}/{filename}\"\n",
    "            urls.append(url)\n",
    "        current += timedelta(days=1)\n",
    "    \n",
    "    return urls\n",
    "\n",
    "\n",
    "def generate_update_urls(collector: str, start_date: str, end_date: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate URLs for all UPDATE files in the given date range.\n",
    "    UPDATE files are available every 5 minutes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    collector : str\n",
    "        RIPE RIS collector ID\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format (inclusive)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of URLs for UPDATE files\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "    \n",
    "    current = start\n",
    "    while current < end:\n",
    "        year_month = current.strftime(\"%Y.%m\")\n",
    "        filename = f\"updates.{current.strftime('%Y%m%d.%H%M')}.gz\"\n",
    "        url = f\"{RIPE_BASE_URL}/{collector}/{year_month}/{filename}\"\n",
    "        urls.append(url)\n",
    "        current += timedelta(minutes=5)\n",
    "    \n",
    "    return urls\n",
    "\n",
    "\n",
    "def discover_files_via_broker(collector: str, start_date: str, end_date: str,\n",
    "                               data_type: str = \"rib\") -> List[dict]:\n",
    "    \"\"\"\n",
    "    Use BGPKIT Broker to discover available MRT files.\n",
    "    Falls back to URL generation if Broker is unavailable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    collector : str\n",
    "        RIPE RIS collector ID\n",
    "    start_date : str\n",
    "        Start date\n",
    "    end_date : str\n",
    "        End date\n",
    "    data_type : str\n",
    "        'rib' or 'update'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[dict]\n",
    "        List of file metadata dicts with 'url' key\n",
    "    \"\"\"\n",
    "    try:\n",
    "        broker = bgpkit.Broker()\n",
    "        items = broker.query(\n",
    "            ts_start=f\"{start_date}T00:00:00\",\n",
    "            ts_end=f\"{end_date}T23:59:59\",\n",
    "            collector_id=collector,\n",
    "            data_type=data_type\n",
    "        )\n",
    "        if items:\n",
    "            logger.info(f\"Broker found {len(items)} {data_type} files\")\n",
    "            return items\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Broker query failed: {e}. Falling back to URL generation.\")\n",
    "    \n",
    "    # Fallback: generate URLs directly\n",
    "    if data_type == \"rib\":\n",
    "        urls = generate_rib_urls(collector, start_date, end_date)\n",
    "    else:\n",
    "        urls = generate_update_urls(collector, start_date, end_date)\n",
    "    \n",
    "    logger.info(f\"Generated {len(urls)} {data_type} URLs\")\n",
    "    return [{\"url\": url} for url in urls]\n",
    "\n",
    "\n",
    "# Discover RIB files\n",
    "rib_files = discover_files_via_broker(COLLECTOR, START_DATE, END_DATE, \"rib\")\n",
    "print(f\"\\nRIB files discovered: {len(rib_files)}\")\n",
    "for f in rib_files[:5]:\n",
    "    url = f['url'] if isinstance(f, dict) else f.url\n",
    "    print(f\"  {url}\")\n",
    "\n",
    "# Discover UPDATE files (if in combined mode)\n",
    "if MODE == \"rib_and_updates\":\n",
    "    update_files = discover_files_via_broker(COLLECTOR, START_DATE, END_DATE, \"update\")\n",
    "    print(f\"\\nUPDATE files discovered: {len(update_files)}\")\n",
    "    for f in update_files[:5]:\n",
    "        url = f['url'] if isinstance(f, dict) else f.url\n",
    "        print(f\"  {url}\")\n",
    "    if len(update_files) > 5:\n",
    "        print(f\"  ... and {len(update_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MRT Parsing with bgpkit-parser\n",
    "\n",
    "**bgpkit-parser** is a Rust-based MRT parser with Python bindings (`pybgpkit`).  \n",
    "It handles HTTP download, gzip decompression, and MRT parsing transparently.\n",
    "\n",
    "Each parsed element exposes: `timestamp`, `elem_type` (\"A\" for announce, \"W\" for withdraw),\n",
    "`peer_ip`, `peer_asn`, `prefix`, `next_hop`, `as_path`, `origin_asns`, `origin`,\n",
    "`local_pref`, `med`, `communities`, and `atomic`.\n",
    "\n",
    "**Graph construction from AS_PATH:**\n",
    "1. Parse AS_PATH string (space-separated ASNs)\n",
    "2. Remove AS prepending (consecutive duplicates)\n",
    "3. Skip AS_SET entries (e.g., `{1234,5678}`)\n",
    "4. Extract pairwise adjacent links → undirected edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_as_path(as_path_str: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Parse an AS_PATH string into a deduplicated list of ASNs.\n",
    "    \n",
    "    Handles:\n",
    "    - Standard AS paths: \"3356 1299 13335\"\n",
    "    - AS prepending: \"3356 3356 3356 1299\" → [3356, 1299]\n",
    "    - AS_SETs: \"{1234,5678}\" → skipped entirely\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    as_path_str : str\n",
    "        Space-separated AS path string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[int]\n",
    "        Deduplicated list of ASNs (prepending removed)\n",
    "    \"\"\"\n",
    "    if not as_path_str:\n",
    "        return []\n",
    "    \n",
    "    tokens = as_path_str.split()\n",
    "    deduped = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Skip AS_SET entries like {1234,5678}\n",
    "        if '{' in token or '}' in token:\n",
    "            continue\n",
    "        try:\n",
    "            asn = int(token)\n",
    "            # Remove prepending (consecutive duplicates)\n",
    "            if not deduped or asn != deduped[-1]:\n",
    "                deduped.append(asn)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    return deduped\n",
    "\n",
    "\n",
    "def extract_edges_from_as_path(as_path: List[int]) -> Set[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Extract pairwise AS adjacency edges from a parsed AS path.\n",
    "    Edges are stored as sorted tuples to ensure undirected representation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    as_path : List[int]\n",
    "        Deduplicated list of ASNs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Set[Tuple[int, int]]\n",
    "        Set of (smaller_ASN, larger_ASN) edge tuples\n",
    "    \"\"\"\n",
    "    edges = set()\n",
    "    for i in range(len(as_path) - 1):\n",
    "        edge = tuple(sorted([as_path[i], as_path[i + 1]]))\n",
    "        # Skip self-loops (should not occur after dedup, but safety check)\n",
    "        if edge[0] != edge[1]:\n",
    "            edges.add(edge)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def parse_mrt_file(url: str, collect_metadata: bool = False) -> Tuple[Set[Tuple[int, int]], dict]:\n",
    "    \"\"\"\n",
    "    Parse a single MRT file (RIB or UPDATE) and extract AS adjacency edges.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL or local path to MRT file\n",
    "    collect_metadata : bool\n",
    "        If True, collect per-prefix metadata for analysis\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Set[Tuple[int, int]], dict]\n",
    "        - Set of AS adjacency edges\n",
    "        - Metadata dict with parsing statistics\n",
    "    \"\"\"\n",
    "    edges = set()\n",
    "    stats = {\n",
    "        'total_elements': 0,\n",
    "        'announcements': 0,\n",
    "        'withdrawals': 0,\n",
    "        'unique_prefixes': set(),\n",
    "        'unique_peers': set(),\n",
    "        'unique_asns': set(),\n",
    "        'parse_errors': 0\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Parsing: {url}\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    try:\n",
    "        parser = bgpkit.Parser(url=url)\n",
    "        \n",
    "        for elem in parser:\n",
    "            stats['total_elements'] += 1\n",
    "            \n",
    "            # Track element type\n",
    "            if elem.elem_type == \"A\":\n",
    "                stats['announcements'] += 1\n",
    "            elif elem.elem_type == \"W\":\n",
    "                stats['withdrawals'] += 1\n",
    "            \n",
    "            # Collect metadata\n",
    "            if elem.prefix:\n",
    "                stats['unique_prefixes'].add(elem.prefix)\n",
    "            if elem.peer_asn:\n",
    "                stats['unique_peers'].add(elem.peer_asn)\n",
    "            \n",
    "            # Parse AS path and extract edges\n",
    "            if elem.as_path:\n",
    "                as_path = parse_as_path(elem.as_path)\n",
    "                for asn in as_path:\n",
    "                    stats['unique_asns'].add(asn)\n",
    "                path_edges = extract_edges_from_as_path(as_path)\n",
    "                edges.update(path_edges)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing {url}: {e}\")\n",
    "        stats['parse_errors'] += 1\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    # Convert sets to counts for serialization\n",
    "    stats['unique_prefixes'] = len(stats['unique_prefixes'])\n",
    "    stats['unique_peers'] = len(stats['unique_peers'])\n",
    "    stats['unique_asns'] = len(stats['unique_asns'])\n",
    "    stats['unique_edges'] = len(edges)\n",
    "    stats['parse_time_sec'] = round(elapsed, 2)\n",
    "    \n",
    "    logger.info(\n",
    "        f\"  Parsed {stats['total_elements']:,} elements in {elapsed:.1f}s → \"\n",
    "        f\"{stats['unique_asns']:,} ASes, {len(edges):,} edges\"\n",
    "    )\n",
    "    \n",
    "    return edges, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Parse all discovered MRT files\n",
    "# ============================================================================\n",
    "\n",
    "all_edges = set()\n",
    "all_stats = []\n",
    "\n",
    "# --- Parse RIB files ---\n",
    "print(\"=\" * 70)\n",
    "print(\"PARSING RIB FILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, f in enumerate(rib_files):\n",
    "    url = f['url'] if isinstance(f, dict) else f.url\n",
    "    print(f\"\\n[{i+1}/{len(rib_files)}] {url}\")\n",
    "    edges, stats = parse_mrt_file(url)\n",
    "    stats['file_type'] = 'rib'\n",
    "    stats['url'] = url\n",
    "    all_edges.update(edges)\n",
    "    all_stats.append(stats)\n",
    "    print(f\"  Running total: {len(all_edges):,} unique edges\")\n",
    "\n",
    "# --- Parse UPDATE files (if in combined mode) ---\n",
    "if MODE == \"rib_and_updates\":\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"PARSING UPDATE FILES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, f in enumerate(update_files):\n",
    "        url = f['url'] if isinstance(f, dict) else f.url\n",
    "        if (i + 1) % 50 == 0 or i == 0:\n",
    "            print(f\"\\n[{i+1}/{len(update_files)}] {url}\")\n",
    "        edges, stats = parse_mrt_file(url)\n",
    "        stats['file_type'] = 'update'\n",
    "        stats['url'] = url\n",
    "        all_edges.update(edges)\n",
    "        all_stats.append(stats)\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"PARSING COMPLETE\")\n",
    "print(f\"  Total unique edges: {len(all_edges):,}\")\n",
    "print(f\"  Files processed: {len(all_stats)}\")\n",
    "print(f\"={'=' * 70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parsing statistics\n",
    "stats_df = pd.DataFrame(all_stats)\n",
    "stats_df.to_csv(OUTPUT_DIR / \"parsing_stats.csv\", index=False)\n",
    "print(\"Parsing statistics:\")\n",
    "stats_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AS-Level Graph Construction\n",
    "\n",
    "Build an undirected NetworkX graph from the extracted AS adjacency pairs.\n",
    "Each node represents an Autonomous System (ASN) and each edge represents\n",
    "an observed routing adjacency in BGP AS_PATH attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_as_graph(edges: Set[Tuple[int, int]]) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Construct an undirected AS-level topology graph from adjacency pairs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    edges : Set[Tuple[int, int]]\n",
    "        Set of (ASN_a, ASN_b) edge tuples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    nx.Graph\n",
    "        Undirected AS-level topology graph\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    # Annotate graph with metadata\n",
    "    G.graph['name'] = f\"AS-level topology ({COLLECTOR})\"\n",
    "    G.graph['collector'] = COLLECTOR\n",
    "    G.graph['start_date'] = START_DATE\n",
    "    G.graph['end_date'] = END_DATE\n",
    "    G.graph['mode'] = MODE\n",
    "    G.graph['created'] = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "G = build_as_graph(all_edges)\n",
    "\n",
    "print(f\"AS-Level Topology Graph:\")\n",
    "print(f\"  Nodes (ASes): {G.number_of_nodes():,}\")\n",
    "print(f\"  Edges (links): {G.number_of_edges():,}\")\n",
    "print(f\"  Connected: {nx.is_connected(G)}\")\n",
    "\n",
    "if not nx.is_connected(G):\n",
    "    components = list(nx.connected_components(G))\n",
    "    sizes = sorted([len(c) for c in components], reverse=True)\n",
    "    print(f\"  Connected components: {len(components)}\")\n",
    "    print(f\"  Largest component: {sizes[0]:,} nodes ({100*sizes[0]/G.number_of_nodes():.1f}%)\")\n",
    "    if len(sizes) > 1:\n",
    "        print(f\"  2nd largest: {sizes[1]:,} nodes\")\n",
    "    print(f\"  Components with 1 node: {sizes.count(1)}\")\n",
    "    \n",
    "    # Extract largest connected component for analysis\n",
    "    # (many graph metrics require connectivity)\n",
    "    largest_cc = max(components, key=len)\n",
    "    G_lcc = G.subgraph(largest_cc).copy()\n",
    "    print(f\"\\n  → Using largest connected component for analysis\")\n",
    "else:\n",
    "    G_lcc = G\n",
    "\n",
    "print(f\"\\nAnalysis graph: {G_lcc.number_of_nodes():,} nodes, {G_lcc.number_of_edges():,} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic degree distribution overview\n",
    "degrees = [d for _, d in G_lcc.degree()]\n",
    "degree_series = pd.Series(degrees)\n",
    "\n",
    "print(\"Degree distribution statistics:\")\n",
    "print(f\"  Min: {degree_series.min()}\")\n",
    "print(f\"  Max: {degree_series.max():,}\")\n",
    "print(f\"  Mean: {degree_series.mean():.2f}\")\n",
    "print(f\"  Median: {degree_series.median():.1f}\")\n",
    "print(f\"  Std: {degree_series.std():.2f}\")\n",
    "print(f\"  Skewness: {degree_series.skew():.2f}\")\n",
    "\n",
    "# Quick degree distribution plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear histogram\n",
    "axes[0].hist(degrees, bins=100, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Degree')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Degree Distribution (Linear Scale)')\n",
    "axes[0].axvline(x=degree_series.mean(), color='red', linestyle='--', label=f'Mean={degree_series.mean():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Log-log CCDF\n",
    "sorted_deg = np.sort(degrees)[::-1]\n",
    "ccdf = np.arange(1, len(sorted_deg) + 1) / len(sorted_deg)\n",
    "axes[1].loglog(sorted_deg, ccdf, '.', markersize=3, color='steelblue')\n",
    "axes[1].set_xlabel('Degree k')\n",
    "axes[1].set_ylabel('P(X ≥ k)')\n",
    "axes[1].set_title('Degree CCDF (Log-Log Scale)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'degree_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'degree_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Graph-Level Feature Extraction\n",
    "\n",
    "Extract 16 graph-level metrics with proper mathematical definitions and academic citations.\n",
    "\n",
    "Each metric includes:\n",
    "- **Definition**: Mathematical formula\n",
    "- **Interpretation**: What it captures in AS topology\n",
    "- **Citation**: Authoritative academic reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Helper: convert NetworkX → NetworKit (if available)\n",
    "# ============================================================================\n",
    "\n",
    "def nx_to_nk(G_nx: nx.Graph):\n",
    "    \"\"\"\n",
    "    Convert a NetworkX graph to NetworKit format.\n",
    "    Returns (nk_graph, node_mapping) where node_mapping maps NX→NK node IDs.\n",
    "    \"\"\"\n",
    "    if not HAS_NETWORKIT:\n",
    "        return None, None\n",
    "    \n",
    "    # NetworKit requires contiguous integer node IDs starting from 0\n",
    "    node_list = sorted(G_nx.nodes())\n",
    "    nx_to_nk_map = {n: i for i, n in enumerate(node_list)}\n",
    "    nk_to_nx_map = {i: n for n, i in nx_to_nk_map.items()}\n",
    "    \n",
    "    G_nk = nk.Graph(len(node_list), weighted=False, directed=False)\n",
    "    for u, v in G_nx.edges():\n",
    "        G_nk.addEdge(nx_to_nk_map[u], nx_to_nk_map[v])\n",
    "    \n",
    "    return G_nk, nx_to_nk_map, nk_to_nx_map\n",
    "\n",
    "\n",
    "# Pre-convert if NetworKit is available\n",
    "if HAS_NETWORKIT:\n",
    "    G_nk, nx2nk_map, nk2nx_map = nx_to_nk(G_lcc)\n",
    "    print(f\"NetworKit graph: {G_nk.numberOfNodes()} nodes, {G_nk.numberOfEdges()} edges\")\n",
    "else:\n",
    "    G_nk, nx2nk_map, nk2nx_map = None, None, None\n",
    "    print(\"Using NetworkX only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Graph-Level Feature Extraction\n",
    "# ============================================================================\n",
    "\n",
    "graph_features = {}\n",
    "n_nodes = G_lcc.number_of_nodes()\n",
    "n_edges = G_lcc.number_of_edges()\n",
    "\n",
    "graph_features['n_nodes'] = n_nodes\n",
    "graph_features['n_edges'] = n_edges\n",
    "\n",
    "print(f\"Extracting graph-level features for {n_nodes:,} nodes, {n_edges:,} edges...\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. ASSORTATIVITY (Degree Assortativity Coefficient)\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: Pearson correlation of degrees at either end of each edge.\n",
    "#   r = [M^-1 Σ_i j_i k_i - (M^-1 Σ_i ½(j_i + k_i))^2] /\n",
    "#       [M^-1 Σ_i ½(j_i² + k_i²) - (M^-1 Σ_i ½(j_i + k_i))^2]\n",
    "# Range: [-1, 1]. Internet AS graphs are typically disassortative (r < 0).\n",
    "# Citation: Newman, M.E.J. \"Assortative Mixing in Networks,\"\n",
    "#           Physical Review Letters 89, 208701 (2002).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "graph_features['assortativity'] = nx.degree_assortativity_coefficient(G_lcc)\n",
    "print(f\"[1/16] Assortativity: {graph_features['assortativity']:.6f}  ({time.time()-t0:.1f}s)\")\n",
    "print(f\"        → {'Disassortative' if graph_features['assortativity'] < 0 else 'Assortative'} \"\n",
    "      f\"(high-degree nodes preferentially connect to {'low' if graph_features['assortativity'] < 0 else 'high'}-degree nodes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 2. DENSITY\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: ρ(G) = 2|E| / [|V|(|V|-1)]\n",
    "# Internet AS graphs are extremely sparse (~10^-5).\n",
    "# Citation: Standard graph theory definition.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "graph_features['density'] = nx.density(G_lcc)\n",
    "print(f\"[2/16] Density: {graph_features['density']:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 3. CLUSTERING COEFFICIENT (Global & Average Local)\n",
    "# --------------------------------------------------------------------------\n",
    "# Global (transitivity): C_global = 3 × triangles / connected_triples\n",
    "# Average local: C̄ = (1/n) Σ_v C(v), where\n",
    "#   C(v) = 2|{edges among N(v)}| / [d_v(d_v - 1)]\n",
    "# Higher than random graphs → regional peering communities, IXP cliques.\n",
    "# Citation: Watts, D.J. & Strogatz, S.H. \"Collective dynamics of 'small-world'\n",
    "#           networks,\" Nature 393, 440-442 (1998).\n",
    "#           Newman, M.E.J. \"The structure and function of complex networks,\"\n",
    "#           SIAM Review 45(2), 167-256 (2003).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "if HAS_NETWORKIT:\n",
    "    graph_features['clustering_global'] = nk.globals.ClusteringCoefficient.exactGlobal(G_nk)\n",
    "    graph_features['clustering_avg_local'] = nk.globals.ClusteringCoefficient.sequentialAvgLocal(G_nk)\n",
    "else:\n",
    "    graph_features['clustering_global'] = nx.transitivity(G_lcc)\n",
    "    graph_features['clustering_avg_local'] = nx.average_clustering(G_lcc)\n",
    "\n",
    "print(f\"[3/16] Clustering coefficient  ({time.time()-t0:.1f}s)\")\n",
    "print(f\"        Global (transitivity): {graph_features['clustering_global']:.6f}\")\n",
    "print(f\"        Average local: {graph_features['clustering_avg_local']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 4. DIAMETER & AVERAGE PATH LENGTH\n",
    "# --------------------------------------------------------------------------\n",
    "# Diameter: D = max_{u,v} d(u,v) (longest shortest path)\n",
    "# Average path length: ℓ = 2/[n(n-1)] · Σ_{u<v} d(u,v)\n",
    "# Internet AS graph: D ≈ 8-11, ℓ ≈ 3-4 hops (small-world).\n",
    "# Citation: Watts, D.J. & Strogatz, S.H. Nature 393 (1998).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    # NetworKit's iFUB algorithm is near-linear for real-world networks\n",
    "    diam_algo = nk.distance.Diameter(G_nk, algo=nk.distance.DiameterAlgo.AUTOMATIC)\n",
    "    diam_algo.run()\n",
    "    graph_features['diameter'] = diam_algo.getDiameter()[0]\n",
    "    print(f\"[4/16] Diameter: {graph_features['diameter']}  ({time.time()-t0:.1f}s, NetworKit iFUB)\")\n",
    "else:\n",
    "    # NetworkX: only feasible for smaller graphs\n",
    "    if n_nodes < 50000:\n",
    "        graph_features['diameter'] = nx.diameter(G_lcc)\n",
    "        print(f\"[4/16] Diameter: {graph_features['diameter']}  ({time.time()-t0:.1f}s)\")\n",
    "    else:\n",
    "        # Approximate via BFS from random nodes\n",
    "        sample_nodes = np.random.choice(list(G_lcc.nodes()), size=min(100, n_nodes), replace=False)\n",
    "        max_ecc = 0\n",
    "        for node in sample_nodes:\n",
    "            ecc = nx.eccentricity(G_lcc, v=node)\n",
    "            max_ecc = max(max_ecc, ecc)\n",
    "        graph_features['diameter'] = max_ecc\n",
    "        print(f\"[4/16] Diameter (approx, 100 samples): {graph_features['diameter']}  ({time.time()-t0:.1f}s)\")\n",
    "\n",
    "# Average path length: expensive, sample-based for large graphs\n",
    "t1 = time.time()\n",
    "if n_nodes < 20000:\n",
    "    graph_features['avg_path_length'] = nx.average_shortest_path_length(G_lcc)\n",
    "    print(f\"        Avg path length: {graph_features['avg_path_length']:.4f}  ({time.time()-t1:.1f}s)\")\n",
    "else:\n",
    "    # Sample-based estimation\n",
    "    sample_size = min(500, n_nodes)\n",
    "    sample_nodes = np.random.choice(list(G_lcc.nodes()), size=sample_size, replace=False)\n",
    "    total_dist = 0\n",
    "    count = 0\n",
    "    for node in sample_nodes:\n",
    "        lengths = nx.single_source_shortest_path_length(G_lcc, node)\n",
    "        total_dist += sum(lengths.values())\n",
    "        count += len(lengths) - 1  # exclude self\n",
    "    graph_features['avg_path_length'] = total_dist / count if count > 0 else float('inf')\n",
    "    print(f\"        Avg path length (sampled, {sample_size} sources): \"\n",
    "          f\"{graph_features['avg_path_length']:.4f}  ({time.time()-t1:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 5. ALGEBRAIC CONNECTIVITY (Fiedler Value)\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: Second smallest eigenvalue of the Laplacian L = D - A:\n",
    "#   a(G) = μ₂(L),  0 = μ₁ ≤ μ₂ ≤ ... ≤ μ_n\n",
    "# μ₂ > 0 iff G is connected. Bounds: μ₂ ≤ κ_v(G) ≤ κ_e(G).\n",
    "# Higher values → harder to partition.\n",
    "# For large graphs: use scipy.sparse.linalg.eigsh with k=2, which='SM'.\n",
    "# Citation: Fiedler, M. \"Algebraic connectivity of graphs,\"\n",
    "#           Czechoslovak Mathematical Journal 23, 298-305 (1973).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if n_nodes < 30000:\n",
    "    try:\n",
    "        graph_features['algebraic_connectivity'] = nx.algebraic_connectivity(\n",
    "            G_lcc, method='tracemin_pcg'\n",
    "        )\n",
    "        print(f\"[5/16] Algebraic connectivity: {graph_features['algebraic_connectivity']:.6f}  ({time.time()-t0:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[5/16] Algebraic connectivity: FAILED ({e})\")\n",
    "        graph_features['algebraic_connectivity'] = None\n",
    "else:\n",
    "    # Use sparse eigenvalue solver for large graphs\n",
    "    try:\n",
    "        L_sparse = nx.laplacian_matrix(G_lcc).astype(float)\n",
    "        # Compute 2 smallest eigenvalues\n",
    "        eigenvalues = eigsh(L_sparse, k=2, which='SM', return_eigenvectors=False)\n",
    "        graph_features['algebraic_connectivity'] = float(np.sort(eigenvalues)[1])\n",
    "        print(f\"[5/16] Algebraic connectivity: {graph_features['algebraic_connectivity']:.6f}  \"\n",
    "              f\"({time.time()-t0:.1f}s, sparse eigsh)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[5/16] Algebraic connectivity: FAILED ({e})\")\n",
    "        graph_features['algebraic_connectivity'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 6. SPECTRAL RADIUS (Largest Eigenvalue of Adjacency Matrix)\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: λ₁ = max_i |λ_i(A)|\n",
    "# By Perron-Frobenius: λ₁ is positive and real for connected graphs.\n",
    "# Bounds: d̄ ≤ λ₁ ≤ d_max. Governs epidemic spreading dynamics.\n",
    "# Citation: Cvetković, D. et al. \"An Introduction to the Theory of Graph\n",
    "#           Spectra,\" Cambridge University Press (2010).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "try:\n",
    "    A_sparse = nx.adjacency_matrix(G_lcc).astype(float)\n",
    "    spectral_radius_vals = eigsh(A_sparse, k=1, which='LM', return_eigenvectors=False)\n",
    "    graph_features['spectral_radius'] = float(spectral_radius_vals[0])\n",
    "    print(f\"[6/16] Spectral radius: {graph_features['spectral_radius']:.4f}  ({time.time()-t0:.1f}s)\")\n",
    "except Exception as e:\n",
    "    print(f\"[6/16] Spectral radius: FAILED ({e})\")\n",
    "    graph_features['spectral_radius'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 7. PERCOLATION LIMIT / EPIDEMIC THRESHOLD\n",
    "# --------------------------------------------------------------------------\n",
    "# For the SIS epidemic model: τ_c ≥ 1/λ₁(A)\n",
    "# Epidemic persists if β/δ > 1/λ₁, dies out below.\n",
    "# Scale-free AS topologies have vanishing threshold as n → ∞.\n",
    "# Citation: Pastor-Satorras, R. & Vespignani, A. \"Epidemic Spreading in\n",
    "#           Scale-Free Networks,\" Phys. Rev. Lett. 86, 3200-3203 (2001).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "if graph_features.get('spectral_radius'):\n",
    "    graph_features['percolation_limit'] = 1.0 / graph_features['spectral_radius']\n",
    "    print(f\"[7/16] Percolation limit: {graph_features['percolation_limit']:.6f}\")\n",
    "    print(f\"        → Worms/epidemics can spread at infection rates above {graph_features['percolation_limit']:.4f}\")\n",
    "else:\n",
    "    graph_features['percolation_limit'] = None\n",
    "    print(f\"[7/16] Percolation limit: SKIPPED (requires spectral radius)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 8-11. SPECTRAL METRICS (Conditional on COMPUTE_SPECTRAL flag)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "if COMPUTE_SPECTRAL:\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # For spectral metrics, we need Laplacian eigenvalues.\n",
    "    # Full decomposition is O(n³), so for large graphs we use partial.\n",
    "    L_sparse = nx.laplacian_matrix(G_lcc).astype(float)\n",
    "    A_sparse = nx.adjacency_matrix(G_lcc).astype(float)\n",
    "    \n",
    "    # Determine how many eigenvalues to compute\n",
    "    n_eigs = min(n_nodes - 2, 300)  # cap at 300 for performance\n",
    "    use_full_spectrum = n_nodes < 5000\n",
    "    \n",
    "    if use_full_spectrum:\n",
    "        print(f\"Computing full spectrum ({n_nodes} eigenvalues)...\")\n",
    "        L_dense = L_sparse.toarray()\n",
    "        A_dense = A_sparse.toarray()\n",
    "        laplacian_eigs = np.sort(np.real(np.linalg.eigvalsh(L_dense)))\n",
    "        adjacency_eigs = np.sort(np.real(np.linalg.eigvalsh(A_dense)))[::-1]\n",
    "    else:\n",
    "        print(f\"Computing partial spectrum ({n_eigs} eigenvalues)...\")\n",
    "        # Smallest Laplacian eigenvalues (for algebraic connectivity, Kirchhoff)\n",
    "        laplacian_eigs_small = eigsh(L_sparse, k=min(n_eigs, n_nodes-2),\n",
    "                                     which='SM', return_eigenvectors=False)\n",
    "        laplacian_eigs_small = np.sort(laplacian_eigs_small)\n",
    "        \n",
    "        # Largest adjacency eigenvalues (for spectral radius, natural connectivity)\n",
    "        adjacency_eigs_large = eigsh(A_sparse, k=min(n_eigs, n_nodes-2),\n",
    "                                     which='LM', return_eigenvectors=False)\n",
    "        adjacency_eigs = np.sort(adjacency_eigs_large)[::-1]\n",
    "        laplacian_eigs = laplacian_eigs_small\n",
    "    \n",
    "    print(f\"  Spectrum computation: {time.time()-t0:.1f}s\")\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 8. SYMMETRY RATIO\n",
    "    # ------------------------------------------------------------------\n",
    "    # Definition: SR(G) = |{distinct eigenvalues of A}| / (D + 1)\n",
    "    # For distance-regular graphs SR = 1. Lower → higher symmetry.\n",
    "    # Citation: Dekker, A. \"The Symmetry Ratio of a Network,\" Proc.\n",
    "    #           Australasian Symp. Theory of Computing, Vol. 41 (2005).\n",
    "    # ------------------------------------------------------------------\n",
    "    if use_full_spectrum:\n",
    "        distinct_eigs = len(np.unique(np.round(adjacency_eigs, 8)))\n",
    "    else:\n",
    "        distinct_eigs = len(np.unique(np.round(adjacency_eigs, 8)))\n",
    "    \n",
    "    D = graph_features.get('diameter', 10)\n",
    "    graph_features['symmetry_ratio'] = distinct_eigs / (D + 1)\n",
    "    \n",
    "    if use_full_spectrum:\n",
    "        print(f\"[8/16] Symmetry ratio: {graph_features['symmetry_ratio']:.4f} \"\n",
    "              f\"({distinct_eigs} distinct eigs / {D+1})\")\n",
    "    else:\n",
    "        print(f\"[8/16] Symmetry ratio: {graph_features['symmetry_ratio']:.4f} \"\n",
    "              f\"(partial spectrum: {distinct_eigs} of {n_eigs} computed eigs distinct / {D+1})\")\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 9. NATURAL CONNECTIVITY\n",
    "    # ------------------------------------------------------------------\n",
    "    # Definition: λ̄ = ln[(1/n) Σ exp(λ_i)]\n",
    "    # Spectral robustness measure. Monotonically increases with added edges.\n",
    "    # Citation: Wu, J. et al. \"Spectral Measure of Structural Robustness,\"\n",
    "    #           IEEE Trans. SMC-A 41(6), 1244-1252 (2011).\n",
    "    # ------------------------------------------------------------------\n",
    "    if use_full_spectrum:\n",
    "        # Use log-sum-exp trick for numerical stability\n",
    "        max_eig = np.max(adjacency_eigs)\n",
    "        graph_features['natural_connectivity'] = float(\n",
    "            max_eig + np.log(np.mean(np.exp(adjacency_eigs - max_eig)))\n",
    "        )\n",
    "        print(f\"[9/16] Natural connectivity: {graph_features['natural_connectivity']:.4f}\")\n",
    "    else:\n",
    "        # Approximation using partial spectrum (dominated by largest eigenvalues)\n",
    "        max_eig = np.max(adjacency_eigs)\n",
    "        graph_features['natural_connectivity'] = float(\n",
    "            max_eig + np.log(np.mean(np.exp(adjacency_eigs - max_eig)))\n",
    "        )\n",
    "        print(f\"[9/16] Natural connectivity (approx, {len(adjacency_eigs)} eigs): \"\n",
    "              f\"{graph_features['natural_connectivity']:.4f}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 10. EFFECTIVE GRAPH RESISTANCE (Kirchhoff Index)\n",
    "    # ------------------------------------------------------------------\n",
    "    # Definition: K_f(G) = n Σ_{k=2}^{n} 1/μ_k\n",
    "    # Lower values → more robustly connected (multiple parallel paths).\n",
    "    # Citation: Klein, D.J. & Randić, M. \"Resistance Distance,\"\n",
    "    #           J. Math. Chem. 12, 81-95 (1993).\n",
    "    #           Ellens, W. et al. \"Effective graph resistance,\"\n",
    "    #           Linear Algebra Appl. 435, 2491-2506 (2011).\n",
    "    # ------------------------------------------------------------------\n",
    "    nonzero_lap_eigs = laplacian_eigs[laplacian_eigs > 1e-10]\n",
    "    if len(nonzero_lap_eigs) > 0:\n",
    "        if use_full_spectrum:\n",
    "            graph_features['kirchhoff_index'] = float(n_nodes * np.sum(1.0 / nonzero_lap_eigs))\n",
    "            print(f\"[10/16] Kirchhoff index: {graph_features['kirchhoff_index']:.2f}\")\n",
    "        else:\n",
    "            # Partial sum (lower bound on true Kirchhoff index)\n",
    "            graph_features['kirchhoff_index'] = float(n_nodes * np.sum(1.0 / nonzero_lap_eigs))\n",
    "            print(f\"[10/16] Kirchhoff index (partial, {len(nonzero_lap_eigs)} eigs): \"\n",
    "                  f\"{graph_features['kirchhoff_index']:.2f}\")\n",
    "    else:\n",
    "        graph_features['kirchhoff_index'] = None\n",
    "        print(f\"[10/16] Kirchhoff index: SKIPPED (no nonzero Laplacian eigenvalues)\")\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 11. NUMBER OF SPANNING TREES\n",
    "    # ------------------------------------------------------------------\n",
    "    # Kirchhoff's Matrix Tree Theorem: τ(G) = (1/n) Π_{k=2}^{n} μ_k\n",
    "    # Higher count → more structural redundancy in routing.\n",
    "    # Citation: Kirchhoff (1847). Godsil, C. & Royle, G. \"Algebraic\n",
    "    #           Graph Theory,\" Springer (2001).\n",
    "    # ------------------------------------------------------------------\n",
    "    if use_full_spectrum and len(nonzero_lap_eigs) > 0:\n",
    "        # Use log space to avoid overflow\n",
    "        log_spanning_trees = np.sum(np.log(nonzero_lap_eigs)) - np.log(n_nodes)\n",
    "        graph_features['log_spanning_trees'] = float(log_spanning_trees)\n",
    "        print(f\"[11/16] log(spanning trees): {graph_features['log_spanning_trees']:.2f}\")\n",
    "    else:\n",
    "        graph_features['log_spanning_trees'] = None\n",
    "        print(f\"[11/16] Spanning trees: SKIPPED (requires full spectrum)\")\n",
    "\n",
    "else:\n",
    "    print(\"[8-11] Spectral metrics SKIPPED (COMPUTE_SPECTRAL=False)\")\n",
    "    graph_features['symmetry_ratio'] = None\n",
    "    graph_features['natural_connectivity'] = None\n",
    "    graph_features['kirchhoff_index'] = None\n",
    "    graph_features['log_spanning_trees'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 12. NODE CONNECTIVITY & EDGE CONNECTIVITY\n",
    "# --------------------------------------------------------------------------\n",
    "# Node connectivity κ_v(G): min vertices to remove to disconnect G.\n",
    "# Edge connectivity κ_e(G): min edges to remove to disconnect G.\n",
    "# Whitney's inequality: κ_v(G) ≤ κ_e(G) ≤ δ(G).\n",
    "# By Menger's theorem: equals max disjoint paths between some pair.\n",
    "# Citation: Whitney, H. \"Congruent graphs and the connectivity of graphs,\"\n",
    "#           American J. Math. 54(1), 150-168 (1932).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Edge connectivity (faster than node connectivity)\n",
    "try:\n",
    "    graph_features['edge_connectivity'] = nx.edge_connectivity(G_lcc)\n",
    "    print(f\"[12/16] Edge connectivity: {graph_features['edge_connectivity']}  ({time.time()-t0:.1f}s)\")\n",
    "except Exception as e:\n",
    "    graph_features['edge_connectivity'] = None\n",
    "    print(f\"[12/16] Edge connectivity: FAILED ({e})\")\n",
    "\n",
    "# Node connectivity\n",
    "t1 = time.time()\n",
    "try:\n",
    "    graph_features['node_connectivity'] = nx.node_connectivity(G_lcc)\n",
    "    print(f\"        Node connectivity: {graph_features['node_connectivity']}  ({time.time()-t1:.1f}s)\")\n",
    "except Exception as e:\n",
    "    graph_features['node_connectivity'] = None\n",
    "    print(f\"        Node connectivity: FAILED ({e})\")\n",
    "\n",
    "min_degree = min(d for _, d in G_lcc.degree())\n",
    "print(f\"        Min degree δ(G): {min_degree}\")\n",
    "print(f\"        Whitney check: κ_v ≤ κ_e ≤ δ → \"\n",
    "      f\"{graph_features.get('node_connectivity','?')} ≤ \"\n",
    "      f\"{graph_features.get('edge_connectivity','?')} ≤ {min_degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 13. RICH-CLUB COEFFICIENT\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: φ(k) = 2E_{>k} / [N_{>k}(N_{>k} - 1)]\n",
    "#   N_{>k} = nodes with degree > k, E_{>k} = edges among them.\n",
    "# Normalized: ρ(k) = φ(k) / φ_rand(k). ρ(k) > 1 → rich-club ordering.\n",
    "# Internet: strong rich-club (Tier-1 providers form dense core).\n",
    "# Citation: Zhou, S. & Mondragón, R.J. \"The Rich-Club Phenomenon in the\n",
    "#           Internet Topology,\" IEEE Comm. Lett. 8(3), 180-182 (2004).\n",
    "#           Colizza, V. et al. \"Detecting rich-club ordering in complex\n",
    "#           networks,\" Nature Physics 2, 110-115 (2006).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "try:\n",
    "    rc = nx.rich_club_coefficient(G_lcc, normalized=False)\n",
    "    # Store at selected degree thresholds\n",
    "    rc_keys = sorted(rc.keys())\n",
    "    # Sample at percentiles of degree distribution\n",
    "    p25_k = int(np.percentile(degrees, 25))\n",
    "    p50_k = int(np.percentile(degrees, 50))\n",
    "    p75_k = int(np.percentile(degrees, 75))\n",
    "    p90_k = int(np.percentile(degrees, 90))\n",
    "    p95_k = int(np.percentile(degrees, 95))\n",
    "    \n",
    "    graph_features['rich_club_p25'] = rc.get(p25_k, None)\n",
    "    graph_features['rich_club_p50'] = rc.get(p50_k, None)\n",
    "    graph_features['rich_club_p75'] = rc.get(p75_k, None)\n",
    "    graph_features['rich_club_p90'] = rc.get(p90_k, None)\n",
    "    graph_features['rich_club_p95'] = rc.get(p95_k, None)\n",
    "    \n",
    "    print(f\"[13/16] Rich-club coefficient (unnormalized)  ({time.time()-t0:.1f}s)\")\n",
    "    print(f\"        φ(k={p25_k}): {graph_features['rich_club_p25']:.6f}\" if graph_features['rich_club_p25'] else \"\")\n",
    "    print(f\"        φ(k={p50_k}): {graph_features['rich_club_p50']:.6f}\" if graph_features['rich_club_p50'] else \"\")\n",
    "    print(f\"        φ(k={p75_k}): {graph_features['rich_club_p75']:.6f}\" if graph_features['rich_club_p75'] else \"\")\n",
    "    print(f\"        φ(k={p90_k}): {graph_features['rich_club_p90']:.6f}\" if graph_features['rich_club_p90'] else \"\")\n",
    "    print(f\"        φ(k={p95_k}): {graph_features['rich_club_p95']:.6f}\" if graph_features['rich_club_p95'] else \"\")\n",
    "    \n",
    "    # Store full RC curve for plotting\n",
    "    rc_df = pd.DataFrame({'k': list(rc.keys()), 'phi': list(rc.values())})\n",
    "    rc_df.to_csv(OUTPUT_DIR / 'rich_club_curve.csv', index=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[13/16] Rich-club coefficient: FAILED ({e})\")\n",
    "    graph_features['rich_club_p50'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 14. BETWEENNESS CENTRALITY DISTRIBUTION STATISTICS\n",
    "# --------------------------------------------------------------------------\n",
    "# Compute C_B(v) for all v, then extract: mean, max, std, skewness.\n",
    "# High skewness → few transit ASes carry disproportionate traffic.\n",
    "# Citation: Brandes, U. \"A faster algorithm for betweenness centrality,\"\n",
    "#           J. Math. Sociology 25(2), 163-177 (2001).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    if BETWEENNESS_SAMPLE_K:\n",
    "        # Approximate betweenness (KADABRA or sampling)\n",
    "        bc_algo = nk.centrality.ApproxBetweenness(G_nk, epsilon=0.01, delta=0.1)\n",
    "        bc_algo.run()\n",
    "        bc_scores = np.array(bc_algo.scores())\n",
    "        print(f\"[14/16] Betweenness distribution (NetworKit approx)  ({time.time()-t0:.1f}s)\")\n",
    "    else:\n",
    "        bc_algo = nk.centrality.Betweenness(G_nk, normalized=True)\n",
    "        bc_algo.run()\n",
    "        bc_scores = np.array(bc_algo.scores())\n",
    "        print(f\"[14/16] Betweenness distribution (NetworKit exact)  ({time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    bc_dict = nx.betweenness_centrality(G_lcc, k=BETWEENNESS_SAMPLE_K, normalized=True)\n",
    "    bc_scores = np.array(list(bc_dict.values()))\n",
    "    label = f\"sampled k={BETWEENNESS_SAMPLE_K}\" if BETWEENNESS_SAMPLE_K else \"exact\"\n",
    "    print(f\"[14/16] Betweenness distribution (NetworkX {label})  ({time.time()-t0:.1f}s)\")\n",
    "\n",
    "graph_features['betweenness_mean'] = float(np.mean(bc_scores))\n",
    "graph_features['betweenness_max'] = float(np.max(bc_scores))\n",
    "graph_features['betweenness_std'] = float(np.std(bc_scores))\n",
    "graph_features['betweenness_skewness'] = float(sp_stats.skew(bc_scores))\n",
    "\n",
    "print(f\"        Mean: {graph_features['betweenness_mean']:.8f}\")\n",
    "print(f\"        Max:  {graph_features['betweenness_max']:.8f}\")\n",
    "print(f\"        Std:  {graph_features['betweenness_std']:.8f}\")\n",
    "print(f\"        Skew: {graph_features['betweenness_skewness']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 15. K-CORE DECOMPOSITION METRICS\n",
    "# --------------------------------------------------------------------------\n",
    "# k-core H_k: maximal subgraph where every vertex has degree ≥ k within H_k.\n",
    "# Degeneracy: k_max = max{k : H_k ≠ ∅}.\n",
    "# Tier-1 providers reside in innermost core → hierarchical structure.\n",
    "# Citation: Seidman, S.B. \"Network structure and minimum degree,\"\n",
    "#           Social Networks 5(3), 269-287 (1983).\n",
    "#           Alvarez-Hamelin, J.I. et al. \"k-Core Decomposition of Internet\n",
    "#           Graphs,\" Networks & Heterogeneous Media 3(2), 371-393 (2008).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    cd = nk.centrality.CoreDecomposition(G_nk)\n",
    "    cd.run()\n",
    "    core_numbers = np.array(cd.scores())\n",
    "    graph_features['degeneracy'] = int(cd.maxCoreNumber())\n",
    "else:\n",
    "    core_dict = nx.core_number(G_lcc)\n",
    "    core_numbers = np.array(list(core_dict.values()))\n",
    "    graph_features['degeneracy'] = int(np.max(core_numbers))\n",
    "\n",
    "graph_features['core_mean'] = float(np.mean(core_numbers))\n",
    "graph_features['core_std'] = float(np.std(core_numbers))\n",
    "graph_features['core_median'] = float(np.median(core_numbers))\n",
    "\n",
    "# Nodes in innermost core\n",
    "innermost_count = int(np.sum(core_numbers == graph_features['degeneracy']))\n",
    "graph_features['innermost_core_size'] = innermost_count\n",
    "\n",
    "print(f\"[15/16] k-Core decomposition  ({time.time()-t0:.1f}s)\")\n",
    "print(f\"        Degeneracy (k_max): {graph_features['degeneracy']}\")\n",
    "print(f\"        Mean core number: {graph_features['core_mean']:.2f}\")\n",
    "print(f\"        Median core number: {graph_features['core_median']:.1f}\")\n",
    "print(f\"        Innermost core size: {innermost_count} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 16. WEIGHTED SPECTRUM STATISTICS\n",
    "# --------------------------------------------------------------------------\n",
    "# The eigenvalue spectrum of the adjacency (or Laplacian) matrix,\n",
    "# summarized via: spectral gap (λ₁ - λ₂), spectral norm,\n",
    "# and normalized Laplacian spectral gap.\n",
    "# In AS topology, the spectral gap relates to expansion properties\n",
    "# and mixing time of random walks.\n",
    "# Citation: Chung, F. \"Spectral Graph Theory,\" AMS (1997).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "if COMPUTE_SPECTRAL:\n",
    "    # Spectral gap = λ₁ - λ₂ of adjacency matrix\n",
    "    if len(adjacency_eigs) >= 2:\n",
    "        sorted_adj_eigs = np.sort(adjacency_eigs)[::-1]  # descending\n",
    "        graph_features['spectral_gap'] = float(sorted_adj_eigs[0] - sorted_adj_eigs[1])\n",
    "        graph_features['adj_eig_ratio_1_2'] = float(sorted_adj_eigs[0] / sorted_adj_eigs[1]) if sorted_adj_eigs[1] != 0 else None\n",
    "        print(f\"[16/16] Spectral gap (λ₁-λ₂): {graph_features['spectral_gap']:.4f}\")\n",
    "        print(f\"        λ₁/λ₂ ratio: {graph_features['adj_eig_ratio_1_2']:.4f}\" if graph_features['adj_eig_ratio_1_2'] else \"\")\n",
    "    else:\n",
    "        graph_features['spectral_gap'] = None\n",
    "        print(f\"[16/16] Spectral gap: SKIPPED (insufficient eigenvalues)\")\n",
    "else:\n",
    "    graph_features['spectral_gap'] = None\n",
    "    print(f\"[16/16] Spectral gap: SKIPPED (COMPUTE_SPECTRAL=False)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GRAPH-LEVEL FEATURE EXTRACTION COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Node-Level Feature Extraction\n",
    "\n",
    "Extract 10 node-level metrics for every AS in the topology graph.\n",
    "\n",
    "Each metric includes its mathematical definition, interpretation in AS topology context,\n",
    "and authoritative citation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Node-Level Feature Extraction\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Extracting node-level features for {n_nodes:,} nodes...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize DataFrame with ASN as index\n",
    "node_features = pd.DataFrame(index=sorted(G_lcc.nodes()))\n",
    "node_features.index.name = 'asn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. DEGREE CENTRALITY\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: C_D(v) = deg(v) / (n - 1)\n",
    "# Identifies major transit providers in AS topology.\n",
    "# Citation: Freeman, L.C. \"Centrality in social networks: Conceptual\n",
    "#           clarification,\" Social Networks 1(3), 215-239 (1979).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "dc = nx.degree_centrality(G_lcc)\n",
    "node_features['degree_centrality'] = node_features.index.map(dc)\n",
    "node_features['degree'] = node_features.index.map(dict(G_lcc.degree()))\n",
    "print(f\"[1/10] Degree centrality  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 2. BETWEENNESS CENTRALITY\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: C_B(v) = Σ_{s≠v≠t} σ_st(v) / σ_st\n",
    "#   σ_st = total shortest paths from s to t\n",
    "#   σ_st(v) = those passing through v\n",
    "# Normalized: divide by (n-1)(n-2)/2 for undirected.\n",
    "# Exact: O(nm). Use sampling or NetworKit for large graphs.\n",
    "# Citation: Brandes, U. J. Math. Sociology 25(2), 163-177 (2001).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    if BETWEENNESS_SAMPLE_K:\n",
    "        bc_algo = nk.centrality.ApproxBetweenness(G_nk, epsilon=0.01, delta=0.1)\n",
    "    else:\n",
    "        bc_algo = nk.centrality.Betweenness(G_nk, normalized=True)\n",
    "    bc_algo.run()\n",
    "    bc_scores_nk = bc_algo.scores()\n",
    "    # Map back to ASNs\n",
    "    bc_map = {nk2nx_map[i]: bc_scores_nk[i] for i in range(len(bc_scores_nk))}\n",
    "    node_features['betweenness_centrality'] = node_features.index.map(bc_map)\n",
    "    label = \"approx\" if BETWEENNESS_SAMPLE_K else \"exact\"\n",
    "    print(f\"[2/10] Betweenness centrality (NetworKit {label})  ({time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    bc = nx.betweenness_centrality(G_lcc, k=BETWEENNESS_SAMPLE_K, normalized=True)\n",
    "    node_features['betweenness_centrality'] = node_features.index.map(bc)\n",
    "    label = f\"sampled k={BETWEENNESS_SAMPLE_K}\" if BETWEENNESS_SAMPLE_K else \"exact\"\n",
    "    print(f\"[2/10] Betweenness centrality (NetworkX {label})  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 3. CLOSENESS CENTRALITY\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: C_C(v) = (n - 1) / Σ_{u≠v} d(v, u)\n",
    "# Measures how quickly an AS reaches all others.\n",
    "# Wasserman-Faust variant handles disconnected graphs.\n",
    "# Citation: Sabidussi, G. \"The centrality index of a graph,\"\n",
    "#           Psychometrika 31(4), 581-603 (1966).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    cc_algo = nk.centrality.Closeness(G_nk, True, nk.centrality.ClosenessVariant.GENERALIZED)\n",
    "    cc_algo.run()\n",
    "    cc_scores = cc_algo.scores()\n",
    "    cc_map = {nk2nx_map[i]: cc_scores[i] for i in range(len(cc_scores))}\n",
    "    node_features['closeness_centrality'] = node_features.index.map(cc_map)\n",
    "    print(f\"[3/10] Closeness centrality (NetworKit)  ({time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    cc = nx.closeness_centrality(G_lcc, wf_improved=True)\n",
    "    node_features['closeness_centrality'] = node_features.index.map(cc)\n",
    "    print(f\"[3/10] Closeness centrality (NetworkX)  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 4. EIGENVECTOR CENTRALITY\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: Principal eigenvector of A: Ax = λ₁x.\n",
    "# A node's centrality ∝ sum of neighbors' centralities.\n",
    "# Citation: Bonacich, P. \"Factoring and weighting approaches to status\n",
    "#           scores and clique identification,\" J. Math. Sociology 2(1),\n",
    "#           113-120 (1972).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    ev_algo = nk.centrality.EigenvectorCentrality(G_nk, tol=1e-8)\n",
    "    ev_algo.run()\n",
    "    ev_scores = ev_algo.scores()\n",
    "    ev_map = {nk2nx_map[i]: ev_scores[i] for i in range(len(ev_scores))}\n",
    "    node_features['eigenvector_centrality'] = node_features.index.map(ev_map)\n",
    "    print(f\"[4/10] Eigenvector centrality (NetworKit)  ({time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    try:\n",
    "        ev = nx.eigenvector_centrality(G_lcc, max_iter=200, tol=1e-6)\n",
    "        node_features['eigenvector_centrality'] = node_features.index.map(ev)\n",
    "        print(f\"[4/10] Eigenvector centrality (NetworkX)  ({time.time()-t0:.1f}s)\")\n",
    "    except nx.PowerIterationFailedConvergence:\n",
    "        ev = nx.eigenvector_centrality_numpy(G_lcc)\n",
    "        node_features['eigenvector_centrality'] = node_features.index.map(ev)\n",
    "        print(f\"[4/10] Eigenvector centrality (NetworkX numpy fallback)  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 5. PAGERANK\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: Stationary distribution of random walk with damping d=0.85:\n",
    "#   PR(v) = (1-d)/n + d Σ_{u∈N(v)} PR(u)/deg(u)\n",
    "# Citation: Brin, S. & Page, L. \"The anatomy of a large-scale hypertextual\n",
    "#           web search engine,\" Computer Networks 30(1-7), 107-117 (1998).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    pr_algo = nk.centrality.PageRank(G_nk, damp=0.85, tol=1e-8)\n",
    "    pr_algo.run()\n",
    "    pr_scores = pr_algo.scores()\n",
    "    pr_map = {nk2nx_map[i]: pr_scores[i] for i in range(len(pr_scores))}\n",
    "    node_features['pagerank'] = node_features.index.map(pr_map)\n",
    "    print(f\"[5/10] PageRank (NetworKit, d=0.85)  ({time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    pr = nx.pagerank(G_lcc, alpha=0.85)\n",
    "    node_features['pagerank'] = node_features.index.map(pr)\n",
    "    print(f\"[5/10] PageRank (NetworkX, d=0.85)  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 6. LOCAL CLUSTERING COEFFICIENT\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: C(v) = 2·triangles(v) / [d_v(d_v - 1)]\n",
    "# High clustering at stub ASes → regional peering clusters.\n",
    "# Low clustering at transit ASes.\n",
    "# Citation: Watts & Strogatz, Nature 393 (1998).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    lcc_algo = nk.centrality.LocalClusteringCoefficient(G_nk, turbo=True)\n",
    "    lcc_algo.run()\n",
    "    lcc_scores = lcc_algo.scores()\n",
    "    lcc_map = {nk2nx_map[i]: lcc_scores[i] for i in range(len(lcc_scores))}\n",
    "    node_features['local_clustering'] = node_features.index.map(lcc_map)\n",
    "    print(f\"[6/10] Local clustering coefficient (NetworKit turbo)  ({time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    clust = nx.clustering(G_lcc)\n",
    "    node_features['local_clustering'] = node_features.index.map(clust)\n",
    "    print(f\"[6/10] Local clustering coefficient (NetworkX)  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 7. AVERAGE NEIGHBOR DEGREE\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: k_nn(v) = (1/d_v) Σ_{u∈N(v)} d_u\n",
    "# Decreasing k_nn(v) vs d_v confirms disassortative mixing in AS topology.\n",
    "# Citation: Pastor-Satorras, R. et al. \"Dynamical and correlation properties\n",
    "#           of the Internet,\" Phys. Rev. Lett. 87(25), 258701 (2001).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "and_dict = nx.average_neighbor_degree(G_lcc)\n",
    "node_features['avg_neighbor_degree'] = node_features.index.map(and_dict)\n",
    "print(f\"[7/10] Average neighbor degree  ({time.time()-t0:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 8. NODE CLIQUE NUMBER\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: ω(v) = max{|Q| : v ∈ Q, Q is a clique}\n",
    "# WARNING: NP-hard. Only computed for graphs below MAX_NODES_FOR_CLIQUE.\n",
    "# Citation: Standard NP-completeness result (Karp, 1972).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "if n_nodes <= MAX_NODES_FOR_CLIQUE:\n",
    "    ncn = nx.node_clique_number(G_lcc)\n",
    "    node_features['node_clique_number'] = node_features.index.map(ncn)\n",
    "    print(f\"[8/10] Node clique number  ({time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    # Compute only for innermost k-core (dense subgraph)\n",
    "    if HAS_NETWORKIT:\n",
    "        k_max = int(graph_features['degeneracy'])\n",
    "    else:\n",
    "        k_max = int(max(core_dict.values())) if 'core_dict' in dir() else int(graph_features['degeneracy'])\n",
    "    \n",
    "    core_subgraph = nx.k_core(G_lcc, k=k_max)\n",
    "    if core_subgraph.number_of_nodes() <= MAX_NODES_FOR_CLIQUE:\n",
    "        ncn_core = nx.node_clique_number(core_subgraph)\n",
    "        node_features['node_clique_number'] = node_features.index.map(\n",
    "            lambda x: ncn_core.get(x, np.nan)\n",
    "        )\n",
    "        print(f\"[8/10] Node clique number (innermost core only, {core_subgraph.number_of_nodes()} nodes)  \"\n",
    "              f\"({time.time()-t0:.1f}s)\")\n",
    "    else:\n",
    "        node_features['node_clique_number'] = np.nan\n",
    "        print(f\"[8/10] Node clique number: SKIPPED (graph too large: {n_nodes:,} > {MAX_NODES_FOR_CLIQUE:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 9. ECCENTRICITY\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: ε(v) = max_u d(v, u)\n",
    "# Radius = min_v ε(v), Diameter = max_v ε(v).\n",
    "# Peripheral stub ASes → high eccentricity.\n",
    "# Central transit ASes → low eccentricity.\n",
    "# Citation: Standard graph theory definition.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if n_nodes < 30000:\n",
    "    ecc = nx.eccentricity(G_lcc)\n",
    "    node_features['eccentricity'] = node_features.index.map(ecc)\n",
    "    graph_features['radius'] = min(ecc.values())\n",
    "    print(f\"[9/10] Eccentricity  ({time.time()-t0:.1f}s)\")\n",
    "    print(f\"        Radius: {graph_features['radius']}\")\n",
    "else:\n",
    "    # Sample-based eccentricity for large graphs\n",
    "    sample_size = min(200, n_nodes)\n",
    "    sample_nodes = np.random.choice(list(G_lcc.nodes()), size=sample_size, replace=False)\n",
    "    ecc_sample = {}\n",
    "    for node in sample_nodes:\n",
    "        lengths = nx.single_source_shortest_path_length(G_lcc, node)\n",
    "        ecc_sample[node] = max(lengths.values())\n",
    "    node_features['eccentricity'] = node_features.index.map(\n",
    "        lambda x: ecc_sample.get(x, np.nan)\n",
    "    )\n",
    "    graph_features['radius'] = min(ecc_sample.values()) if ecc_sample else None\n",
    "    print(f\"[9/10] Eccentricity (sampled, {sample_size} nodes)  ({time.time()-t0:.1f}s)\")\n",
    "    print(f\"        Approx radius: {graph_features.get('radius')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 10. K-SHELL / CORE NUMBER\n",
    "# --------------------------------------------------------------------------\n",
    "# Definition: core(v) = max{k : v ∈ H_k}\n",
    "# Highest-shell ASes form the Internet's dense nucleus.\n",
    "# O(m) via iterative pruning.\n",
    "# Citation: Seidman, S.B. Social Networks 5(3), 269-287 (1983).\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if HAS_NETWORKIT:\n",
    "    cd = nk.centrality.CoreDecomposition(G_nk)\n",
    "    cd.run()\n",
    "    core_scores = cd.scores()\n",
    "    core_map = {nk2nx_map[i]: int(core_scores[i]) for i in range(len(core_scores))}\n",
    "    node_features['core_number'] = node_features.index.map(core_map)\n",
    "    print(f\"[10/10] Core number (NetworKit)  ({time.time()-t0:.1f}s)\")\n",
    "else:\n",
    "    cn = nx.core_number(G_lcc)\n",
    "    node_features['core_number'] = node_features.index.map(cn)\n",
    "    print(f\"[10/10] Core number (NetworkX)  ({time.time()-t0:.1f}s)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"NODE-LEVEL FEATURE EXTRACTION COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nNode features shape: {node_features.shape}\")\n",
    "node_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Summary & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Summary of Graph-Level Features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GRAPH-LEVEL FEATURES SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Feature':<35} {'Value':>20} {'Citation'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "feature_citations = {\n",
    "    'n_nodes': 'Graph property',\n",
    "    'n_edges': 'Graph property',\n",
    "    'assortativity': 'Newman, Phys.Rev.Lett. 89 (2002)',\n",
    "    'density': 'Standard',\n",
    "    'clustering_global': 'Watts & Strogatz, Nature 393 (1998)',\n",
    "    'clustering_avg_local': 'Watts & Strogatz, Nature 393 (1998)',\n",
    "    'diameter': 'Watts & Strogatz, Nature 393 (1998)',\n",
    "    'avg_path_length': 'Watts & Strogatz, Nature 393 (1998)',\n",
    "    'algebraic_connectivity': 'Fiedler, Czech.Math.J. 23 (1973)',\n",
    "    'spectral_radius': 'Cvetković et al., Cambridge (2010)',\n",
    "    'percolation_limit': 'Pastor-Satorras, Phys.Rev.Lett. 86 (2001)',\n",
    "    'symmetry_ratio': 'Dekker, CATS (2005)',\n",
    "    'natural_connectivity': 'Wu et al., IEEE Trans. SMC-A 41 (2011)',\n",
    "    'kirchhoff_index': 'Klein & Randić, J.Math.Chem. 12 (1993)',\n",
    "    'log_spanning_trees': 'Kirchhoff (1847)',\n",
    "    'edge_connectivity': 'Whitney, Am.J.Math. 54 (1932)',\n",
    "    'node_connectivity': 'Whitney, Am.J.Math. 54 (1932)',\n",
    "    'rich_club_p50': 'Zhou & Mondragón, IEEE Comm.Lett. (2004)',\n",
    "    'rich_club_p90': 'Zhou & Mondragón, IEEE Comm.Lett. (2004)',\n",
    "    'betweenness_mean': 'Brandes, J.Math.Soc. 25 (2001)',\n",
    "    'betweenness_max': 'Brandes, J.Math.Soc. 25 (2001)',\n",
    "    'betweenness_std': 'Brandes, J.Math.Soc. 25 (2001)',\n",
    "    'betweenness_skewness': 'Brandes, J.Math.Soc. 25 (2001)',\n",
    "    'degeneracy': 'Seidman, Social Networks 5 (1983)',\n",
    "    'core_mean': 'Seidman, Social Networks 5 (1983)',\n",
    "    'innermost_core_size': 'Alvarez-Hamelin et al. (2008)',\n",
    "    'spectral_gap': 'Chung, AMS (1997)',\n",
    "    'radius': 'Standard',\n",
    "}\n",
    "\n",
    "for feat, val in graph_features.items():\n",
    "    if val is not None:\n",
    "        if isinstance(val, float):\n",
    "            val_str = f\"{val:.6f}\" if abs(val) < 100 else f\"{val:.2f}\"\n",
    "        else:\n",
    "            val_str = f\"{val:,}\" if isinstance(val, int) else str(val)\n",
    "        citation = feature_citations.get(feat, '')\n",
    "        print(f\"{feat:<35} {val_str:>20}  {citation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Export Results\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Graph-level features as JSON\n",
    "graph_features_serializable = {}\n",
    "for k, v in graph_features.items():\n",
    "    if isinstance(v, (np.integer, np.int64)):\n",
    "        graph_features_serializable[k] = int(v)\n",
    "    elif isinstance(v, (np.floating, np.float64)):\n",
    "        graph_features_serializable[k] = float(v)\n",
    "    else:\n",
    "        graph_features_serializable[k] = v\n",
    "\n",
    "with open(OUTPUT_DIR / 'graph_level_features.json', 'w') as f:\n",
    "    json.dump(graph_features_serializable, f, indent=2, default=str)\n",
    "\n",
    "# 2. Node-level features as CSV\n",
    "node_features.to_csv(OUTPUT_DIR / 'node_level_features.csv')\n",
    "\n",
    "# 3. Edge list\n",
    "edges_df = pd.DataFrame(list(G_lcc.edges()), columns=['source_asn', 'target_asn'])\n",
    "edges_df.to_csv(OUTPUT_DIR / 'as_edges.csv', index=False)\n",
    "\n",
    "# 4. Graph in multiple formats\n",
    "nx.write_graphml(G_lcc, str(OUTPUT_DIR / 'as_topology.graphml'))\n",
    "\n",
    "print(f\"Files saved to {OUTPUT_DIR}:\")\n",
    "for f in sorted(OUTPUT_DIR.iterdir()):\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {f.name:<35} {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization: Centrality Correlations\n",
    "# ============================================================================\n",
    "\n",
    "centrality_cols = ['degree_centrality', 'betweenness_centrality', 'closeness_centrality',\n",
    "                   'eigenvector_centrality', 'pagerank']\n",
    "existing_cols = [c for c in centrality_cols if c in node_features.columns]\n",
    "\n",
    "if len(existing_cols) >= 2:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    corr = node_features[existing_cols].corr(method='spearman')\n",
    "    im = ax.imshow(corr, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    ax.set_xticks(range(len(existing_cols)))\n",
    "    ax.set_yticks(range(len(existing_cols)))\n",
    "    short_names = [c.replace('_centrality', '').replace('_', ' ').title() for c in existing_cols]\n",
    "    ax.set_xticklabels(short_names, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(short_names)\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(existing_cols)):\n",
    "        for j in range(len(existing_cols)):\n",
    "            ax.text(j, i, f\"{corr.iloc[i, j]:.2f}\", ha='center', va='center',\n",
    "                    color='white' if abs(corr.iloc[i, j]) > 0.5 else 'black', fontsize=10)\n",
    "    \n",
    "    plt.colorbar(im, label='Spearman ρ')\n",
    "    ax.set_title('Centrality Measure Correlations (Spearman)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'centrality_correlations.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'centrality_correlations.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization: Core Number Distribution\n",
    "# ============================================================================\n",
    "\n",
    "if 'core_number' in node_features.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Core number histogram\n",
    "    core_vals = node_features['core_number'].dropna()\n",
    "    axes[0].hist(core_vals, bins=range(int(core_vals.min()), int(core_vals.max()) + 2),\n",
    "                 edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[0].set_xlabel('Core Number (k-shell)')\n",
    "    axes[0].set_ylabel('Number of ASes')\n",
    "    axes[0].set_title('k-Core Decomposition Distribution')\n",
    "    axes[0].set_yscale('log')\n",
    "    \n",
    "    # Degree vs Core Number scatter\n",
    "    sample_idx = np.random.choice(len(node_features), size=min(5000, len(node_features)), replace=False)\n",
    "    sample_df = node_features.iloc[sample_idx]\n",
    "    axes[1].scatter(sample_df['degree'], sample_df['core_number'],\n",
    "                    alpha=0.3, s=5, c='steelblue')\n",
    "    axes[1].set_xlabel('Degree')\n",
    "    axes[1].set_ylabel('Core Number')\n",
    "    axes[1].set_title('Degree vs Core Number')\n",
    "    axes[1].set_xscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'kcore_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'kcore_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization: Disassortative Mixing (k_nn vs k)\n",
    "# ============================================================================\n",
    "\n",
    "if 'avg_neighbor_degree' in node_features.columns:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Bin by degree for cleaner visualization\n",
    "    df_plot = node_features[['degree', 'avg_neighbor_degree']].dropna()\n",
    "    degree_bins = pd.cut(df_plot['degree'], bins=50)\n",
    "    grouped = df_plot.groupby(degree_bins, observed=True).agg(\n",
    "        mean_k=('degree', 'mean'),\n",
    "        mean_knn=('avg_neighbor_degree', 'mean'),\n",
    "        count=('degree', 'count')\n",
    "    ).dropna()\n",
    "    \n",
    "    ax.scatter(grouped['mean_k'], grouped['mean_knn'],\n",
    "               s=np.clip(grouped['count'], 1, 500), alpha=0.6, c='steelblue')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Degree k')\n",
    "    ax.set_ylabel('Average Neighbor Degree k_nn(k)')\n",
    "    ax.set_title(f'Disassortative Mixing (r = {graph_features[\"assortativity\"]:.4f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'disassortative_mixing.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'disassortative_mixing.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization: Rich-Club Coefficient\n",
    "# ============================================================================\n",
    "\n",
    "rc_path = OUTPUT_DIR / 'rich_club_curve.csv'\n",
    "if rc_path.exists():\n",
    "    rc_df = pd.read_csv(rc_path)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(rc_df['k'], rc_df['phi'], '-', color='crimson', linewidth=1.5)\n",
    "    ax.set_xlabel('Degree k')\n",
    "    ax.set_ylabel('φ(k)')\n",
    "    ax.set_title('Rich-Club Coefficient (Unnormalized)')\n",
    "    ax.set_xscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'rich_club_coefficient.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'rich_club_coefficient.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Top ASes by Various Centrality Measures\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nTop 15 ASes by different centrality measures:\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for metric in ['degree_centrality', 'betweenness_centrality', 'pagerank', 'eigenvector_centrality']:\n",
    "    if metric in node_features.columns:\n",
    "        top = node_features[metric].nlargest(15)\n",
    "        print(f\"\\n--- {metric.replace('_', ' ').title()} ---\")\n",
    "        for asn, val in top.items():\n",
    "            core_n = node_features.loc[asn, 'core_number'] if 'core_number' in node_features.columns else '?'\n",
    "            deg = node_features.loc[asn, 'degree'] if 'degree' in node_features.columns else '?'\n",
    "            print(f\"  AS{asn:<8}  {metric}: {val:.8f}  degree: {deg}  core: {core_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Definitions Reference\n",
    "\n",
    "### Graph-Level Features (16)\n",
    "\n",
    "| # | Feature | Definition | Citation |\n",
    "|---|---------|------------|----------|\n",
    "| 1 | **Assortativity** | Pearson correlation of degrees at edge endpoints | Newman, *Phys. Rev. Lett.* 89, 208701 (2002) |\n",
    "| 2 | **Density** | ρ = 2\\|E\\| / [\\|V\\|(\\|V\\|-1)] | Standard |\n",
    "| 3 | **Clustering (global)** | 3×triangles / connected triples | Watts & Strogatz, *Nature* 393 (1998) |\n",
    "| 4 | **Diameter** | max\\_{u,v} d(u,v) | Watts & Strogatz (1998) |\n",
    "| 5 | **Algebraic connectivity** | μ₂(L), 2nd smallest Laplacian eigenvalue | Fiedler, *Czech. Math. J.* 23 (1973) |\n",
    "| 6 | **Spectral radius** | λ₁(A), largest adjacency eigenvalue | Cvetković et al., Cambridge (2010) |\n",
    "| 7 | **Percolation limit** | τ\\_c = 1/λ₁(A), epidemic threshold | Pastor-Satorras & Vespignani, *PRL* 86 (2001) |\n",
    "| 8 | **Symmetry ratio** | \\|distinct eigenvalues\\| / (D+1) | Dekker, CATS (2005) |\n",
    "| 9 | **Natural connectivity** | ln[(1/n) Σ exp(λᵢ)] | Wu et al., *IEEE Trans. SMC-A* 41 (2011) |\n",
    "| 10 | **Kirchhoff index** | n Σ\\_{k≥2} 1/μ\\_k | Klein & Randić, *J. Math. Chem.* 12 (1993) |\n",
    "| 11 | **log(Spanning trees)** | (1/n) Π\\_{k≥2} μ\\_k (via matrix tree theorem) | Kirchhoff (1847) |\n",
    "| 12 | **Edge/node connectivity** | Min cut size | Whitney, *Am. J. Math.* 54 (1932) |\n",
    "| 13 | **Rich-club coefficient** | φ(k) = 2E\\_{>k} / [N\\_{>k}(N\\_{>k}-1)] | Zhou & Mondragón, *IEEE Comm. Lett.* (2004) |\n",
    "| 14 | **Betweenness distribution** | Mean, max, std, skewness of C\\_B(v) | Brandes, *J. Math. Soc.* 25 (2001) |\n",
    "| 15 | **k-Core metrics** | Degeneracy k\\_max, core distribution | Seidman, *Social Networks* 5 (1983) |\n",
    "| 16 | **Spectral gap** | λ₁ - λ₂ of adjacency matrix | Chung, *Spectral Graph Theory*, AMS (1997) |\n",
    "\n",
    "### Node-Level Features (10)\n",
    "\n",
    "| # | Feature | Definition | Citation |\n",
    "|---|---------|------------|----------|\n",
    "| 1 | **Degree centrality** | C\\_D(v) = deg(v)/(n-1) | Freeman, *Social Networks* 1 (1979) |\n",
    "| 2 | **Betweenness centrality** | C\\_B(v) = Σ σ\\_st(v)/σ\\_st | Brandes, *J. Math. Soc.* 25 (2001) |\n",
    "| 3 | **Closeness centrality** | C\\_C(v) = (n-1)/Σ d(v,u) | Sabidussi, *Psychometrika* 31 (1966) |\n",
    "| 4 | **Eigenvector centrality** | Principal eigenvector of A | Bonacich, *J. Math. Soc.* 2 (1972) |\n",
    "| 5 | **PageRank** | Stationary random walk dist. (d=0.85) | Brin & Page, *Computer Networks* 30 (1998) |\n",
    "| 6 | **Local clustering** | C(v) = 2·tri(v)/[d(d-1)] | Watts & Strogatz (1998) |\n",
    "| 7 | **Avg neighbor degree** | k\\_nn(v) = (1/d) Σ\\_{u∈N(v)} d\\_u | Pastor-Satorras et al., *PRL* 87 (2001) |\n",
    "| 8 | **Node clique number** | ω(v) = max clique containing v | NP-hard (Karp, 1972) |\n",
    "| 9 | **Eccentricity** | ε(v) = max\\_u d(v,u) | Standard |\n",
    "| 10 | **Core number (k-shell)** | max{k : v ∈ H\\_k} | Seidman, *Social Networks* 5 (1983) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nCollector: {COLLECTOR}\")\n",
    "print(f\"Date range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"Mode: {MODE}\")\n",
    "print(f\"Graph: {G_lcc.number_of_nodes():,} nodes, {G_lcc.number_of_edges():,} edges\")\n",
    "print(f\"Graph-level features: {len([v for v in graph_features.values() if v is not None])}\")\n",
    "print(f\"Node-level features: {node_features.shape[1]} columns × {node_features.shape[0]:,} ASes\")\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(f\"Figures directory: {FIGURES_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
