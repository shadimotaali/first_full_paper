{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# BGP AS-Level Topology: Ego-Network Graph Feature Extraction Pipeline\n\n**Purpose:** Download RIPE RIS MRT data (RIB dumps), parse with bgpkit-parser,\nconstruct AS-level topology graphs **per snapshot**, extract a **k-hop ego-network**\naround a target AS, and compute comprehensive graph-theoretic features as\n**time series** for BGP anomaly detection.\n\n**Key Idea \u2014 Ego-Network Approach:**\nInstead of computing expensive features on the full 84K+ node global AS graph,\nwe focus on a **target AS and its k-hop neighborhood**. This mirrors how ISPs\nactually monitor BGP: they track their own AS and their neighbors, not the\nentire Internet topology.\n\n**Benefits:**\n- **Fast:** ~500-2,000 nodes instead of 84K+ \u2192 features compute in seconds, not hours\n- **Realistic:** Matches real-world ISP monitoring (track your own neighborhood)\n- **Better anomaly signal:** Local topology changes are not diluted by 84K other nodes\n\n**Workflow:**\n1. Parse RIB dump \u2192 build full AS-level edge set\n2. Construct full graph (needed to know who connects to whom)\n3. Extract **k-hop ego subgraph** around `TARGET_AS`\n4. Compute all 16 graph-level + 10 node-level features on the ego subgraph\n5. Repeat per snapshot \u2192 time series\n\n**Output:**\n- `graph_level_timeseries_*.csv` \u2014 one row per snapshot, all 16 graph-level features (ego subgraph)\n- `node_level_timeseries_*.csv` \u2014 one row per (snapshot, ASN), all 10 node-level features (ego nodes)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install pybgpkit networkx scipy numpy pandas matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Optional, Dict, List, Tuple, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy import stats as sp_stats\n",
    "import bgpkit\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Optional: NetworKit for high-performance computation on large graphs\n",
    "try:\n",
    "    import networkit as nk\n",
    "    HAS_NETWORKIT = True\n",
    "    logger.info(\"NetworKit available - will use for performance-critical computations\")\n",
    "except ImportError:\n",
    "    HAS_NETWORKIT = False\n",
    "    logger.info(\"NetworKit not available - using NetworkX only (slower for large graphs)\")\n",
    "\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"NetworKit available: {HAS_NETWORKIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "All parameters are configurable. Adjust the collector, date range, and processing mode below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these parameters as needed\n",
    "# ============================================================================\n",
    "\n",
    "# --- RIPE RIS Collector ---\n",
    "COLLECTOR = \"rrc04\"\n",
    "\n",
    "# --- Date Range ---\n",
    "START_DATE = \"2025-11-17\"  # YYYY-MM-DD\n",
    "END_DATE = \"2025-11-18\"    # YYYY-MM-DD (inclusive)\n",
    "\n",
    "# --- Ego-Network Settings ---\n",
    "# TARGET_AS: The AS number to center the ego-network on.\n",
    "#   This is the AS whose local neighborhood you want to monitor.\n",
    "#   Examples: 174 (Cogent), 3356 (Lumen/Level3), 13335 (Cloudflare),\n",
    "#             1299 (Arelion/Telia), 6939 (Hurricane Electric)\n",
    "TARGET_AS = 3333  # RIPE NCC \u2014 small ego network, exact features\n",
    "\n",
    "# EGO_K_HOP: Number of hops from TARGET_AS to include in the subgraph.\n",
    "#   1-hop = TARGET_AS + direct peers only (star-like, limited structure)\n",
    "#   2-hop = TARGET_AS + peers + peers-of-peers (recommended, captures local topology)\n",
    "#   3-hop = larger neighborhood (slower, but more context)\n",
    "EGO_K_HOP = 2\n",
    "\n",
    "# --- Output Directories ---\n",
    "BASE_DIR = Path(\"./bgp_graph_features\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "FIGURES_DIR = BASE_DIR / \"figures\"\n",
    "\n",
    "for d in [DATA_DIR, OUTPUT_DIR, FIGURES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Per-Snapshot Processing ---\n",
    "PER_SNAPSHOT_CSV = True  # Save a separate CSV for each RIB file\n",
    "SNAPSHOTS_DIR = OUTPUT_DIR / \"snapshots\"\n",
    "SNAPSHOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Private / Reserved ASN Ranges (to be filtered out) ---\n",
    "PRIVATE_ASNS = (\n",
    "    set(range(64512, 65535))              # 16-bit private (RFC 6996)\n",
    "    | set(range(4200000000, 4294967295))  # 32-bit private (RFC 6996)\n",
    "    | {0, 23456, 65535, 4294967295}       # reserved / special (RFC 7300)\n",
    ")\n",
    "\n",
    "def is_valid_public_asn(asn: int) -> bool:\n",
    "    \"\"\"Return True if the ASN is a valid public (non-private, non-reserved) ASN.\"\"\"\n",
    "    return asn not in PRIVATE_ASNS\n",
    "\n",
    "# --- Performance Settings ---\n",
    "BETWEENNESS_SAMPLE_K = None  # None = exact (feasible on small ego subgraphs)\n",
    "COMPUTE_SPECTRAL = True\n",
    "MAX_NODES_FOR_CLIQUE = 5000  # ego subgraphs are typically well under this\n",
    "\n",
    "# --- RIPE RIS URL Pattern ---\n",
    "RIPE_BASE_URL = \"https://data.ris.ripe.net\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Collector: {COLLECTOR}\")\n",
    "print(f\"  Date range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"  Target AS: {TARGET_AS}\")\n",
    "print(f\"  Ego-network hops: {EGO_K_HOP}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(f\"  Snapshots dir: {SNAPSHOTS_DIR}\")\n",
    "print(f\"  Private ASN filter: {len(PRIVATE_ASNS):,} ASNs will be excluded\")\n",
    "print(f\"  Betweenness sampling: {'exact' if BETWEENNESS_SAMPLE_K is None else f'k={BETWEENNESS_SAMPLE_K}'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Discovery & Download",
    "",
    "Uses BGPKIT Broker to discover available MRT files for the configured collector and time range,",
    "then constructs direct URLs for RIB dumps (`bview.*`).",
    "",
    "**RIPE RIS URL pattern:**",
    "```",
    "https://data.ris.ripe.net/{collector}/{YYYY.MM}/bview.{YYYYMMDD}.{HHMM}.gz",
    "```",
    "",
    "RIB dumps are generated every **8 hours** at 00:00, 08:00, 16:00 UTC."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_rib_urls(collector: str, start_date: str, end_date: str) -> List[str]:",
    "    \"\"\"",
    "    Generate URLs for all RIB dump files in the given date range.",
    "    RIB dumps are available at 00:00, 08:00, 16:00 UTC daily.",
    "    \"\"\"",
    "    urls = []",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")",
    "    rib_hours = [0, 8, 16]",
    "",
    "    current = start",
    "    while current <= end:",
    "        year_month = current.strftime(\"%Y.%m\")",
    "        for hour in rib_hours:",
    "            ts = current.replace(hour=hour, minute=0)",
    "            if ts < start or ts > end + timedelta(days=1) - timedelta(seconds=1):",
    "                continue",
    "            filename = f\"bview.{ts.strftime('%Y%m%d.%H%M')}.gz\"",
    "            url = f\"{RIPE_BASE_URL}/{collector}/{year_month}/{filename}\"",
    "            urls.append(url)",
    "        current += timedelta(days=1)",
    "",
    "    return urls",
    "",
    "",
    "def discover_files_via_broker(collector: str, start_date: str, end_date: str) -> List[dict]:",
    "    \"\"\"",
    "    Use BGPKIT Broker to discover available RIB MRT files.",
    "    Falls back to URL generation if Broker is unavailable.",
    "    \"\"\"",
    "    try:",
    "        broker = bgpkit.Broker()",
    "        items = broker.query(",
    "            ts_start=f\"{start_date}T00:00:00\",",
    "            ts_end=f\"{end_date}T23:59:59\",",
    "            collector_id=collector,",
    "            data_type=\"rib\"",
    "        )",
    "        if items:",
    "            logger.info(f\"Broker found {len(items)} RIB files\")",
    "            return items",
    "    except Exception as e:",
    "        logger.warning(f\"Broker query failed: {e}. Falling back to URL generation.\")",
    "",
    "    urls = generate_rib_urls(collector, start_date, end_date)",
    "    logger.info(f\"Generated {len(urls)} RIB URLs\")",
    "    return [{\"url\": url} for url in urls]",
    "",
    "",
    "# Discover RIB files",
    "rib_files = discover_files_via_broker(COLLECTOR, START_DATE, END_DATE)",
    "print(f\"\\nRIB files discovered: {len(rib_files)}\")",
    "for f in rib_files[:5]:",
    "    url = f['url'] if isinstance(f, dict) else f.url",
    "    print(f\"  {url}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. MRT Parsing with bgpkit-parser\n\n**bgpkit-parser** is a Rust-based MRT parser with Python bindings (`pybgpkit`).  \nIt handles gzip decompression and MRT parsing transparently.\n\n### Workflow (CSV-first approach)\n1. **Download** MRT `.gz` files locally (saved to `data/mrt_files/`)\n2. **Parse** each file into structured rows matching the standard TABLE_DUMP2 format\n3. **Save** all rows as a single CSV (saved to `output/`)\n4. **Build** AS edges from the CSV for graph construction\n\n### CSV Schema (TABLE_DUMP2 format for RIB entries)\n\n| Column | Description | Example |\n|--------|-------------|---------|\n| `MRT_Type` | Always `TABLE_DUMP2` for RIB dumps | `TABLE_DUMP2` |\n| `Timestamp` | Dump timestamp (UTC) | `2025-11-17 00:00:00` |\n| `Entry_Type` | Always `B` (table entry) for RIB | `B` |\n| `Peer_IP` | IP of the BGP peer | `198.32.132.97` |\n| `Peer_AS` | ASN of the peer | `13335` |\n| `Prefix` | Announced prefix | `1.0.0.0/24` |\n| `AS_Path` | Full AS path (space-separated) | `3356 1299 13335` |\n| `Origin` | ORIGIN attribute (IGP/EGP/INCOMPLETE) | `IGP` |\n| `Next_Hop` | Next-hop IP | `198.32.132.97` |\n| `Local_Pref` | LOCAL_PREF value | `100` |\n| `MED` | Multi-Exit Discriminator | `0` |\n| `Community` | BGP communities (space-separated) | `3356:100 3356:123` |\n| `Atomic_Aggregate` | Atomic aggregate flag | `AG` or empty |\n| `Aggregator` | Aggregator AS and IP | `13335 198.32.132.97` |\n\n**Note:** RIB dumps use `TABLE_DUMP2` with entry type `B`, unlike UPDATE files which use `BGP4MP` with `A` (announcement) and `W` (withdrawal).\n\n### Graph construction from AS_PATH\n1. Parse AS_PATH string (space-separated ASNs)\n2. Remove AS prepending (consecutive duplicates)\n3. Skip AS_SET entries (e.g., `{1234,5678}`)\n4. Filter private/reserved ASNs (RFC 6996, RFC 7300)\n5. Extract pairwise adjacent links -> undirected edges"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import urllib.request\n\ndef parse_as_path(as_path_str: str) -> List[int]:\n    \"\"\"\n    Parse an AS_PATH string into a deduplicated list of valid public ASNs.\n    Handles:\n    - Standard AS paths: \"3356 1299 13335\"\n    - AS prepending: \"3356 3356 3356 1299\" -> [3356, 1299]\n    - AS_SETs: \"{1234,5678}\" -> skipped entirely\n    - Private/reserved ASNs: filtered out (RFC 6996, RFC 7300)\n    \"\"\"\n    if not as_path_str:\n        return []\n\n    tokens = as_path_str.split()\n    deduped = []\n    for token in tokens:\n        if '{' in token or '}' in token:\n            continue\n        try:\n            asn = int(token)\n            if not is_valid_public_asn(asn):\n                continue\n            if not deduped or asn != deduped[-1]:\n                deduped.append(asn)\n        except ValueError:\n            continue\n    return deduped\n\n\ndef extract_edges_from_as_path(as_path: List[int]) -> List[Tuple[int, int]]:\n    \"\"\"\n    Extract pairwise AS adjacency edges from a parsed AS path.\n    Returns a list (not set) so callers can count occurrences for weighting.\n    \"\"\"\n    edges = []\n    for i in range(len(as_path) - 1):\n        edge = tuple(sorted([as_path[i], as_path[i + 1]]))\n        if edge[0] != edge[1]:\n            edges.append(edge)\n    return edges\n\n\ndef download_mrt_file(url: str, dest_dir: Path) -> Path:\n    \"\"\"Download an MRT file and save it locally. Returns the local file path.\"\"\"\n    filename = url.split('/')[-1]\n    local_path = dest_dir / filename\n\n    if local_path.exists():\n        size_mb = local_path.stat().st_size / (1024 * 1024)\n        logger.info(f\"  Already downloaded: {filename} ({size_mb:.1f} MB)\")\n        return local_path\n\n    logger.info(f\"  Downloading: {filename}\")\n    t0 = time.time()\n    urllib.request.urlretrieve(url, str(local_path))\n    elapsed = time.time() - t0\n    size_mb = local_path.stat().st_size / (1024 * 1024)\n    logger.info(f\"  Saved: {filename} ({size_mb:.1f} MB, {elapsed:.1f}s)\")\n    return local_path\n\n\ndef parse_mrt_to_rows(file_path: str) -> Tuple[List[dict], dict]:\n    \"\"\"\n    Parse a single MRT RIB dump file into structured TABLE_DUMP2 rows.\n\n    Each RIB entry is mapped to:\n        TABLE_DUMP2|timestamp|B|peer_ip|peer_as|prefix|as_path|origin|\n        next_hop|local_pref|med|community|atomic_agg|aggregator\n\n    Returns:\n        rows: list of dicts (one per RIB entry)\n        stats: parsing statistics\n    \"\"\"\n    rows = []\n    stats = {\n        'total_elements': 0,\n        'announcements': 0,\n        'withdrawals': 0,\n        'unique_prefixes': set(),\n        'unique_peers': set(),\n        'parse_errors': 0,\n    }\n\n    logger.info(f\"  Parsing: {Path(file_path).name}\")\n    t0 = time.time()\n\n    try:\n        parser = bgpkit.Parser(url=str(file_path))\n        for elem in parser:\n            stats['total_elements'] += 1\n\n            if elem.elem_type == \"W\":\n                stats['withdrawals'] += 1\n                continue\n\n            stats['announcements'] += 1\n\n            if elem.prefix:\n                stats['unique_prefixes'].add(elem.prefix)\n            if elem.peer_asn:\n                stats['unique_peers'].add(elem.peer_asn)\n\n            # Convert timestamp to readable UTC string\n            ts = datetime.fromtimestamp(\n                elem.timestamp, tz=timezone.utc\n            ).strftime('%Y-%m-%d %H:%M:%S')\n\n            # Build community string (space-separated)\n            communities = ''\n            if elem.communities:\n                communities = ' '.join(str(c) for c in elem.communities)\n\n            # Build aggregator string\n            aggregator = ''\n            aggr_asn = getattr(elem, 'aggr_asn', None)\n            aggr_ip = getattr(elem, 'aggr_ip', None)\n            if aggr_asn:\n                aggregator = f\"{aggr_asn} {aggr_ip}\".strip() if aggr_ip else str(aggr_asn)\n\n            row = {\n                'MRT_Type': 'TABLE_DUMP2',\n                'Timestamp': ts,\n                'Entry_Type': 'B',\n                'Peer_IP': elem.peer_ip or '',\n                'Peer_AS': elem.peer_asn if elem.peer_asn else '',\n                'Prefix': elem.prefix or '',\n                'AS_Path': elem.as_path or '',\n                'Origin': elem.origin or '',\n                'Next_Hop': elem.next_hop or '',\n                'Local_Pref': elem.local_pref if elem.local_pref is not None else '',\n                'MED': elem.med if elem.med is not None else '',\n                'Community': communities,\n                'Atomic_Aggregate': 'AG' if elem.atomic else '',\n                'Aggregator': aggregator,\n            }\n            rows.append(row)\n\n    except Exception as e:\n        logger.error(f\"Error parsing {file_path}: {e}\")\n        stats['parse_errors'] += 1\n\n    elapsed = time.time() - t0\n    stats['unique_prefixes'] = len(stats['unique_prefixes'])\n    stats['unique_peers'] = len(stats['unique_peers'])\n    stats['rows_parsed'] = len(rows)\n    stats['parse_time_sec'] = round(elapsed, 2)\n\n    logger.info(\n        f\"  -> {stats['total_elements']:,} elements, \"\n        f\"{len(rows):,} rows, {stats['unique_prefixes']:,} prefixes, \"\n        f\"{stats['unique_peers']:,} peers in {elapsed:.1f}s\"\n    )\n    return rows, stats\n\n\nprint(\"Parse functions defined:\")\nprint(\"  - parse_as_path(as_path_str) -> List[int]\")\nprint(\"  - extract_edges_from_as_path(as_path) -> List[Tuple[int,int]]\")\nprint(\"  - download_mrt_file(url, dest_dir) -> Path\")\nprint(\"  - parse_mrt_to_rows(file_path) -> (rows, stats)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Download MRT files locally & parse to structured CSV\n",
    "# ============================================================================\n",
    "# Downloads raw MRT .gz files and parses every RIB entry into CSV format.\n",
    "# Also saves per-snapshot CSVs and builds a snapshot manifest.\n",
    "# ============================================================================\n",
    "\n",
    "MRT_DIR = DATA_DIR / \"mrt_files\"\n",
    "MRT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_rows = []\n",
    "all_stats = []\n",
    "snapshot_manifest = []  # Per-snapshot metadata for the main processing loop\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: DOWNLOAD MRT FILES & PARSE TO CSV\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, f in enumerate(rib_files):\n",
    "    url = f[\"url\"] if isinstance(f, dict) else f.url\n",
    "    print(f\"\n",
    "[{i+1}/{len(rib_files)}] {url}\")\n",
    "\n",
    "    local_path = download_mrt_file(url, MRT_DIR)\n",
    "    rows, stats = parse_mrt_to_rows(str(local_path))\n",
    "    stats[\"file_type\"] = \"rib\"\n",
    "    stats[\"url\"] = url\n",
    "    stats[\"local_path\"] = str(local_path)\n",
    "    all_rows.extend(rows)\n",
    "    all_stats.append(stats)\n",
    "\n",
    "    # Save per-snapshot CSV\n",
    "    if PER_SNAPSHOT_CSV and rows:\n",
    "        filename = Path(local_path).name  # e.g. bview.20251117.0000.gz\n",
    "        parts = filename.replace(\"bview.\", \"\").replace(\".gz\", \"\").split(\".\")\n",
    "        date_part, time_part = parts[0], parts[1]\n",
    "        snapshot_ts = f\"{date_part[:4]}-{date_part[4:6]}-{date_part[6:8]}T{time_part[:2]}:{time_part[2:]}:00Z\"\n",
    "        snapshot_id = f\"{COLLECTOR}_{date_part}_{time_part}\"\n",
    "\n",
    "        snap_csv_path = SNAPSHOTS_DIR / f\"rib_{snapshot_id}.csv\"\n",
    "        pd.DataFrame(rows).to_csv(snap_csv_path, index=False)\n",
    "\n",
    "        snapshot_manifest.append({\n",
    "            \"snapshot_id\": snapshot_id,\n",
    "            \"timestamp\": snapshot_ts,\n",
    "            \"collector\": COLLECTOR,\n",
    "            \"url\": url,\n",
    "            \"mrt_path\": str(local_path),\n",
    "            \"csv_path\": str(snap_csv_path),\n",
    "            \"n_rows\": len(rows),\n",
    "        })\n",
    "        print(f\"  Snapshot CSV: {snap_csv_path.name} ({len(rows):,} rows)\")\n",
    "\n",
    "    print(f\"  Running total: {len(all_rows):,} rows\")\n",
    "\n",
    "# Save combined CSV\n",
    "csv_filename = f\"rib_parsed_{COLLECTOR}_{START_DATE}_{END_DATE}.csv\"\n",
    "csv_path = OUTPUT_DIR / csv_filename\n",
    "rib_df = pd.DataFrame(all_rows)\n",
    "rib_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Save stats and manifest\n",
    "pd.DataFrame(all_stats).to_csv(OUTPUT_DIR / \"parsing_stats.csv\", index=False)\n",
    "pd.DataFrame(snapshot_manifest).to_csv(OUTPUT_DIR / \"snapshot_manifest.csv\", index=False)\n",
    "\n",
    "print(f\"\n",
    "{'=' * 70}\")\n",
    "print(f\"DOWNLOAD & PARSE COMPLETE\")\n",
    "print(f\"  Combined CSV: {csv_path} ({csv_path.stat().st_size / (1024*1024):.1f} MB)\")\n",
    "print(f\"  Total rows: {len(rib_df):,}\")\n",
    "print(f\"  Snapshot manifest: {len(snapshot_manifest)} snapshots\")\n",
    "print(f\"{'=' * 70}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Edge Extraction Function\n",
    "\n",
    "Build AS topology edges from a parsed RIB CSV. Called once per snapshot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_edges_from_csv(csv_path_or_df):\n",
    "    \"\"\"\n",
    "    Load a per-snapshot CSV and build AS topology edges.\n",
    "\n",
    "    Args:\n",
    "        csv_path_or_df: Path to CSV file or pre-loaded DataFrame\n",
    "\n",
    "    Returns:\n",
    "        all_edges: set of unique (asn_a, asn_b) edges (sorted tuple)\n",
    "        edge_counts: Counter mapping edge -> observation count\n",
    "    \"\"\"\n",
    "    if isinstance(csv_path_or_df, (str, Path)):\n",
    "        df = pd.read_csv(csv_path_or_df)\n",
    "    else:\n",
    "        df = csv_path_or_df\n",
    "\n",
    "    all_edges_list = []\n",
    "    for as_path_raw in df[\"AS_Path\"].dropna().astype(str):\n",
    "        if as_path_raw == \"\" or as_path_raw == \"nan\":\n",
    "            continue\n",
    "        as_path = parse_as_path(as_path_raw)\n",
    "        edges = extract_edges_from_as_path(as_path)\n",
    "        all_edges_list.extend(edges)\n",
    "\n",
    "    edge_counts = Counter(all_edges_list)\n",
    "    all_edges = set(edge_counts.keys())\n",
    "\n",
    "    return all_edges, edge_counts\n",
    "\n",
    "\n",
    "print(\"Defined: build_edges_from_csv(csv_path_or_df) -> (edges, edge_counts)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Graph Construction & Ego-Network Extraction\n\nBuild an undirected NetworkX graph from AS adjacency pairs, then extract the\n**k-hop ego subgraph** around the target AS. Only the ego subgraph is used\nfor feature computation.\n\n### Ego-Network Extraction\nGiven the full AS graph G and a target AS node v:\n1. Find all nodes within k hops of v using BFS\n2. Extract the induced subgraph on those nodes (preserves all edges between them)\n3. Check connectivity \u2014 if the ego subgraph is disconnected, take the LCC containing v\n4. Convert to NetworKit for performance-critical computations"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def nx_to_nk(G_nx):\n    \"\"\"Convert a NetworkX graph to NetworKit format.\"\"\"\n    if not HAS_NETWORKIT:\n        return None, None, None\n    node_list = sorted(G_nx.nodes())\n    nx2nk_map = {n: i for i, n in enumerate(node_list)}\n    nk2nx_map = {i: n for n, i in nx2nk_map.items()}\n    G_nk = nk.Graph(len(node_list), weighted=False, directed=False)\n    for u, v in G_nx.edges():\n        G_nk.addEdge(nx2nk_map[u], nx2nk_map[v])\n    return G_nk, nx2nk_map, nk2nx_map\n\n\ndef extract_ego_subgraph(G, target_as, k_hop):\n    \"\"\"\n    Extract k-hop ego subgraph around target_as from the full graph.\n\n    Args:\n        G: full NetworkX graph\n        target_as: the center AS number\n        k_hop: number of hops to include\n\n    Returns:\n        G_ego: the ego subgraph (connected component containing target_as)\n        ego_info: dict with metadata about the extraction\n    \"\"\"\n    if target_as not in G:\n        return None, {\"error\": f\"AS {target_as} not found in graph\"}\n\n    # BFS to find all nodes within k hops\n    ego_nodes = set()\n    ego_nodes.add(target_as)\n    frontier = {target_as}\n    for hop in range(k_hop):\n        next_frontier = set()\n        for node in frontier:\n            for neighbor in G.neighbors(node):\n                if neighbor not in ego_nodes:\n                    next_frontier.add(neighbor)\n                    ego_nodes.add(neighbor)\n        frontier = next_frontier\n        if not frontier:\n            break\n\n    # Extract induced subgraph\n    G_ego = G.subgraph(ego_nodes).copy()\n\n    ego_info = {\n        \"target_as\": target_as,\n        \"k_hop\": k_hop,\n        \"ego_nodes\": len(ego_nodes),\n        \"ego_edges\": G_ego.number_of_edges(),\n        \"target_degree_full\": G.degree(target_as),\n    }\n\n    # Ensure connectivity (ego subgraph should be connected by construction,\n    # but check anyway in case of graph oddities)\n    if not nx.is_connected(G_ego):\n        components = list(nx.connected_components(G_ego))\n        # Take the component containing the target AS\n        for comp in components:\n            if target_as in comp:\n                G_ego = G_ego.subgraph(comp).copy()\n                break\n        ego_info[\"ego_connected\"] = False\n        ego_info[\"ego_nodes_after_lcc\"] = G_ego.number_of_nodes()\n    else:\n        ego_info[\"ego_connected\"] = True\n\n    return G_ego, ego_info\n\n\ndef build_snapshot_graph(edges, edge_counts, collector, snapshot_id, timestamp,\n                         target_as=None, k_hop=2):\n    \"\"\"\n    Build AS-level topology graph, optionally extract ego subgraph.\n\n    If target_as is provided, extracts k-hop ego subgraph and uses that\n    for all feature computation. Otherwise uses full graph LCC (original behavior).\n\n    Returns:\n        G: full graph\n        G_sub: subgraph for feature computation (ego or LCC)\n        G_nk: NetworKit graph of G_sub (or None)\n        nx2nk_map: NetworkX->NetworKit node ID mapping (or None)\n        nk2nx_map: NetworKit->NetworkX node ID mapping (or None)\n        info: dict with graph metadata\n    \"\"\"\n    G = nx.Graph()\n    for (u, v), w in edge_counts.items():\n        G.add_edge(u, v, weight=w)\n\n    G.graph[\"name\"] = f\"AS-level topology ({collector}, {snapshot_id})\"\n    G.graph[\"collector\"] = collector\n    G.graph[\"snapshot_id\"] = snapshot_id\n    G.graph[\"timestamp\"] = timestamp\n\n    info = {\n        \"n_nodes_full\": G.number_of_nodes(),\n        \"n_edges_full\": G.number_of_edges(),\n    }\n\n    # --- Ego-network extraction ---\n    if target_as is not None:\n        G_ego, ego_info = extract_ego_subgraph(G, target_as, k_hop)\n        info.update(ego_info)\n\n        if G_ego is None:\n            raise ValueError(f\"Target AS {target_as} not found in snapshot {snapshot_id}\")\n\n        G_sub = G_ego\n        info[\"mode\"] = \"ego\"\n        info[\"n_nodes\"] = G_sub.number_of_nodes()\n        info[\"n_edges\"] = G_sub.number_of_edges()\n        info[\"lcc_fraction\"] = G_sub.number_of_nodes() / G.number_of_nodes()\n\n    # --- Full graph LCC (fallback / original behavior) ---\n    else:\n        info[\"is_connected\"] = nx.is_connected(G)\n        info[\"mode\"] = \"full\"\n\n        if not nx.is_connected(G):\n            components = list(nx.connected_components(G))\n            sizes = sorted([len(c) for c in components], reverse=True)\n            info[\"n_components\"] = len(components)\n            info[\"lcc_fraction\"] = sizes[0] / G.number_of_nodes()\n            largest_cc = max(components, key=len)\n            G_sub = G.subgraph(largest_cc).copy()\n        else:\n            info[\"n_components\"] = 1\n            info[\"lcc_fraction\"] = 1.0\n            G_sub = G\n\n        info[\"n_nodes\"] = G_sub.number_of_nodes()\n        info[\"n_edges\"] = G_sub.number_of_edges()\n\n    # NetworKit conversion\n    G_nk, nx2nk_map, nk2nx_map = nx_to_nk(G_sub)\n\n    return G, G_sub, G_nk, nx2nk_map, nk2nx_map, info\n\n\nprint(\"Defined: extract_ego_subgraph(G, target_as, k_hop) -> (G_ego, ego_info)\")\nprint(\"Defined: build_snapshot_graph(edges, edge_counts, ..., target_as, k_hop)\")\nprint(\"  Returns (G, G_sub, G_nk, nx2nk_map, nk2nx_map, info)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Graph-Level Feature Extraction Function\n",
    "\n",
    "Extract all 16 graph-level metrics from a single snapshot graph.\n",
    "Each feature computation is wrapped in try/except for graceful failure handling."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def extract_graph_level_features(G_lcc, G_nk, nk2nx_map, config):\n",
    "    \"\"\"\n",
    "    Extract all 16 graph-level features from a snapshot's LCC graph.\n",
    "\n",
    "    Args:\n",
    "        G_lcc: NetworkX graph (largest connected component)\n",
    "        G_nk: NetworKit graph (or None)\n",
    "        nk2nx_map: NetworKit->NetworkX node ID mapping (or None)\n",
    "        config: dict with 'compute_spectral', 'betweenness_sample_k'\n",
    "\n",
    "    Returns:\n",
    "        features: dict of feature_name -> value (None if failed)\n",
    "        shared_data: dict with '_bc_map', '_core_map' for node-level reuse\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    shared_data = {}\n",
    "    n_nodes = G_lcc.number_of_nodes()\n",
    "    n_edges = G_lcc.number_of_edges()\n",
    "    features['n_nodes'] = n_nodes\n",
    "    features['n_edges'] = n_edges\n",
    "\n",
    "    # Pre-compute sparse matrices (reused by features 5, 6, 8-11, 16)\n",
    "    A_sparse = nx.adjacency_matrix(G_lcc).astype(float)\n",
    "    L_sparse = nx.laplacian_matrix(G_lcc).astype(float)\n",
    "\n",
    "    # Degree list (reused by rich-club percentiles)\n",
    "    degrees = [d for _, d in G_lcc.degree()]\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. ASSORTATIVITY\n",
    "    # Citation: Newman, Phys. Rev. Lett. 89, 208701 (2002).\n",
    "    # ------------------------------------------------------------------\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        features['assortativity'] = nx.degree_assortativity_coefficient(G_lcc)\n",
    "        logger.info(f\"  [1/16] Assortativity: {features['assortativity']:.6f} ({time.time()-t0:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        features['assortativity'] = None\n",
    "        logger.warning(f\"  [1/16] Assortativity: FAILED ({e})\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2. DENSITY\n",
    "    # ------------------------------------------------------------------\n",
    "    features['density'] = nx.density(G_lcc)\n",
    "    logger.info(f\"  [2/16] Density: {features['density']:.8f}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3. CLUSTERING COEFFICIENT\n",
    "    # Citation: Watts & Strogatz, Nature 393 (1998).\n",
    "    # ------------------------------------------------------------------\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        if HAS_NETWORKIT:\n",
    "            features['clustering_global'] = nk.globals.ClusteringCoefficient.exactGlobal(G_nk)\n",
    "            features['clustering_avg_local'] = nk.globals.ClusteringCoefficient.sequentialAvgLocal(G_nk)\n",
    "        else:\n",
    "            features['clustering_global'] = nx.transitivity(G_lcc)\n",
    "            features['clustering_avg_local'] = nx.average_clustering(G_lcc)\n",
    "        logger.info(f\"  [3/16] Clustering: global={features['clustering_global']:.6f}, \"\n",
    "                    f\"local={features['clustering_avg_local']:.6f} ({time.time()-t0:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        features['clustering_global'] = None\n",
    "        features['clustering_avg_local'] = None\n",
    "        logger.warning(f\"  [3/16] Clustering: FAILED ({e})\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4. DIAMETER & AVERAGE PATH LENGTH\n",
    "    # Citation: Watts & Strogatz, Nature 393 (1998).\n",
    "    # ------------------------------------------------------------------\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        if HAS_NETWORKIT:\n",
    "            diam_algo = nk.distance.Diameter(G_nk, algo=nk.distance.DiameterAlgo.AUTOMATIC)\n",
    "            diam_algo.run()\n",
    "            features['diameter'] = diam_algo.getDiameter()[0]\n",
    "        elif n_nodes < 50000:\n",
    "            features['diameter'] = nx.diameter(G_lcc)\n",
    "        else:\n",
    "            sample_nodes = np.random.choice(list(G_lcc.nodes()), size=min(100, n_nodes), replace=False)\n",
    "            features['diameter'] = max(nx.eccentricity(G_lcc, v=node) for node in sample_nodes)\n",
    "        logger.info(f\"  [4/16] Diameter: {features['diameter']} ({time.time()-t0:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        features['diameter'] = None\n",
    "        logger.warning(f\"  [4/16] Diameter: FAILED ({e})\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        if n_nodes < 20000:\n",
    "            features['avg_path_length'] = nx.average_shortest_path_length(G_lcc)\n",
    "        elif HAS_NETWORKIT:\n",
    "            # Use NetworKit BFS for ~10x speedup over NetworkX\n",
    "            sample_size = min(500, n_nodes)\n",
    "            sample_nk_ids = np.random.choice(G_nk.numberOfNodes(), size=sample_size, replace=False)\n",
    "            total_dist, count = 0, 0\n",
    "            for nk_id in sample_nk_ids:\n",
    "                bfs = nk.distance.BFS(G_nk, int(nk_id))\n",
    "                bfs.run()\n",
    "                dists = bfs.getDistances()\n",
    "                total_dist += sum(dists)\n",
    "                count += len(dists) - 1\n",
    "            features['avg_path_length'] = total_dist / count if count > 0 else float('inf')\n",
    "        else:\n",
    "            sample_size = min(500, n_nodes)\n",
    "            sample_nodes = np.random.choice(list(G_lcc.nodes()), size=sample_size, replace=False)\n",
    "            total_dist, count = 0, 0\n",
    "            for node in sample_nodes:\n",
    "                lengths = nx.single_source_shortest_path_length(G_lcc, node)\n",
    "                total_dist += sum(lengths.values())\n",
    "                count += len(lengths) - 1\n",
    "            features['avg_path_length'] = total_dist / count if count > 0 else float('inf')\n",
    "        logger.info(f\"  [4/16] Avg path length: {features['avg_path_length']:.4f} ({time.time()-t0:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        features['avg_path_length'] = None\n",
    "        logger.warning(f\"  [4/16] Avg path length: FAILED ({e})\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5. ALGEBRAIC CONNECTIVITY (Fiedler Value)\n",
    "    # Use shift-invert mode (sigma near 0) for fast convergence on\n",
    "    # the smallest Laplacian eigenvalues. Falls back to which='SM'.\n",
    "    # Citation: Fiedler, Czech. Math. J. 23 (1973).\n",
    "    # ------------------------------------------------------------------\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        try:\n",
    "            eigenvalues = eigsh(L_sparse, k=2, sigma=1e-6, which='LM',\n",
    "                                maxiter=5000, return_eigenvectors=False)\n",
    "        except Exception:\n",
    "            eigenvalues = eigsh(L_sparse, k=2, which='SM',\n",
    "                                maxiter=n_nodes, return_eigenvectors=False)\n",
    "        features['algebraic_connectivity'] = float(np.sort(eigenvalues)[1])\n",
    "        logger.info(f\"  [5/16] Algebraic connectivity: {features['algebraic_connectivity']:.6f} \"\n",
    "                    f\"({time.time()-t0:.1f}s, sparse eigsh)\")\n",
    "    except Exception as e:\n",
    "        features['algebraic_connectivity'] = None\n",
    "        logger.warning(f\"  [5/16] Algebraic connectivity: FAILED ({e})\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 6. SPECTRAL RADIUS\n",
    "    # Citation: Cvetkovic et al., Cambridge (2010).\n",
    "    # ------------------------------------------------------------------\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        sr_vals = eigsh(A_sparse, k=1, which='LM', maxiter=5000,\n",
    "                        return_eigenvectors=False)\n",
    "        features['spectral_radius'] = float(sr_vals[0])\n",
    "        logger.info(f\"  [6/16] Spectral radius: {features['spectral_radius']:.4f} ({time.time()-t0:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        features['spectral_radius'] = None\n",
    "        logger.warning(f\"  [6/16] Spectral radius: FAILED ({e})\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 7. PERCOLATION LIMIT\n",
    "    # Citation: Pastor-Satorras & Vespignani, PRL 86 (2001).\n",
    "    # ------------------------------------------------------------------\n",
    "    if features.get('spectral_radius'):\n",
    "        features['percolation_limit'] = 1.0 / features['spectral_radius']\n",
    "        logger.info(f\"  [7/16] Percolation limit: {features['percolation_limit']:.6f}\")\n",
    "    else:\n",
    "        features['percolation_limit'] = None\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 8-11. SPECTRAL METRICS (conditional on COMPUTE_SPECTRAL)\n",
    "    # Uses shift-invert mode for Laplacian eigenvalues to avoid\n",
    "    # ARPACK convergence issues with which='SM' on singular matrices.\n",
    "    # ------------------------------------------------------------------\n",
    "    if config.get('compute_spectral', True):\n",
    "        t0 = time.time()\n",
    "        # 50 eigenvalues is sufficient for symmetry ratio, natural\n",
    "        # connectivity, and Kirchhoff index. 300 was causing ARPACK\n",
    "        # to hang on large graphs.\n",
    "        n_eigs = min(n_nodes - 2, 50)\n",
    "        use_full_spectrum = n_nodes < 5000\n",
    "\n",
    "        try:\n",
    "            if use_full_spectrum:\n",
    "                L_dense = L_sparse.toarray()\n",
    "                A_dense = A_sparse.toarray()\n",
    "                laplacian_eigs = np.sort(np.real(np.linalg.eigvalsh(L_dense)))\n",
    "                adjacency_eigs = np.sort(np.real(np.linalg.eigvalsh(A_dense)))[::-1]\n",
    "            else:\n",
    "                # Laplacian: shift-invert mode for fast convergence near 0\n",
    "                try:\n",
    "                    laplacian_eigs = np.sort(eigsh(\n",
    "                        L_sparse, k=min(n_eigs, n_nodes-2),\n",
    "                        sigma=1e-6, which='LM', maxiter=5000,\n",
    "                        return_eigenvectors=False))\n",
    "                except Exception as e_si:\n",
    "                    logger.warning(f\"  Shift-invert failed ({e_si}), falling back to which='SM'\")\n",
    "                    laplacian_eigs = np.sort(eigsh(\n",
    "                        L_sparse, k=min(n_eigs, n_nodes-2),\n",
    "                        which='SM', maxiter=n_nodes,\n",
    "                        return_eigenvectors=False))\n",
    "                # Adjacency: which='LM' converges fast (no issues)\n",
    "                adjacency_eigs = np.sort(eigsh(\n",
    "                    A_sparse, k=min(n_eigs, n_nodes-2),\n",
    "                    which='LM', maxiter=5000,\n",
    "                    return_eigenvectors=False))[::-1]\n",
    "            logger.info(f\"  Spectrum computation: {time.time()-t0:.1f}s \"\n",
    "                        f\"({'full' if use_full_spectrum else f'partial, {len(adjacency_eigs)} eigs'})\")\n",
    "\n",
    "            # 8. SYMMETRY RATIO\n",
    "            distinct_eigs = len(np.unique(np.round(adjacency_eigs, 8)))\n",
    "            D = features.get('diameter', 10) or 10\n",
    "            features['symmetry_ratio'] = distinct_eigs / (D + 1)\n",
    "            features['symmetry_ratio_partial'] = not use_full_spectrum\n",
    "            logger.info(f\"  [8/16] Symmetry ratio: {features['symmetry_ratio']:.4f}\")\n",
    "\n",
    "            # 9. NATURAL CONNECTIVITY\n",
    "            # Fix: use n_nodes as denominator for partial spectrum\n",
    "            max_eig = np.max(adjacency_eigs)\n",
    "            shifted = np.exp(adjacency_eigs - max_eig)\n",
    "            if use_full_spectrum:\n",
    "                features['natural_connectivity'] = float(max_eig + np.log(np.mean(shifted)))\n",
    "            else:\n",
    "                features['natural_connectivity'] = float(max_eig + np.log(np.sum(shifted) / n_nodes))\n",
    "            logger.info(f\"  [9/16] Natural connectivity: {features['natural_connectivity']:.4f}\")\n",
    "\n",
    "            # 10. KIRCHHOFF INDEX\n",
    "            nonzero_lap = laplacian_eigs[laplacian_eigs > 1e-10]\n",
    "            if len(nonzero_lap) > 0:\n",
    "                features['kirchhoff_index'] = float(n_nodes * np.sum(1.0 / nonzero_lap))\n",
    "            else:\n",
    "                features['kirchhoff_index'] = None\n",
    "            logger.info(f\"  [10/16] Kirchhoff index: {features.get('kirchhoff_index')}\")\n",
    "\n",
    "            # 11. SPANNING TREES (full spectrum only)\n",
    "            if use_full_spectrum and len(nonzero_lap) > 0:\n",
    "                features['log_spanning_trees'] = float(np.sum(np.log(nonzero_lap)) - np.log(n_nodes))\n",
    "            else:\n",
    "                features['log_spanning_trees'] = None\n",
    "            logger.info(f\"  [11/16] log(spanning trees): {features.get('log_spanning_trees')}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"  [8-11] Spectral metrics: FAILED ({e})\")\n",
    "            for k in ['symmetry_ratio', 'symmetry_ratio_partial', 'natural_connectivity',\n",
    "                       'kirchhoff_index', 'log_spanning_trees']:\n",
    "                features.setdefault(k, None)\n",
    "    else:\n",
    "        for k in ['symmetry_ratio', 'symmetry_ratio_partial', 'natural_connectivity',\n",
    "                   'kirchhoff_index', 'log_spanning_trees']:\n",
    "            features[k] = None\n",
    "        logger.info(\"  [8-11] Spectral metrics: SKIPPED (COMPUTE_SPECTRAL=False)\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 12. NODE & EDGE CONNECTIVITY\n",
    "    # Citation: Whitney, Am. J. Math. 54 (1932).\n",
    "    # Exact computation with timeout to prevent indefinite hanging.\n",
    "    # ------------------------------------------------------------------\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        features['edge_connectivity'] = nx.edge_connectivity(G_lcc)\n",
    "        logger.info(f\"  [12/16] Edge connectivity: {features['edge_connectivity']} ({time.time()-t0:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        features['edge_connectivity'] = None\n",
    "        logger.warning(f\"  [12/16] Edge connectivity: FAILED ({e})\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        features['node_connectivity'] = nx.node_connectivity(G_lcc)\n",
    "        logger.info(f\"  [12/16] Node connectivity: {features['node_connectivity']} ({time.time()-t0:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        features['node_connectivity'] = None\n",
    "        logger.warning(f\"  [12/16] Node connectivity: FAILED ({e})\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 13. RICH-CLUB COEFFICIENT\n",
    "    # Citation: Zhou & Mondragon, IEEE Comm. Lett. (2004).\n",
    "    # ------------------------------------------------------------------\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        rc = nx.rich_club_coefficient(G_lcc, normalized=False)\n",
    "        p25_k = int(np.percentile(degrees, 25))\n",
    "        p50_k = int(np.percentile(degrees, 50))\n",
    "        p75_k = int(np.percentile(degrees, 75))\n",
    "        p90_k = int(np.percentile(degrees, 90))\n",
    "        p95_k = int(np.percentile(degrees, 95))\n",
    "        features['rich_club_p25'] = rc.get(p25_k)\n",
    "        features['rich_club_p50'] = rc.get(p50_k)\n",
    "        features['rich_club_p75'] = rc.get(p75_k)\n",
    "        features['rich_club_p90'] = rc.get(p90_k)\n",
    "        features['rich_club_p95'] = rc.get(p95_k)\n",
    "        logger.info(f\"  [13/16] Rich-club coefficient ({time.time()-t0:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        for k in ['rich_club_p25', 'rich_club_p50', 'rich_club_p75', 'rich_club_p90', 'rich_club_p95']:\n",
    "            features[k] = None\n",
    "        logger.warning(f\"  [13/16] Rich-club: FAILED ({e})\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 14. BETWEENNESS CENTRALITY DISTRIBUTION\n",
    "    # Computed ONCE, stored in shared_data for node-level reuse.\n",
    "    # Citation: Brandes, J. Math. Soc. 25(2) (2001).\n",
    "    # ------------------------------------------------------------------\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        if HAS_NETWORKIT:\n",
    "            if config.get('betweenness_sample_k'):\n",
    "                bc_algo = nk.centrality.ApproxBetweenness(G_nk, epsilon=0.01, delta=0.1)\n",
    "            else:\n",
    "                bc_algo = nk.centrality.Betweenness(G_nk, normalized=True)\n",
    "            bc_algo.run()\n",
    "            bc_scores_nk = bc_algo.scores()\n",
    "            _bc_map = {nk2nx_map[i]: bc_scores_nk[i] for i in range(len(bc_scores_nk))}\n",
    "        else:\n",
    "            _bc_map = nx.betweenness_centrality(\n",
    "                G_lcc, k=config.get('betweenness_sample_k'), normalized=True)\n",
    "\n",
    "        bc_scores = np.array(list(_bc_map.values()))\n",
    "        features['betweenness_mean'] = float(np.mean(bc_scores))\n",
    "        features['betweenness_max'] = float(np.max(bc_scores))\n",
    "        features['betweenness_std'] = float(np.std(bc_scores))\n",
    "        features['betweenness_skewness'] = float(sp_stats.skew(bc_scores))\n",
    "        shared_data['_bc_map'] = _bc_map\n",
    "        logger.info(f\"  [14/16] Betweenness distribution ({time.time()-t0:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        for k in ['betweenness_mean', 'betweenness_max', 'betweenness_std', 'betweenness_skewness']:\n",
    "            features[k] = None\n",
    "        logger.warning(f\"  [14/16] Betweenness: FAILED ({e})\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 15. K-CORE DECOMPOSITION\n",
    "    # Computed ONCE, stored in shared_data for node-level reuse.\n",
    "    # Citation: Seidman, Social Networks 5(3) (1983).\n",
    "    # ------------------------------------------------------------------\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        if HAS_NETWORKIT:\n",
    "            cd = nk.centrality.CoreDecomposition(G_nk)\n",
    "            cd.run()\n",
    "            _core_scores_nk = cd.scores()\n",
    "            _core_map = {nk2nx_map[i]: int(_core_scores_nk[i]) for i in range(len(_core_scores_nk))}\n",
    "            features['degeneracy'] = int(cd.maxCoreNumber())\n",
    "        else:\n",
    "            _core_map = nx.core_number(G_lcc)\n",
    "            features['degeneracy'] = int(max(_core_map.values()))\n",
    "\n",
    "        core_numbers = np.array(list(_core_map.values()))\n",
    "        features['core_mean'] = float(np.mean(core_numbers))\n",
    "        features['core_std'] = float(np.std(core_numbers))\n",
    "        features['core_median'] = float(np.median(core_numbers))\n",
    "        features['innermost_core_size'] = int(np.sum(core_numbers == features['degeneracy']))\n",
    "        shared_data['_core_map'] = _core_map\n",
    "        logger.info(f\"  [15/16] k-Core: degeneracy={features['degeneracy']} ({time.time()-t0:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        for k in ['degeneracy', 'core_mean', 'core_std', 'core_median', 'innermost_core_size']:\n",
    "            features[k] = None\n",
    "        logger.warning(f\"  [15/16] k-Core: FAILED ({e})\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 16. SPECTRAL GAP\n",
    "    # Citation: Chung, Spectral Graph Theory, AMS (1997).\n",
    "    # ------------------------------------------------------------------\n",
    "    if config.get('compute_spectral', True):\n",
    "        try:\n",
    "            adjacency_eigs_local = adjacency_eigs  # from spectral block above\n",
    "            if len(adjacency_eigs_local) >= 2:\n",
    "                sorted_eigs = np.sort(adjacency_eigs_local)[::-1]\n",
    "                features['spectral_gap'] = float(sorted_eigs[0] - sorted_eigs[1])\n",
    "                features['adj_eig_ratio_1_2'] = (\n",
    "                    float(sorted_eigs[0] / sorted_eigs[1]) if sorted_eigs[1] != 0 else None\n",
    "                )\n",
    "            else:\n",
    "                features['spectral_gap'] = None\n",
    "                features['adj_eig_ratio_1_2'] = None\n",
    "            logger.info(f\"  [16/16] Spectral gap: {features.get('spectral_gap')}\")\n",
    "        except Exception:\n",
    "            features['spectral_gap'] = None\n",
    "            features['adj_eig_ratio_1_2'] = None\n",
    "    else:\n",
    "        features['spectral_gap'] = None\n",
    "        features['adj_eig_ratio_1_2'] = None\n",
    "\n",
    "    shared_data['degrees'] = degrees\n",
    "    return features, shared_data\n",
    "\n",
    "\n",
    "print(\"Defined: extract_graph_level_features(G_lcc, G_nk, nk2nx_map, config)\")\n",
    "print(\"  Returns (features_dict, shared_data)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Node-Level Feature Extraction Function\n",
    "\n",
    "Extract all 10 node-level metrics for every AS in the snapshot's LCC graph.\n",
    "Reuses betweenness and k-core results from graph-level extraction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def extract_node_level_features(G_lcc, G_nk, nx2nk_map, nk2nx_map, shared_data, config):\n    \"\"\"\n    Extract all 10 node-level features for a single snapshot.\n\n    Args:\n        G_lcc: NetworkX graph (LCC)\n        G_nk: NetworKit graph (or None)\n        nx2nk_map, nk2nx_map: node ID mappings\n        shared_data: dict with '_bc_map' and '_core_map' from graph-level\n        config: dict with 'max_nodes_for_clique'\n\n    Returns:\n        node_df: DataFrame indexed by ASN with columns for each node feature\n        extra_graph_features: dict with 'radius' (from eccentricity)\n    \"\"\"\n    n_nodes = G_lcc.number_of_nodes()\n    extra_graph_features = {}\n    node_features = pd.DataFrame(index=sorted(G_lcc.nodes()))\n    node_features.index.name = 'asn'\n\n    # ------------------------------------------------------------------\n    # 1. DEGREE CENTRALITY\n    # Citation: Freeman, Social Networks 1(3) (1979).\n    # ------------------------------------------------------------------\n    t0 = time.time()\n    try:\n        dc = nx.degree_centrality(G_lcc)\n        node_features['degree_centrality'] = node_features.index.map(dc)\n        node_features['degree'] = node_features.index.map(dict(G_lcc.degree()))\n        logger.info(f\"    [1/10] Degree centrality ({time.time()-t0:.1f}s)\")\n    except Exception as e:\n        logger.warning(f\"    [1/10] Degree centrality: FAILED ({e})\")\n\n    # ------------------------------------------------------------------\n    # 2. BETWEENNESS CENTRALITY (reused from graph-level)\n    # ------------------------------------------------------------------\n    t0 = time.time()\n    try:\n        _bc_map = shared_data.get('_bc_map', {})\n        if _bc_map:\n            node_features['betweenness_centrality'] = node_features.index.map(_bc_map)\n        else:\n            bc = nx.betweenness_centrality(G_lcc, k=config.get('betweenness_sample_k'), normalized=True)\n            node_features['betweenness_centrality'] = node_features.index.map(bc)\n        logger.info(f\"    [2/10] Betweenness centrality (reused) ({time.time()-t0:.1f}s)\")\n    except Exception as e:\n        logger.warning(f\"    [2/10] Betweenness: FAILED ({e})\")\n\n    # ------------------------------------------------------------------\n    # 3. CLOSENESS CENTRALITY\n    # Citation: Sabidussi, Psychometrika 31(4) (1966).\n    # ------------------------------------------------------------------\n    t0 = time.time()\n    try:\n        if HAS_NETWORKIT:\n            cc_algo = nk.centrality.Closeness(G_nk, True, nk.centrality.ClosenessVariant.GENERALIZED)\n            cc_algo.run()\n            cc_scores = cc_algo.scores()\n            cc_map = {nk2nx_map[i]: cc_scores[i] for i in range(len(cc_scores))}\n            node_features['closeness_centrality'] = node_features.index.map(cc_map)\n        else:\n            cc = nx.closeness_centrality(G_lcc, wf_improved=True)\n            node_features['closeness_centrality'] = node_features.index.map(cc)\n        logger.info(f\"    [3/10] Closeness centrality ({time.time()-t0:.1f}s)\")\n    except Exception as e:\n        logger.warning(f\"    [3/10] Closeness: FAILED ({e})\")\n\n    # ------------------------------------------------------------------\n    # 4. EIGENVECTOR CENTRALITY\n    # Citation: Bonacich, J. Math. Soc. 2(1) (1972).\n    # ------------------------------------------------------------------\n    t0 = time.time()\n    try:\n        if HAS_NETWORKIT:\n            ev_algo = nk.centrality.EigenvectorCentrality(G_nk, tol=1e-8)\n            ev_algo.run()\n            ev_scores = ev_algo.scores()\n            ev_map = {nk2nx_map[i]: ev_scores[i] for i in range(len(ev_scores))}\n            node_features['eigenvector_centrality'] = node_features.index.map(ev_map)\n        else:\n            try:\n                ev = nx.eigenvector_centrality(G_lcc, max_iter=200, tol=1e-6)\n            except nx.PowerIterationFailedConvergence:\n                ev = nx.eigenvector_centrality_numpy(G_lcc)\n            node_features['eigenvector_centrality'] = node_features.index.map(ev)\n        logger.info(f\"    [4/10] Eigenvector centrality ({time.time()-t0:.1f}s)\")\n    except Exception as e:\n        logger.warning(f\"    [4/10] Eigenvector: FAILED ({e})\")\n\n    # ------------------------------------------------------------------\n    # 5. PAGERANK\n    # Citation: Brin & Page, Computer Networks 30 (1998).\n    # ------------------------------------------------------------------\n    t0 = time.time()\n    try:\n        if HAS_NETWORKIT:\n            pr_algo = nk.centrality.PageRank(G_nk, damp=0.85, tol=1e-8)\n            pr_algo.run()\n            pr_scores = pr_algo.scores()\n            pr_map = {nk2nx_map[i]: pr_scores[i] for i in range(len(pr_scores))}\n            node_features['pagerank'] = node_features.index.map(pr_map)\n        else:\n            pr = nx.pagerank(G_lcc, alpha=0.85)\n            node_features['pagerank'] = node_features.index.map(pr)\n        logger.info(f\"    [5/10] PageRank ({time.time()-t0:.1f}s)\")\n    except Exception as e:\n        logger.warning(f\"    [5/10] PageRank: FAILED ({e})\")\n\n    # ------------------------------------------------------------------\n    # 6. LOCAL CLUSTERING COEFFICIENT\n    # Citation: Watts & Strogatz, Nature 393 (1998).\n    # ------------------------------------------------------------------\n    t0 = time.time()\n    try:\n        if HAS_NETWORKIT:\n            lcc_algo = nk.centrality.LocalClusteringCoefficient(G_nk, turbo=True)\n            lcc_algo.run()\n            lcc_scores = lcc_algo.scores()\n            lcc_map = {nk2nx_map[i]: lcc_scores[i] for i in range(len(lcc_scores))}\n            node_features['local_clustering'] = node_features.index.map(lcc_map)\n        else:\n            clust = nx.clustering(G_lcc)\n            node_features['local_clustering'] = node_features.index.map(clust)\n        logger.info(f\"    [6/10] Local clustering ({time.time()-t0:.1f}s)\")\n    except Exception as e:\n        logger.warning(f\"    [6/10] Local clustering: FAILED ({e})\")\n\n    # ------------------------------------------------------------------\n    # 7. AVERAGE NEIGHBOR DEGREE\n    # Citation: Pastor-Satorras et al., PRL 87 (2001).\n    # ------------------------------------------------------------------\n    t0 = time.time()\n    try:\n        and_dict = nx.average_neighbor_degree(G_lcc)\n        node_features['avg_neighbor_degree'] = node_features.index.map(and_dict)\n        logger.info(f\"    [7/10] Avg neighbor degree ({time.time()-t0:.1f}s)\")\n    except Exception as e:\n        logger.warning(f\"    [7/10] Avg neighbor degree: FAILED ({e})\")\n\n    # ------------------------------------------------------------------\n    # 8. NODE CLIQUE NUMBER (NP-hard, with greedy fallback)\n    # ------------------------------------------------------------------\n    t0 = time.time()\n    max_nodes_clique = config.get('max_nodes_for_clique', 5000)\n    try:\n        if n_nodes <= max_nodes_clique:\n            ncn = nx.node_clique_number(G_lcc)\n            node_features['node_clique_number'] = node_features.index.map(ncn)\n        else:\n            def greedy_clique_size(G, node):\n                clique = {node}\n                candidates = set(G.neighbors(node))\n                for cand in sorted(candidates, key=lambda x: G.degree(x), reverse=True):\n                    if all(G.has_edge(cand, c) for c in clique):\n                        clique.add(cand)\n                return len(clique)\n\n            _core_map = shared_data.get('_core_map', {})\n            k_max = max(_core_map.values()) if _core_map else 0\n            ncn_approx = {}\n\n            if k_max > 0:\n                core_subgraph = nx.k_core(G_lcc, k=k_max)\n                if core_subgraph.number_of_nodes() <= max_nodes_clique:\n                    ncn_approx.update(nx.node_clique_number(core_subgraph))\n\n            for node in G_lcc.nodes():\n                if node not in ncn_approx:\n                    ncn_approx[node] = greedy_clique_size(G_lcc, node)\n            node_features['node_clique_number'] = node_features.index.map(ncn_approx)\n        logger.info(f\"    [8/10] Node clique number ({time.time()-t0:.1f}s)\")\n    except Exception as e:\n        logger.warning(f\"    [8/10] Node clique number: FAILED ({e})\")\n\n    # ------------------------------------------------------------------\n    # 9. ECCENTRICITY (also yields graph radius)\n    # For large graphs with NetworKit: iterate BFS per node in C++.\n    # The graph is LCC (connected), so no need to filter inf distances.\n    # ------------------------------------------------------------------\n    t0 = time.time()\n    try:\n        if n_nodes < 30000:\n            ecc = nx.eccentricity(G_lcc)\n            node_features['eccentricity'] = node_features.index.map(ecc)\n            extra_graph_features['radius'] = min(ecc.values())\n        elif HAS_NETWORKIT:\n            ecc_map = {}\n            n = G_nk.numberOfNodes()\n            for nk_id in range(n):\n                bfs = nk.distance.BFS(G_nk, nk_id)\n                bfs.run()\n                # Graph is LCC (connected) \u2014 no inf values to filter\n                ecc_map[nk2nx_map[nk_id]] = int(max(bfs.getDistances()))\n                if (nk_id + 1) % 10000 == 0:\n                    logger.info(f\"      Eccentricity: {nk_id+1}/{n} nodes ({time.time()-t0:.1f}s)\")\n            node_features['eccentricity'] = node_features.index.map(ecc_map)\n            extra_graph_features['radius'] = min(ecc_map.values())\n        else:\n            sample_size = min(500, n_nodes)\n            sample_nodes = np.random.choice(list(G_lcc.nodes()), size=sample_size, replace=False)\n            ecc_sample = {}\n            for node in sample_nodes:\n                lengths = nx.single_source_shortest_path_length(G_lcc, node)\n                ecc_sample[node] = max(lengths.values())\n            node_features['eccentricity'] = node_features.index.map(\n                lambda x: ecc_sample.get(x, np.nan))\n            extra_graph_features['radius'] = min(ecc_sample.values()) if ecc_sample else None\n        logger.info(f\"    [9/10] Eccentricity ({time.time()-t0:.1f}s), \"\n                    f\"radius={extra_graph_features.get('radius')}\")\n    except Exception as e:\n        extra_graph_features['radius'] = None\n        logger.warning(f\"    [9/10] Eccentricity: FAILED ({e})\")\n\n    # ------------------------------------------------------------------\n    # 10. K-SHELL / CORE NUMBER (reused from graph-level)\n    # Citation: Seidman, Social Networks 5(3) (1983).\n    # ------------------------------------------------------------------\n    t0 = time.time()\n    try:\n        _core_map = shared_data.get('_core_map', {})\n        if _core_map:\n            node_features['core_number'] = node_features.index.map(_core_map)\n        else:\n            cn = nx.core_number(G_lcc)\n            node_features['core_number'] = node_features.index.map(cn)\n        logger.info(f\"    [10/10] Core number (reused) ({time.time()-t0:.1f}s)\")\n    except Exception as e:\n        logger.warning(f\"    [10/10] Core number: FAILED ({e})\")\n\n    return node_features, extra_graph_features\n\n\nprint(\"Defined: extract_node_level_features(G_lcc, G_nk, nx2nk_map, nk2nx_map, shared_data, config)\")\nprint(\"  Returns (node_df, extra_graph_features)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Per-Snapshot Processing Loop\n\nProcess each RIB snapshot independently:\n1. Load per-snapshot CSV -> build edges\n2. Build full graph -> extract **ego subgraph** around `TARGET_AS`\n3. Extract graph-level features on the ego subgraph\n4. Extract node-level features for all ego nodes\n5. Append results to time-series accumulators\n\nWith the ego-network approach, each snapshot processes in **seconds** instead of hours."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================================\n# MAIN LOOP: Process each RIB snapshot independently\n# ============================================================================\n\nconfig = {\n    'compute_spectral': COMPUTE_SPECTRAL,\n    'betweenness_sample_k': BETWEENNESS_SAMPLE_K,\n    'max_nodes_for_clique': MAX_NODES_FOR_CLIQUE,\n}\n\ngraph_level_rows = []       # list of dicts, one per snapshot\nnode_level_rows = []        # list of DataFrames, one per snapshot\nsnapshot_errors = []        # track any snapshot that fails entirely\n\nmode_label = f\"EGO (AS {TARGET_AS}, {EGO_K_HOP}-hop)\" if TARGET_AS else \"FULL GRAPH\"\n\nprint(\"=\" * 70)\nprint(f\"PROCESSING {len(snapshot_manifest)} SNAPSHOTS \u2014 Mode: {mode_label}\")\nprint(\"=\" * 70)\n\nfor idx, snap in enumerate(snapshot_manifest):\n    snap_id = snap['snapshot_id']\n    snap_ts = snap['timestamp']\n    snap_csv = snap['csv_path']\n\n    print(f\"\\n{'='*70}\")\n    print(f\"[{idx+1}/{len(snapshot_manifest)}] Snapshot: {snap_id} ({snap_ts})\")\n    print(f\"{'='*70}\")\n\n    t_snap_start = time.time()\n\n    try:\n        # Step 1: Build edges\n        t0 = time.time()\n        edges, edge_counts = build_edges_from_csv(snap_csv)\n        print(f\"  Edges: {len(edges):,} unique ({time.time()-t0:.1f}s)\")\n\n        if len(edges) == 0:\n            print(f\"  WARNING: No edges found, skipping snapshot\")\n            snapshot_errors.append({'snapshot_id': snap_id, 'error': 'no edges'})\n            continue\n\n        # Step 2: Build graph + ego extraction\n        t0 = time.time()\n        G, G_sub, G_nk, nx2nk, nk2nx, graph_info = build_snapshot_graph(\n            edges, edge_counts, COLLECTOR, snap_id, snap_ts,\n            target_as=TARGET_AS, k_hop=EGO_K_HOP\n        )\n\n        if TARGET_AS:\n            print(f\"  Full graph: {graph_info['n_nodes_full']:,} nodes, {graph_info['n_edges_full']:,} edges\")\n            print(f\"  Ego subgraph (AS {TARGET_AS}, {EGO_K_HOP}-hop): \"\n                  f\"{graph_info['n_nodes']:,} nodes, {graph_info['n_edges']:,} edges \"\n                  f\"({graph_info['n_nodes']/graph_info['n_nodes_full']:.1%} of full) ({time.time()-t0:.1f}s)\")\n            print(f\"  Target AS degree in full graph: {graph_info.get('target_degree_full', '?')}\")\n        else:\n            print(f\"  Graph: {graph_info['n_nodes']:,} nodes, {graph_info['n_edges']:,} edges \"\n                  f\"(LCC {graph_info['lcc_fraction']:.1%}) ({time.time()-t0:.1f}s)\")\n\n        # Step 3: Graph-level features (on ego subgraph or LCC)\n        t0 = time.time()\n        graph_feats, shared_data = extract_graph_level_features(G_sub, G_nk, nk2nx, config)\n        graph_feats['snapshot_id'] = snap_id\n        graph_feats['timestamp'] = snap_ts\n        graph_feats['collector'] = COLLECTOR\n        graph_feats.update(graph_info)\n        print(f\"  Graph features: {time.time()-t0:.1f}s\")\n\n        # Step 4: Node-level features (for all nodes in ego subgraph)\n        t0 = time.time()\n        node_df, extra_graph_feats = extract_node_level_features(\n            G_sub, G_nk, nx2nk, nk2nx, shared_data, config\n        )\n        graph_feats.update(extra_graph_feats)\n        node_df['snapshot_id'] = snap_id\n        node_df['timestamp'] = snap_ts\n        node_df.index.name = 'asn'\n        print(f\"  Node features: {node_df.shape[1]} cols x {node_df.shape[0]:,} ASes ({time.time()-t0:.1f}s)\")\n\n        # Accumulate results\n        graph_level_rows.append(graph_feats)\n        node_level_rows.append(node_df.reset_index())\n\n        elapsed = time.time() - t_snap_start\n        print(f\"  SNAPSHOT COMPLETE in {elapsed:.1f}s\")\n\n        # Free memory\n        del G, G_sub, G_nk, node_df\n\n    except Exception as e:\n        logger.error(f\"  SNAPSHOT FAILED: {e}\")\n        snapshot_errors.append({'snapshot_id': snap_id, 'error': str(e)})\n        import traceback\n        traceback.print_exc()\n\nprint(f\"\\n{'='*70}\")\nprint(f\"ALL SNAPSHOTS PROCESSED\")\nprint(f\"  Mode: {mode_label}\")\nprint(f\"  Successful: {len(graph_level_rows)}/{len(snapshot_manifest)}\")\nprint(f\"  Errors: {len(snapshot_errors)}\")\nprint(f\"{'='*70}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Combine Results into Time-Series DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# Combine per-snapshot results into time-series DataFrames\n",
    "# ============================================================================\n",
    "\n",
    "# Graph-level time series\n",
    "graph_ts_df = pd.DataFrame(graph_level_rows)\n",
    "meta_cols = ['snapshot_id', 'timestamp', 'collector']\n",
    "feature_cols = [c for c in graph_ts_df.columns if c not in meta_cols]\n",
    "graph_ts_df = graph_ts_df[meta_cols + sorted(feature_cols)]\n",
    "graph_ts_df['timestamp'] = pd.to_datetime(graph_ts_df['timestamp'])\n",
    "graph_ts_df = graph_ts_df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "print(f\"Graph-level time series: {graph_ts_df.shape}\")\n",
    "\n",
    "# Node-level time series\n",
    "node_ts_df = pd.concat(node_level_rows, ignore_index=True)\n",
    "node_ts_df['timestamp'] = pd.to_datetime(node_ts_df['timestamp'])\n",
    "node_ts_df = node_ts_df.sort_values(['timestamp', 'asn']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Node-level time series: {node_ts_df.shape}\")\n",
    "print(f\"  Unique snapshots: {node_ts_df['snapshot_id'].nunique()}\")\n",
    "print(f\"  Unique ASes: {node_ts_df['asn'].nunique()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================================\n# Export time-series results\n# ============================================================================\n\n# Build filename suffix based on mode\nif TARGET_AS:\n    suffix = f\"{COLLECTOR}_AS{TARGET_AS}_{EGO_K_HOP}hop_{START_DATE}_{END_DATE}\"\nelse:\n    suffix = f\"{COLLECTOR}_{START_DATE}_{END_DATE}\"\n\n# 1. Graph-level time series CSV\ngraph_ts_path = OUTPUT_DIR / f\"graph_level_timeseries_{suffix}.csv\"\ngraph_ts_df.to_csv(graph_ts_path, index=False)\nprint(f\"Graph-level time series: {graph_ts_path}\")\n\n# 2. Node-level time series CSV\nnode_ts_path = OUTPUT_DIR / f\"node_level_timeseries_{suffix}.csv\"\nnode_ts_df.to_csv(node_ts_path, index=False)\nprint(f\"Node-level time series: {node_ts_path}\")\n\n# 3. Snapshot errors (if any)\nif snapshot_errors:\n    errors_df = pd.DataFrame(snapshot_errors)\n    errors_df.to_csv(OUTPUT_DIR / \"snapshot_errors.csv\", index=False)\n    print(f\"Snapshot errors: {OUTPUT_DIR / 'snapshot_errors.csv'}\")\n\n# 4. Per-snapshot graph features as JSON\nfor _, row in graph_ts_df.iterrows():\n    snap_id = row['snapshot_id']\n    feats = row.drop(['snapshot_id', 'timestamp', 'collector']).to_dict()\n    feats_clean = {}\n    for k, v in feats.items():\n        if isinstance(v, (np.integer,)):\n            feats_clean[k] = int(v)\n        elif isinstance(v, (np.floating,)):\n            feats_clean[k] = float(v) if not np.isnan(v) else None\n        elif pd.isna(v):\n            feats_clean[k] = None\n        else:\n            feats_clean[k] = v\n    with open(SNAPSHOTS_DIR / f\"graph_features_{snap_id}.json\", 'w') as f:\n        json.dump(feats_clean, f, indent=2, default=str)\n\nprint(f\"\\nAll output files in {OUTPUT_DIR}:\")\nfor f in sorted(OUTPUT_DIR.iterdir()):\n    if f.is_file():\n        sz = f.stat().st_size / (1024 * 1024)\n        print(f\"  {f.name:<55} {sz:.2f} MB\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization: Time-Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================================\n# Visualization: Graph-level feature time series\n# ============================================================================\n\ntitle_suffix = f\"AS {TARGET_AS} {EGO_K_HOP}-hop Ego\" if TARGET_AS else \"Full Graph\"\n\nkey_features = [\n    ('n_nodes', 'Number of Nodes'),\n    ('n_edges', 'Number of Edges'),\n    ('assortativity', 'Assortativity'),\n    ('density', 'Density'),\n    ('clustering_global', 'Global Clustering'),\n    ('diameter', 'Diameter'),\n    ('algebraic_connectivity', 'Algebraic Connectivity'),\n    ('spectral_radius', 'Spectral Radius'),\n    ('degeneracy', 'Degeneracy (k-max)'),\n    ('betweenness_max', 'Max Betweenness'),\n]\n\nn_plots = len(key_features)\nn_cols = 2\nn_rows = (n_plots + 1) // 2\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 3 * n_rows), sharex=True)\naxes = axes.flatten()\n\nfor i, (feat, label) in enumerate(key_features):\n    ax = axes[i]\n    if feat in graph_ts_df.columns:\n        vals = graph_ts_df[feat].dropna()\n        ts = graph_ts_df.loc[vals.index, 'timestamp']\n        ax.plot(ts, vals, 'o-', markersize=4, linewidth=1.5)\n        ax.set_ylabel(label)\n        ax.grid(True, alpha=0.3)\n        ax.set_title(label)\n    else:\n        ax.set_visible(False)\n\nfor j in range(i + 1, len(axes)):\n    axes[j].set_visible(False)\n\nfig.autofmt_xdate()\nplt.suptitle(f'Graph-Level Feature Time Series ({COLLECTOR}, {title_suffix}, {START_DATE} to {END_DATE})',\n             fontsize=14, y=1.02)\nplt.tight_layout()\nplt.savefig(FIGURES_DIR / 'graph_features_timeseries.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(f\"Saved: {FIGURES_DIR / 'graph_features_timeseries.png'}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================================\n# Visualization: Degree Distribution (last snapshot)\n# ============================================================================\n\nif len(node_level_rows) > 0:\n    last_snap = node_ts_df[node_ts_df['snapshot_id'] == graph_ts_df.iloc[-1]['snapshot_id']]\n    degrees_last = last_snap['degree'].dropna().values\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Linear histogram\n    axes[0].hist(degrees_last, bins=100, edgecolor='black', alpha=0.7, color='steelblue')\n    axes[0].set_xlabel('Degree')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title(f'Degree Distribution \u2014 {title_suffix}')\n    axes[0].axvline(x=np.mean(degrees_last), color='red', linestyle='--',\n                    label=f'Mean={np.mean(degrees_last):.1f}')\n    axes[0].legend()\n\n    # Highlight target AS if in ego mode\n    if TARGET_AS and TARGET_AS in last_snap['asn'].values:\n        target_deg = last_snap.loc[last_snap['asn'] == TARGET_AS, 'degree'].iloc[0]\n        axes[0].axvline(x=target_deg, color='green', linestyle='-', linewidth=2,\n                        label=f'AS {TARGET_AS} (deg={int(target_deg)})')\n        axes[0].legend()\n\n    # Log-log CCDF\n    sorted_deg = np.sort(degrees_last)[::-1]\n    ccdf = np.arange(1, len(sorted_deg) + 1) / len(sorted_deg)\n    axes[1].loglog(sorted_deg, ccdf, '.', markersize=3, color='steelblue')\n    axes[1].set_xlabel('Degree k')\n    axes[1].set_ylabel('P(X >= k)')\n    axes[1].set_title(f'Degree CCDF \u2014 {title_suffix}')\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(FIGURES_DIR / 'degree_distribution.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"Saved: {FIGURES_DIR / 'degree_distribution.png'}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================================\n# Visualization: Centrality Correlations (last snapshot)\n# ============================================================================\n\nif len(node_level_rows) > 0:\n    last_snap = node_ts_df[node_ts_df['snapshot_id'] == graph_ts_df.iloc[-1]['snapshot_id']]\n    centrality_cols = ['degree_centrality', 'betweenness_centrality', 'closeness_centrality',\n                       'eigenvector_centrality', 'pagerank']\n    existing_cols = [c for c in centrality_cols if c in last_snap.columns]\n\n    if len(existing_cols) >= 2:\n        fig, ax = plt.subplots(figsize=(8, 6))\n        corr = last_snap[existing_cols].corr(method='spearman')\n        im = ax.imshow(corr, cmap='RdBu_r', vmin=-1, vmax=1)\n        ax.set_xticks(range(len(existing_cols)))\n        ax.set_yticks(range(len(existing_cols)))\n        short_names = [c.replace('_centrality', '').replace('_', ' ').title() for c in existing_cols]\n        ax.set_xticklabels(short_names, rotation=45, ha='right')\n        ax.set_yticklabels(short_names)\n\n        for i_r in range(len(existing_cols)):\n            for j_r in range(len(existing_cols)):\n                ax.text(j_r, i_r, f\"{corr.iloc[i_r, j_r]:.2f}\", ha='center', va='center',\n                        color='white' if abs(corr.iloc[i_r, j_r]) > 0.5 else 'black', fontsize=10)\n\n        plt.colorbar(im, label='Spearman rho')\n        ax.set_title(f'Centrality Correlations \u2014 {title_suffix}')\n        plt.tight_layout()\n        plt.savefig(FIGURES_DIR / 'centrality_correlations.png', dpi=150, bbox_inches='tight')\n        plt.show()\n        print(f\"Saved: {FIGURES_DIR / 'centrality_correlations.png'}\")\n\n    # --- Target AS feature summary ---\n    if TARGET_AS:\n        target_row = last_snap[last_snap['asn'] == TARGET_AS]\n        if not target_row.empty:\n            print(f\"\\nTarget AS {TARGET_AS} \u2014 Feature Summary (last snapshot):\")\n            feature_cols = [c for c in last_snap.columns\n                          if c not in ['asn', 'snapshot_id', 'timestamp']]\n            for col in feature_cols:\n                val = target_row[col].iloc[0]\n                if pd.notna(val):\n                    # Show rank within ego network\n                    rank = (last_snap[col].dropna() >= val).sum()\n                    total = last_snap[col].dropna().shape[0]\n                    print(f\"  {col:<30} {val:>12.6f}  (rank {rank}/{total})\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Feature Definitions Reference\n\n**Note:** All features below are computed on the **ego subgraph** (k-hop neighborhood\nof the target AS), not the full global AS graph. This means:\n- Graph-level features describe the **local topology** around the target AS\n- Node-level features are computed for the target AS **and all its k-hop neighbors**\n- Changes in these features over time indicate **local topology anomalies**\n\n### Graph-Level Features (16)\n\n| # | Feature | Definition | Ego Interpretation | Citation |\n|---|---------|------------|--------------------|----------|\n| 1 | **Assortativity** | Pearson correlation of degrees at edge endpoints | Do high-degree nodes in the neighborhood connect to each other? | Newman (2002) |\n| 2 | **Density** | 2\\|E\\| / [\\|V\\|(\\|V\\|-1)] | How interconnected is the local neighborhood? | Standard |\n| 3 | **Clustering** | 3 x triangles / connected triples | Do the target's peers form triangles with each other? | Watts & Strogatz (1998) |\n| 4 | **Diameter** | max d(u,v) in ego subgraph | How \"wide\" is the local neighborhood? | Watts & Strogatz (1998) |\n| 5 | **Algebraic connectivity** | 2nd smallest Laplacian eigenvalue | How robust is the local neighborhood to splits? | Fiedler (1973) |\n| 6 | **Spectral radius** | Largest adjacency eigenvalue | Concentration of connectivity in the neighborhood | Cvetkovic et al. (2010) |\n| 7 | **Percolation limit** | 1/spectral_radius | Epidemic threshold in the local network | Pastor-Satorras & Vespignani (2001) |\n| 8 | **Symmetry ratio** | \\|distinct eigenvalues\\| / (D+1) | Structural regularity of the neighborhood | Dekker (2005) |\n| 9 | **Natural connectivity** | ln[(1/n) sum exp(eigenvalues)] | Redundancy / fault tolerance of neighborhood | Wu et al. (2011) |\n| 10 | **Kirchhoff index** | n sum(1/laplacian_eigs) | Effective resistance of the neighborhood | Klein & Randic (1993) |\n| 11 | **log(Spanning trees)** | Matrix tree theorem | Path diversity in the local network | Kirchhoff (1847) |\n| 12 | **Edge/node connectivity** | Min cut size | Minimum links to disconnect the neighborhood | Whitney (1932) |\n| 13 | **Rich-club coefficient** | phi(k) at degree percentiles | Do high-degree neighbors preferentially connect? | Zhou & Mondragon (2004) |\n| 14 | **Betweenness distribution** | Mean, max, std, skewness | Traffic concentration in the neighborhood | Brandes (2001) |\n| 15 | **k-Core metrics** | Degeneracy, core distribution | Depth of hierarchical structure | Seidman (1983) |\n| 16 | **Spectral gap** | lambda_1 - lambda_2 | Expansion properties of the neighborhood | Chung (1997) |\n\n### Node-Level Features (10)\n\n| # | Feature | Definition | Citation |\n|---|---------|------------|----------|\n| 1 | **Degree centrality** | deg(v)/(n-1) | Freeman (1979) |\n| 2 | **Betweenness centrality** | Sum of shortest path fractions through v | Brandes (2001) |\n| 3 | **Closeness centrality** | (n-1) / sum of distances from v | Sabidussi (1966) |\n| 4 | **Eigenvector centrality** | Principal eigenvector of A | Bonacich (1972) |\n| 5 | **PageRank** | Stationary random walk distribution (d=0.85) | Brin & Page (1998) |\n| 6 | **Local clustering** | 2 x triangles(v) / [d(d-1)] | Watts & Strogatz (1998) |\n| 7 | **Avg neighbor degree** | (1/d) sum of neighbor degrees | Pastor-Satorras et al. (2001) |\n| 8 | **Node clique number** | Max clique containing v | NP-hard (Karp, 1972) |\n| 9 | **Eccentricity** | max distance from v | Standard |\n| 10 | **Core number (k-shell)** | max{k : v in H_k} | Seidman (1983) |"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================================\n# PIPELINE COMPLETE\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"PIPELINE COMPLETE\")\nprint(\"=\" * 70)\nprint(f\"\\nCollector: {COLLECTOR}\")\nprint(f\"Date range: {START_DATE} to {END_DATE}\")\nif TARGET_AS:\n    print(f\"Mode: Ego-network (AS {TARGET_AS}, {EGO_K_HOP}-hop)\")\nelse:\n    print(f\"Mode: Full graph\")\nprint(f\"Snapshots processed: {len(graph_level_rows)}/{len(snapshot_manifest)}\")\nif snapshot_errors:\n    print(f\"Snapshot errors: {len(snapshot_errors)}\")\nprint(f\"\\nGraph-level time series: {graph_ts_df.shape}\")\nprint(f\"Node-level time series: {node_ts_df.shape}\")\nprint(f\"\\nOutput directory: {OUTPUT_DIR}\")\nprint(f\"Figures directory: {FIGURES_DIR}\")\n\n# Quick verification\nassert len(graph_ts_df) > 0, \"No snapshots processed\"\nassert 'snapshot_id' in graph_ts_df.columns\nassert 'timestamp' in graph_ts_df.columns\nassert 'algebraic_connectivity' in graph_ts_df.columns\n\nfor snap_id_v in graph_ts_df['snapshot_id']:\n    n_expected = graph_ts_df.loc[graph_ts_df['snapshot_id'] == snap_id_v, 'n_nodes'].iloc[0]\n    n_actual = len(node_ts_df[node_ts_df['snapshot_id'] == snap_id_v])\n    assert n_actual == n_expected, f\"Node count mismatch for {snap_id_v}: {n_actual} vs {n_expected}\"\n\nprint(\"\\nAll verification checks passed.\")",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}